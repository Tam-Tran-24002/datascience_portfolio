---
title: "Client Report - Project 4"
subtitle: "Course DS 250"
author: "Tam Tran"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---
---
## Elevator pitch

_I've developed a machine learning model that classifies homes based on their construction date, specifically distinguishing between those built before 1980 and those built in or after 1980. By analyzing features like living area size, basement presence, and stories, we were able to achieve an accuracy of 85%. This model not only helps us predict the age of properties more accurately but also identifies key factors such as home size and structure that significantly impact this classification. My approach uses advanced machine learning techniques like Random Forests, ensuring reliable predictions that could be valuable for property valuations or real estate decision-making._

```{python}
#| label: libraries
#| include: false
import pandas as pd
from lets_plot import *
import numpy as np
LetsPlot.setup_html(isolated_frame=True)
```
__Create 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.__

_These charts help us see patterns in home size, number of stories, and basement presence. Homes built before 1980 tend to have smaller living areas and larger finished basement sizes on average compared to homes built during or after 1980. This relationship suggests a general trend in design and construction styles over time, with older homes prioritizing basement space more than living area. However, the overlap between the two groups indicates that while these features provide useful insights, they are not definitive on their own and would work best in combination with other variables in a machine learning model._

```{python}
#| label: project-data
#| code-summary: Read and format project data
#Create a new dataframe:
dwellings_ml = "https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv"

data = pd.read_csv(dwellings_ml)
```
```{python}
#| label: Q1-data
#| code-summary: Read and format project data
#Create a new dataframe:
df = data[['livearea', 'finbsmnt', 'basement', 'before1980', 'yrbuilt', 'stories', 'abstrprd', 
'totunits', 'nocars', 'numbdrm', 'numbaths', 'sprice', 'deduct','netprice','tasp','smonth','syear',
'condition_AVG','condition_Excel','condition_Fair','condition_Good','condition_VGood','quality_A','quality_B','quality_C']]

df.head()
```
```{python}
#| label: Q1-data chart
#| code-summary: Read and format project data
#Create a new dataframe:
# Box Plot
LetsPlot.setup_html()

box_plot = ggplot(df, aes(x='before1980', y='livearea')) + \
    geom_boxplot() + \
    ggtitle("Distribution of Home Sizes by 'before1980'") + \
    xlab("Built Before 1980") + \
    ylab("Living Area (sq ft)") + \
    coord_cartesian(ylim=(0, 5000))

box_plot.show()
df
```
```{python}
#| label: Q1-bar plot
#| code-summary: Read and format project data

LetsPlot.setup_html()

box_plot = ggplot(df, aes(x='before1980', y='finbsmnt')) + \
    geom_boxplot() + \
    ggtitle("Distribution of Home Sizes by 'before1980'") + \
    xlab("Built Before 1980") + \
    ylab("Finished Basement (sq ft)") + \
    coord_cartesian(ylim=(0, 5000))

box_plot.show()
df
```
__Build a classification model labeling houses as being built “before 1980” or “during or after 1980”__
_I chose Random Forest Classifier as the final model because it outperformed the others in terms of accuracy and is more robust in handling complex relationships between features. Additionally, it is less prone to overfitting compared to a single Decision Tree, and it provides feature importance insights, which can be valuable for model interpretation._

```{python}
# 1. Random Forest Classifier:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
import pandas as pd

# Drop the unnecessary columns
columns_to_drop = ['before1980', 'parcel', 'yrbuilt']
df_cleaned = df.drop(columns=[col for col in columns_to_drop if col in df.columns])

# Separate features and target variable
X = df_cleaned
y = df['before1980'] 

# Identify categorical columns
categorical_columns = X.select_dtypes(include=['object']).columns
#Dont need to do dummies because the data is cleaned already
# Apply pd.get_dummies to the categorical columns
X_dummies = pd.get_dummies(X, columns=categorical_columns, drop_first=True)

# Train-test split using the transformed data (X_dummies)
X_train, X_test, y_train, y_test = train_test_split(X_dummies, y, test_size=0.2, random_state=42)

# Initialize the RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Predict and evaluate
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy * 100:.2f}%")
print(metrics.classification_report(y_test, y_pred))

# Feature importance analysis (from Random Forest model)
importances = clf.feature_importances_
print("Feature importances:", importances)
```
```{python}
# 2. LogisticRegression:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd

# Drop the unnecessary columns
columns_to_drop = ['before1980', 'parcel', 'yrbuilt']
df_cleaned = df.drop(columns=[col for col in columns_to_drop if col in df.columns])

# Separate features and target variable
X = df_cleaned
y = df['before1980'] 

# Identify categorical columns
categorical_columns = X.select_dtypes(include=['object']).columns

# Apply pd.get_dummies to the categorical columns
X_dummies = pd.get_dummies(X, columns=categorical_columns, drop_first=True)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_dummies, y, test_size=0.2, random_state=42)

# Initialize the Logistic Regression model
model = LogisticRegression(max_iter=1000)

# Fit the model to the training data
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")  
print(classification_report(y_test, y_pred))

```
```{python}
# 3. Decision Tree Classifier:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Drop the unnecessary columns
columns_to_drop = ['before1980', 'parcel', 'yrbuilt']
df_cleaned = df.drop(columns=[col for col in columns_to_drop if col in df.columns])
X = df_cleaned
y = df['before1980'] 
# Identify categorical columns
categorical_columns = X.select_dtypes(include=['object']).columns
# Apply pd.get_dummies to the categorical columns
X_dummies = pd.get_dummies(X, columns=categorical_columns, drop_first=True)
# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Decision Tree model
model = DecisionTreeClassifier(random_state=42)

# Fit the model to the training data
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")  
print(classification_report(y_test, y_pred))
```
__Justify your classification model by discussing the most important features selected by your model. This discussion should include a feature importance chart and a description of the features.__
```{python}
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import pandas as pd
from lets_plot import *

# Drop unnecessary columns
columns_to_drop = ['before1980', 'parcel', 'yrbuilt']
df_cleaned = df.drop(columns=[col for col in columns_to_drop if col in df.columns])

# Separate features and target variable
X = df_cleaned
y = df['before1980']

# Identify categorical columns
categorical_columns = X.select_dtypes(include=['object']).columns

# Apply pd.get_dummies to categorical columns
X_dummies = pd.get_dummies(X, columns=categorical_columns, drop_first=True)  # Avoid multicollinearity

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_dummies, y, test_size=0.2, random_state=42)

# Train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict on test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Feature importance analysis
feature_importances = pd.DataFrame({
    'Feature': X_dummies.columns,
    'Importance': model.feature_importances_
}).sort_values(by='Importance', ascending=True)
 # Sort for better visualization
# Initialize lets-plot
LetsPlot.setup_html()

# Create a ggplot-style bar plot for feature importance
ggplot(feature_importances) + \
    geom_bar(aes(x='Feature', y='Importance'), stat='identity', fill='blue') + \
    ggtitle('Feature Importance in Random Forest Classifier (Using Dummies)') + \
    xlab('Feature') + \
    ylab('Feature Importance') + \
    coord_flip()  # Flip axes for better readability

```

__Describe the quality of your classification model using 2-3 different evaluation metrics__

_My best model is the Random Forest Classifierwith an accuracy of 91.66%, which means that 91.66% of the predictions made by the model were correct, providing a strong indication of overall performance._

_Precision: For homes classified as 'built before 1980,' the model achieved a precision of 93%. This means that 93% of the homes predicted to be 'before 1980' were actually built in that time period._

_Recall: The model achieved a recall of 89% for homes built 'before 1980.' This means the model correctly identified 89% of all the homes that were actually built before 1980._

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
print(classification_report(y_test, y_pred))
cm = confusion_matrix(y_test, y_pred)

# Print the confusion matrix
print("Confusion Matrix:\n", cm)
# #	Predicted Negative (0)	Predicted Positive (1)
# Actual Negative (0)	TN = 1468	FP = 242
# Actual Positive (1)	FN = 298	TP = 2575
# Understanding Each Value:
# True Negatives (TN) = 1468

# The model correctly predicted 1,468 cases as negative (class 0), and they were actually negative.
# False Positives (FP) = 242

# The model incorrectly predicted 242 cases as positive (class 1), but they were actually negative.
# This is a Type I error (False Alarm).
# False Negatives (FN) = 298

# The model incorrectly predicted 298 cases as negative (class 0), but they were actually positive.
# This is a Type II error (Missed Detection).
# True Positives (TP) = 2575

# The model correctly predicted 2,575 cases as positive (class 1), and they were actually positive.
# ```