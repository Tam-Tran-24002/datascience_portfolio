[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Isaac Newtons‚Äôs CV",
    "section": "",
    "text": "Physicist, Mathematician, Cambridge professor.\n\nisaac@applesdofall.org | My wikipedia page\n\n\n\nStanding on the shoulders of giants\n\n\nLaws of motion, gravitation, minting coins, disliking Robert Hooke\n\n\n\nCooling, power series, optics, alchemy, planetary motions, apples.\n\n\n\n\n1654-1660 The King‚Äôs School, Grantham.\nJune 1661 - now Trinity College, Cambridge\n\nSizar\n\n1667 - death Trinity College, Cambridge\n\nFellow\n\n\n\n\n2012 President, Royal Society, London, UK\nAssociate, French Academy of Science, Paris, France\n\n\n\n\n\n\n1669 Newton Sir I, De analysi per √¶quationes numero terminorum infinitas.\n1669 Lectiones optic√¶.\netc. etc. etc.\n\n\n\n2012 Infinitesimal calculus for solutions to physics problems, SMBC patent 001\n\n\n\n\n1600 Royal Mint, London\n\nWarden\nMinted coins\n\n1600 Lucasian professor of Mathematics, Cambridge University"
  },
  {
    "objectID": "resume.html#currently",
    "href": "resume.html#currently",
    "title": "Isaac Newtons‚Äôs CV",
    "section": "",
    "text": "Standing on the shoulders of giants\n\n\nLaws of motion, gravitation, minting coins, disliking Robert Hooke\n\n\n\nCooling, power series, optics, alchemy, planetary motions, apples."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Isaac Newtons‚Äôs CV",
    "section": "",
    "text": "1654-1660 The King‚Äôs School, Grantham.\nJune 1661 - now Trinity College, Cambridge\n\nSizar\n\n1667 - death Trinity College, Cambridge\n\nFellow"
  },
  {
    "objectID": "resume.html#awards",
    "href": "resume.html#awards",
    "title": "Isaac Newtons‚Äôs CV",
    "section": "",
    "text": "2012 President, Royal Society, London, UK\nAssociate, French Academy of Science, Paris, France"
  },
  {
    "objectID": "resume.html#publications",
    "href": "resume.html#publications",
    "title": "Isaac Newtons‚Äôs CV",
    "section": "",
    "text": "1669 Newton Sir I, De analysi per √¶quationes numero terminorum infinitas.\n1669 Lectiones optic√¶.\netc. etc. etc.\n\n\n\n2012 Infinitesimal calculus for solutions to physics problems, SMBC patent 001"
  },
  {
    "objectID": "resume.html#occupation",
    "href": "resume.html#occupation",
    "title": "Isaac Newtons‚Äôs CV",
    "section": "",
    "text": "1600 Royal Mint, London\n\nWarden\nMinted coins\n\n1600 Lucasian professor of Mathematics, Cambridge University"
  },
  {
    "objectID": "250_Projects/project1.html",
    "href": "250_Projects/project1.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Machine Learning projects/3a_Regression_Model/readme.html",
    "href": "Machine Learning projects/3a_Regression_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "There are two datasets in the Data folder that will be used to learn regression. 1. New York City bicycle routes through bridges 2. Idaho Falls Chukars (Pioneer League Baseball) data to find the optimal amount of pitches to throw out a batter based on the teamID (player is a trench goal).\nOverview: XGBoost Regression is a supervised machine learning algorithm that builds an ensemble of decision trees to predict continuous values. It is optimized for speed and performance using gradient boosting techniques.\n\nDefinition XGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:\n\nBoosting Trees: A collection of decision trees built sequentially to reduce errors.\n\nGradient Descent Optimization: Adjusts model weights using the gradient of a loss function.\n\nRegularization: Controls model complexity to prevent overfitting.\n\nFor regression, XGBoost predicts continuous values by minimizing a chosen loss function, commonly Mean Squared Error (MSE) or Mean Absolute Error (MAE).\n\nKey Concepts 1. Boosting Mechanism:\n- Unlike a single decision tree, XGBoost builds multiple trees in sequence.\n- Each new tree corrects the errors of the previous ones by focusing on residuals.\n\nLoss Functions:\n\nDetermines how errors are measured and minimized.\n\nCommon choices:\n\nMean Squared Error (MSE) ‚Äì Penalizes larger errors more heavily.\n\nMean Absolute Error (MAE) ‚Äì Treats all errors equally.\n\nHuber Loss ‚Äì A mix of MSE and MAE to handle outliers.\n\n\nRegularization Techniques:\n\nPrevents overfitting by adding penalties to complex models.\n\nL1 Regularization (Lasso) ‚Äì Shrinks coefficients, promoting sparsity.\n\nL2 Regularization (Ridge) ‚Äì Penalizes large coefficients to reduce variance.\n\nFeature Importance & Selection:\n\nXGBoost ranks features by importance, aiding feature selection.\n\nCan be used to eliminate redundant or irrelevant features.\n\n\n\nPros 1. High Performance ‚Äì Optimized for speed, scalability, and efficiency.\n2. Handles Missing Data ‚Äì Automatically learns how to deal with missing values.\n3. Regularization Built-in ‚Äì Reduces overfitting with L1 and L2 penalties.\n4. Works Well with Large Datasets ‚Äì Efficient memory usage and parallel processing.\n\nCons 1. Complexity ‚Äì More difficult to tune compared to simpler models.\n2. Computationally Intensive ‚Äì Training can be slow on very large datasets.\n3. Sensitive to Hyperparameters ‚Äì Performance depends on careful tuning of learning rate, tree depth, and regularization.\n\nTips * Optimize Hyperparameters ‚Äì Use grid search or Bayesian optimization for tuning.\n* Use Early Stopping ‚Äì Stops training if performance stops improving on validation data.\n* Scale Features if Needed ‚Äì Although XGBoost can handle unscaled data, standardization may help in some cases.\n* Leverage Feature Importance ‚Äì Identify and remove less relevant features to improve efficiency.\n\nUseful Articles and Videos\n* XGBoost Official Documentation\n* XGBoost for Regression ‚Äì Machine Learning Mastery\n* Understanding XGBoost ‚Äì Analytics Vidhya\n* XGBoost Explained ‚Äì YouTube\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine Learning projects/1_KNN_Model/readme.html",
    "href": "Machine Learning projects/1_KNN_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "This folder‚Äôs purpose is to give an example of using KNN machine learning model.\nFoundation datset: iris dataset Stretch dataset: the Heart diseases - UCI (https://archive.ics.uci.edu/dataset/45/heart+disease).\nK-Nearest Neighbors (KNN) is a supervised machine learning algorithm commonly used for classification and regression tasks. It‚Äôs one of the simplest and most intuitive models in machine learning.\n\nDefinition: KNN works by finding the ‚Äòk‚Äô closest data points (neighbors) to a given input based on some distance metric (e.g., Euclidean distance). The predicted value or class is determined by these neighbors:\n\nFor classification, the input is assigned the class most common among its neighbors (majority vote).\nFor regression, the predicted value is the average (or sometimes weighted average) of the neighbors‚Äô values.\n\n\nKey Concepts:\n\nK (Number of Neighbors): The algorithm uses ‚Äòk‚Äô neighbors to make predictions. Choosing the right ‚Äòk‚Äô is crucial:\n\n\nSmall ‚Äòk‚Äô (e.g., 1 or 3) makes the model sensitive to noise.\nLarge ‚Äòk‚Äô smooths out predictions but may overlook local patterns.\n\n\nDistance Metrics: Determines how ‚Äúclose‚Äù neighbors are. Common metrics include:\n\n\nEuclidean Distance: Straight-line distance between points.\nManhattan Distance: Distance measured along axes at right angles.\nCosine Similarity: Measures the cosine of the angle between two vectors (useful for text or high-dimensional data).\n\n\nLaziness: KNN is a lazy learner, meaning it doesn‚Äôt learn a model during training. Instead, it stores the data and makes predictions when queried. This is why it‚Äôs called a ‚Äúmemory-based‚Äù approach.\n\nUseful Articles and Videos: * https://www.w3schools.com/python/python_ml_knn.asp * https://realpython.com/knn-python/ * https://www.geeksforgeeks.org/k-nearest-neighbor-algorithm-in-python/ * https://www.youtube.com/watch?v=CQveSaMyEwM * https://www.youtube.com/watch?v=b6uHw7QW_n4 * https://www.youtube.com/watch?v=w6bOBZX-1kY\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine Learning projects/3b_Classification_Model/ML_Bootcamp_Classification_XGBoost_Template.html#overview",
    "href": "Machine Learning projects/3b_Classification_Model/ML_Bootcamp_Classification_XGBoost_Template.html#overview",
    "title": "Classification Machine Learning Model üß† üìà",
    "section": "Overview",
    "text": "Overview\nXGBoost Classification is a supervised machine learning algorithm that builds an ensemble of decision trees to predict categorical outcomes. It is optimized for speed and performance using gradient boosting techniques.\n\n\n\nDefinition\nXGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:\n\nBoosting Trees: A collection of decision trees built sequentially to reduce errors.\n\nGradient Descent Optimization: Adjusts model weights using the gradient of a loss function.\n\nRegularization: Controls model complexity to prevent overfitting.\n\nFor classification, XGBoost predicts categorical outcomes by minimizing a chosen loss function, such as logistic loss for binary classification or softmax (cross-entropy) loss for multi-class classification.\n\n\n\nKey Concepts\n1. Boosting Mechanism:\n- Unlike a single decision tree, XGBoost builds multiple trees in sequence.\n- Each new tree corrects the errors of the previous ones by focusing on misclassified examples.\n\nLoss Functions:\n\nDetermines how errors are measured and minimized.\n\nCommon choices for classification include:\n\nLogistic Loss ‚Äì Used for binary classification tasks.\n\nSoftmax (Cross-Entropy Loss) ‚Äì Used for multi-class classification tasks.\n\n\nRegularization Techniques:\n\nPrevents overfitting by adding penalties to complex models.\n\nL1 Regularization (Lasso) ‚Äì Encourages sparsity by shrinking coefficients.\n\nL2 Regularization (Ridge) ‚Äì Penalizes large coefficients to reduce variance.\n\nFeature Importance & Selection:\n\nXGBoost ranks features by importance, aiding in feature selection.\n\nHelps in eliminating redundant or irrelevant features for better performance.\n\n\n\n\n\nPros\n1. High Performance ‚Äì Optimized for speed, scalability, and efficiency.\n2. Handles Missing Data ‚Äì Automatically learns how to deal with missing values.\n3. Regularization Built-in ‚Äì Reduces overfitting with L1 and L2 penalties.\n4. Probabilistic Predictions ‚Äì Provides probability scores for classification, enabling threshold tuning.\n5. Works Well with Large Datasets ‚Äì Efficient memory usage and parallel processing.\n\n\n\nCons\n1. Complexity ‚Äì More difficult to tune compared to simpler models.\n2. Computationally Intensive ‚Äì Training can be slow on very large datasets.\n3. Sensitive to Hyperparameters ‚Äì Performance depends on careful tuning of learning rate, tree depth, and regularization.\n4. Less Interpretable ‚Äì Decision boundaries may be challenging to interpret compared to simpler models.\n\n\n\nTips\n* Optimize Hyperparameters ‚Äì Use grid search or Bayesian optimization for tuning.\n* Use Early Stopping ‚Äì Stop training if performance ceases to improve on validation data.\n* Scale Features if Needed ‚Äì Although XGBoost can handle unscaled data, standardization might improve performance.\n* Leverage Feature Importance ‚Äì Identify and remove less relevant features to improve efficiency.\n* Adjust Decision Thresholds ‚Äì Fine-tune the probability threshold to balance precision and recall for your specific task.\n\n\n\nUseful Articles and Videos\n* XGBoost Official Documentation\n* XGBoost for Classification ‚Äì Machine Learning Mastery\n* Understanding XGBoost ‚Äì Analytics Vidhya\n* XGBoost Explained for Classification ‚Äì YouTube"
  },
  {
    "objectID": "Machine Learning projects/3b_Classification_Model/ML_Bootcamp_Classification_XGBoost_Template.html#import-datalibraries",
    "href": "Machine Learning projects/3b_Classification_Model/ML_Bootcamp_Classification_XGBoost_Template.html#import-datalibraries",
    "title": "Classification Machine Learning Model üß† üìà",
    "section": "Import Data/Libraries",
    "text": "Import Data/Libraries\n\n!pip install lets_plot\n\nRequirement already satisfied: lets_plot in /usr/local/lib/python3.11/dist-packages (4.6.1)\nRequirement already satisfied: pypng in /usr/local/lib/python3.11/dist-packages (from lets_plot) (0.20220715.0)\nRequirement already satisfied: palettable in /usr/local/lib/python3.11/dist-packages (from lets_plot) (3.3.3)\n\n\n\n# needed libraries for Classification models\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import f1_score\nfrom xgboost import XGBClassifier\nfrom collections import Counter\n\n# Load the training dataset\n#### If this gives an error go into the Data folder in GitHub and click on the data csv and then \"Raw\"\n#### (underneath history in the upper righthand corner) then copy that url to replace the \"data_raw_url\"\ndata_raw_url = 'https://raw.githubusercontent.com/BYUIDSS/DSS-ML-Bootcamp/refs/heads/main/3b_Classification_Model/data/Churn_Modelling.csv?token=GHSAT0AAAAAAC6JBYNPUIMLJRDEKPFICXJQZ6XVWSQ'\nbanking_df = pd.read_csv(data_raw_url)"
  },
  {
    "objectID": "Machine Learning projects/3b_Classification_Model/ML_Bootcamp_Classification_XGBoost_Template.html#explore-visualize-and-understand-the-data",
    "href": "Machine Learning projects/3b_Classification_Model/ML_Bootcamp_Classification_XGBoost_Template.html#explore-visualize-and-understand-the-data",
    "title": "Classification Machine Learning Model üß† üìà",
    "section": "Explore, Visualize and Understand the Data",
    "text": "Explore, Visualize and Understand the Data\n\nbanking_df.head(10)\n\n\n  \n    \n\n\n\n\n\n\nRowNumber\nCustomerId\nSurname\nCreditScore\nGeography\nGender\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\n\n\n\n\n0\n1\n15634602\nHargrave\n619\nFrance\nFemale\n42.0\n2\n0.00\n1\n1.0\n1.0\n101348.88\n1\n\n\n1\n2\n15647311\nHill\n608\nSpain\nFemale\n41.0\n1\n83807.86\n1\n0.0\n1.0\n112542.58\n0\n\n\n2\n3\n15619304\nOnio\n502\nFrance\nFemale\n42.0\n8\n159660.80\n3\n1.0\n0.0\n113931.57\n1\n\n\n3\n4\n15701354\nBoni\n699\nFrance\nFemale\n39.0\n1\n0.00\n2\n0.0\n0.0\n93826.63\n0\n\n\n4\n5\n15737888\nMitchell\n850\nSpain\nFemale\n43.0\n2\n125510.82\n1\nNaN\n1.0\n79084.10\n0\n\n\n5\n6\n15574012\nChu\n645\nSpain\nMale\n44.0\n8\n113755.78\n2\n1.0\n0.0\n149756.71\n1\n\n\n6\n7\n15592531\nBartlett\n822\nNaN\nMale\n50.0\n7\n0.00\n2\n1.0\n1.0\n10062.80\n0\n\n\n7\n8\n15656148\nObinna\n376\nGermany\nFemale\n29.0\n4\n115046.74\n4\n1.0\n0.0\n119346.88\n1\n\n\n8\n9\n15792365\nHe\n501\nFrance\nMale\n44.0\n4\n142051.07\n2\n0.0\nNaN\n74940.50\n0\n\n\n9\n10\n15592389\nH?\n684\nFrance\nMale\nNaN\n2\n134603.88\n1\n1.0\n1.0\n71725.73\n0"
  },
  {
    "objectID": "Machine Learning projects/3b_Classification_Model/ML_Bootcamp_Classification_XGBoost_Template.html#feature-enginnering-and-data-augmentation",
    "href": "Machine Learning projects/3b_Classification_Model/ML_Bootcamp_Classification_XGBoost_Template.html#feature-enginnering-and-data-augmentation",
    "title": "Classification Machine Learning Model üß† üìà",
    "section": "Feature Enginnering and Data Augmentation",
    "text": "Feature Enginnering and Data Augmentation\n\nData Augmentation\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\nFeature Engineering\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns."
  },
  {
    "objectID": "Machine Learning projects/3b_Classification_Model/ML_Bootcamp_Classification_XGBoost_Template.html#machine-learning-model",
    "href": "Machine Learning projects/3b_Classification_Model/ML_Bootcamp_Classification_XGBoost_Template.html#machine-learning-model",
    "title": "Classification Machine Learning Model üß† üìà",
    "section": "Machine Learning Model",
    "text": "Machine Learning Model\n\nSplit the data to train and test\n\n\nCreate the model\n\n\nTrain the model\n\n\nMake predictions\n\nHyperparameter Search\n\n\n\nEvaluate the Model\nAccuracy ‚Äì The percentage of total predictions that are correct.\nExample: If a spam filter correctly classifies 90 out of 100 emails (whether spam or not), the accuracy is 90%.\nF1 Score ‚Äì Out of all the positive predictions, how many were actually correct.\nExample: If a spam filter predicts 20 emails as spam, but only 15 are actually spam, precision is 15/20 = 75%.\nRecall Score ‚Äì Out of all the actual positive cases, how many did the model correctly identify.\nExample: If there were 25 spam emails in total, and the model correctly identified 15 of them, recall is 15/25 = 60%.\nPrecision Score ‚Äì A balance between precision and recall (harmonic mean).\nExample: If precision is 75% and recall is 60%, F1 score is (2 √ó 75 √ó 60) / (75 + 60) = 66.7%.\n\n# Evaluate the model using classification metrics\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mse)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\n\n# Calculate R-squared (R2)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint('Mean Squared Error (MSE):', mse)\nprint('Root Mean Squared Error (RMSE):', rmse)\nprint('Mean Absolute Error (MAE):', mae)\nprint('R-squared (R2):', r2)"
  },
  {
    "objectID": "Machine Learning projects/2_DecisionTree_RandomForests_Model/readme.html",
    "href": "Machine Learning projects/2_DecisionTree_RandomForests_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "Decision Tree (DT) is a supervised machine learning algorithm used for both classification and regression tasks. It splits data into branches based on feature values to make predictions.\nDefinition A Decision Tree is a flowchart-like model where data is split into smaller subsets based on feature conditions. The structure consists of:\nRoot Node: The starting point that represents the entire dataset. Decision Nodes: Points where the data is split based on a feature. Leaves: Terminal nodes that provide the final prediction. For classification, a Decision Tree assigns class labels based on feature splits. For regression, it predicts continuous values using the average or mean of data points in each leaf.\nKey Concepts\nSplitting Criteria:\nDetermines how the dataset is divided at each step. Common methods: Gini Impurity (Classification) ‚Äì Measures the likelihood of incorrect classification. Entropy (Classification) ‚Äì Uses information gain to decide splits. Mean Squared Error (MSE) (Regression) ‚Äì Measures variance within nodes. Tree Depth & Overfitting:\nDeeper trees fit training data better but may overfit. Pruning (removing unnecessary branches) improves generalization. Feature Importance:\nDecision Trees rank features by their impact on predictions. Helps in feature selection for other models. Handling Missing Data:\nSome implementations allow surrogate splits to handle missing values.\nPros\nEasy to Understand & Interpret ‚Äì Can be visualized as a simple flowchart. No Need for Feature Scaling ‚Äì Works with both categorical and numerical features. Handles Non-Linearity ‚Äì Can model complex relationships without requiring transformation. Fast for Small Datasets ‚Äì Training and inference are relatively quick.\nCons\nProne to Overfitting ‚Äì Deep trees can memorize training data, reducing generalization. Unstable to Small Changes ‚Äì Small variations in data can change the tree structure significantly. Less Efficient on Large Datasets ‚Äì Computationally expensive for large datasets.\nTips\nLimit Tree Depth ‚Äì Use max_depth to prevent overfitting. Pruning Techniques ‚Äì Use pre-pruning (max_leaf_nodes) or post-pruning to remove unnecessary branches. Use Feature Importance ‚Äì Identify the most influential features and remove irrelevant ones. Consider Ensemble Methods ‚Äì Random Forest or Gradient Boosting improve Decision Trees by reducing variance.\nUseful Articles and Videos\nhttps://www.datacamp.com/tutorial/decision-tree-classification-python https://www.ibm.com/think/topics/decision-trees https://www.youtube.com/watch?v=6DlWndLbk90 https://www.youtube.com/watch?v=ZOiBe-nrmc4\nRandom Forest (RF) is a supervised machine learning algorithm used for both classification and regression tasks. It builds multiple decision trees and combines their outputs for better accuracy and stability.\nDefinition A Random Forest is an ensemble learning method that creates a ‚Äúforest‚Äù of decision trees during training. Instead of relying on a single tree, Random Forest combines the predictions of multiple trees to improve performance. The structure consists of:\nDecision Trees: Individual trees that make predictions based on subsets of data and features. Bagging (Bootstrap Aggregating): Each tree is trained on a randomly sampled subset of the data. Voting/Averaging: Combines the predictions from all trees: For classification, the majority vote decides the class. For regression, the average of all tree predictions is used. Random Forest reduces the risk of overfitting compared to individual decision trees and works well for both categorical and numerical data.\nKey Concepts\nBagging: Random Forest uses bootstrapping to train each tree on a different sample of the data. This creates diversity among trees, making the model more robust. Feature Randomness: At each split, Random Forest considers a random subset of features rather than all features. This reduces correlation between trees and improves generalization. Out-of-Bag (OOB) Error: Trees not trained on certain data points (left out during bootstrapping) can be used to validate the model. OOB error gives an unbiased estimate of model performance. Feature Importance: Random Forest provides a ranking of feature importance based on how often features are used for splitting across trees. Useful for identifying key predictors in your data. Pros\nImproved Accuracy ‚Äì Combines multiple trees, reducing overfitting. Robust to Noise ‚Äì Handles outliers and noisy data better than individual trees. Handles Large Datasets ‚Äì Can scale well with more data. Feature Selection ‚Äì Provides insights into the importance of features. No Need for Feature Scaling ‚Äì Works with unscaled data, both numerical and categorical.\nCons\nLess Interpretable ‚Äì Harder to visualize compared to a single decision tree. Computationally Intensive ‚Äì Training many trees can take time and memory for large datasets. Overfitting Risk with Too Many Trees ‚Äì Although rare, excessive trees might still overfit without tuning. Slower Inference ‚Äì Predictions may take longer because they aggregate results from multiple trees.\nTips\nTune n_estimators ‚Äì Adjust the number of trees to balance accuracy and computational cost. Limit Tree Depth ‚Äì Use max_depth to avoid overfitting while maintaining performance. Optimize Feature Subset Size ‚Äì Use max_features to control how many features each tree considers at a split. Use Feature Importance ‚Äì Rank and prioritize the most important features in your dataset. Combine with Other Methods ‚Äì Random Forest pairs well with techniques like PCA for dimensionality reduction. Useful Articles and Videos\nhttps://www.datacamp.com/tutorial/random-forests-classifier-python https://www.ibm.com/topics/random-forest https://www.youtube.com/watch?v=J4Wdy0Wc_xQ https://www.youtube.com/watch?v=QHOazyP-YlM\n\n\n\n Back to top"
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Machine Learning projects/2_DecisionTree_RandomForests_Model/Bootcamp_DecisionTree_RandomForest_Template.html#overview",
    "href": "Machine Learning projects/2_DecisionTree_RandomForests_Model/Bootcamp_DecisionTree_RandomForest_Template.html#overview",
    "title": "Decision Tree Machine Learning Model üíª üß†",
    "section": "Overview",
    "text": "Overview\nDecision Tree (DT) is a supervised machine learning algorithm used for both classification and regression tasks. It splits data into branches based on feature values to make predictions.\n\n\n\nDefinition\nA Decision Tree is a flowchart-like model where data is split into smaller subsets based on feature conditions. The structure consists of:\n\nRoot Node: The starting point that represents the entire dataset.\n\nDecision Nodes: Points where the data is split based on a feature.\n\nLeaves: Terminal nodes that provide the final prediction.\n\nFor classification, a Decision Tree assigns class labels based on feature splits.\nFor regression, it predicts continuous values using the average or mean of data points in each leaf.\n\n\n\nKey Concepts 1. Splitting Criteria:\n- Determines how the dataset is divided at each step.\n- Common methods: * Gini Impurity (Classification) ‚Äì Measures the likelihood of incorrect classification.\n* Entropy (Classification) ‚Äì Uses information gain to decide splits.\n* Mean Squared Error (MSE) (Regression) ‚Äì Measures variance within nodes.\n\nTree Depth & Overfitting:\n\nDeeper trees fit training data better but may overfit.\n\nPruning (removing unnecessary branches) improves generalization.\n\nFeature Importance:\n\nDecision Trees rank features by their impact on predictions.\n\nHelps in feature selection for other models.\n\nHandling Missing Data:\n\nSome implementations allow surrogate splits to handle missing values.\n\n\n\n\n\nPros 1. Easy to Understand & Interpret ‚Äì Can be visualized as a simple flowchart.\n2. No Need for Feature Scaling ‚Äì Works with both categorical and numerical features.\n3. Handles Non-Linearity ‚Äì Can model complex relationships without requiring transformation.\n4. Fast for Small Datasets ‚Äì Training and inference are relatively quick.\n\n\n\nCons 1. Prone to Overfitting ‚Äì Deep trees can memorize training data, reducing generalization.\n2. Unstable to Small Changes ‚Äì Small variations in data can change the tree structure significantly.\n3. Less Efficient on Large Datasets ‚Äì Computationally expensive for large datasets.\n\n\n\nTips * Limit Tree Depth ‚Äì Use max_depth to prevent overfitting.\n* Pruning Techniques ‚Äì Use pre-pruning (max_leaf_nodes) or post-pruning to remove unnecessary branches.\n* Use Feature Importance ‚Äì Identify the most influential features and remove irrelevant ones.\n* Consider Ensemble Methods ‚Äì Random Forest or Gradient Boosting improve Decision Trees by reducing variance.\n\n\n\nUseful Articles and Videos * https://www.datacamp.com/tutorial/decision-tree-classification-python * https://www.ibm.com/think/topics/decision-trees * https://www.youtube.com/watch?v=6DlWndLbk90 * https://www.youtube.com/watch?v=ZOiBe-nrmc4"
  },
  {
    "objectID": "Machine Learning projects/2_DecisionTree_RandomForests_Model/Bootcamp_DecisionTree_RandomForest_Template.html#import-datalibraries",
    "href": "Machine Learning projects/2_DecisionTree_RandomForests_Model/Bootcamp_DecisionTree_RandomForest_Template.html#import-datalibraries",
    "title": "Decision Tree Machine Learning Model üíª üß†",
    "section": "Import Data/Libraries",
    "text": "Import Data/Libraries\n\n!pip install lets_plot\n\nCollecting lets_plot\n  Downloading lets_plot-4.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting pypng (from lets_plot)\n  Downloading pypng-0.20220715.0-py3-none-any.whl.metadata (13 kB)\nCollecting palettable (from lets_plot)\n  Downloading palettable-3.3.3-py2.py3-none-any.whl.metadata (3.3 kB)\nDownloading lets_plot-4.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.4/3.4 MB 20.3 MB/s eta 0:00:00\nDownloading palettable-3.3.3-py2.py3-none-any.whl (332 kB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 332.3/332.3 kB 8.7 MB/s eta 0:00:00\nDownloading pypng-0.20220715.0-py3-none-any.whl (58 kB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 58.1/58.1 kB 911.6 kB/s eta 0:00:00\nInstalling collected packages: pypng, palettable, lets_plot\nSuccessfully installed lets_plot-4.5.2 palettable-3.3.3 pypng-0.20220715.0\n\n\n\n# needed libraries for Decision Tree models\nimport pandas as pd\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.model_selection import cross_val_score,  train_test_split , KFold\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nimport lets_plot as lp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# foundation dataset\ntitanic_df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n\n# stretch dataset\ncleveland_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data', header=None)\nhungarian_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data', header=None)\nswitzerland_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data', header=None)"
  },
  {
    "objectID": "Machine Learning projects/2_DecisionTree_RandomForests_Model/Bootcamp_DecisionTree_RandomForest_Template.html#explore-visualize-and-understand-the-data",
    "href": "Machine Learning projects/2_DecisionTree_RandomForests_Model/Bootcamp_DecisionTree_RandomForest_Template.html#explore-visualize-and-understand-the-data",
    "title": "Decision Tree Machine Learning Model üíª üß†",
    "section": "Explore, Visualize and Understand the Data",
    "text": "Explore, Visualize and Understand the Data"
  },
  {
    "objectID": "Machine Learning projects/2_DecisionTree_RandomForests_Model/Bootcamp_DecisionTree_RandomForest_Template.html#feature-enginnering-and-data-augmentation",
    "href": "Machine Learning projects/2_DecisionTree_RandomForests_Model/Bootcamp_DecisionTree_RandomForest_Template.html#feature-enginnering-and-data-augmentation",
    "title": "Decision Tree Machine Learning Model üíª üß†",
    "section": "Feature Enginnering and Data Augmentation",
    "text": "Feature Enginnering and Data Augmentation\n\nData Augmentation\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\nFeature Engineering\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns."
  },
  {
    "objectID": "Machine Learning projects/2_DecisionTree_RandomForests_Model/Bootcamp_DecisionTree_RandomForest_Template.html#machine-learning-model-decision-tree",
    "href": "Machine Learning projects/2_DecisionTree_RandomForests_Model/Bootcamp_DecisionTree_RandomForest_Template.html#machine-learning-model-decision-tree",
    "title": "Decision Tree Machine Learning Model üíª üß†",
    "section": "Machine Learning Model: Decision Tree",
    "text": "Machine Learning Model: Decision Tree\n\nSplit the data\n\n\nCreate the model\n\n\nTrain the model\n\n# Fitting the model\n\n\n\nMake predictions\n\n# Predicting the Test set results\n\n\n\nEvaluate the Model\nAccuracy ‚Äì The percentage of total predictions that are correct.\nExample: If a spam filter correctly classifies 90 out of 100 emails (whether spam or not), the accuracy is 90%.\nF1 Score ‚Äì Out of all the positive predictions, how many were actually correct.\nExample: If a spam filter predicts 20 emails as spam, but only 15 are actually spam, precision is 15/20 = 75%.\nRecall Score ‚Äì Out of all the actual positive cases, how many did the model correctly identify.\nExample: If there were 25 spam emails in total, and the model correctly identified 15 of them, recall is 15/25 = 60%.\nPrecision Score ‚Äì A balance between precision and recall (harmonic mean).\nExample: If precision is 75% and recall is 60%, F1 score is (2 √ó 75 √ó 60) / (75 + 60) = 66.7%.\n\n# Accuracy, F1 Score, Recall score, Precision score\naccuracy = accuracy_score(y_test, y_pred)*100\nf1_score = f1_score(y_test, y_pred, average='weighted')\nrecall_score = recall_score(y_test, y_pred, average='weighted')\nprecision_score = precision_score(y_test, y_pred, average='weighted')\n\nprint('Accuracy: ' + str(round(accuracy, 2)) + ' %.')\nprint('F1 Score: ' + str(round(f1_score, 2)) + ' %.')\nprint('Recall Score: ' + str(round(recall_score, 2)) + ' %.')\nprint('Precision Score: ' + str(round(precision_score, 2)) + ' %.')"
  },
  {
    "objectID": "Machine Learning projects/2_DecisionTree_RandomForests_Model/Bootcamp_DecisionTree_RandomForest_Template.html#overview-of-random-forests",
    "href": "Machine Learning projects/2_DecisionTree_RandomForests_Model/Bootcamp_DecisionTree_RandomForest_Template.html#overview-of-random-forests",
    "title": "Decision Tree Machine Learning Model üíª üß†",
    "section": "Overview of Random Forests",
    "text": "Overview of Random Forests\nRandom Forest (RF) is a supervised machine learning algorithm used for both classification and regression tasks. It builds multiple decision trees and combines their outputs for better accuracy and stability.\n\nDefinition\nA Random Forest is an ensemble learning method that creates a ‚Äúforest‚Äù of decision trees during training. Instead of relying on a single tree, Random Forest combines the predictions of multiple trees to improve performance. The structure consists of:\n\nDecision Trees: Individual trees that make predictions based on subsets of data and features.\nBagging (Bootstrap Aggregating): Each tree is trained on a randomly sampled subset of the data.\nVoting/Averaging: Combines the predictions from all trees:\nFor classification, the majority vote decides the class.\nFor regression, the average of all tree predictions is used.\nRandom Forest reduces the risk of overfitting compared to individual decision trees and works well for both categorical and numerical data.\n\n\nKey Concepts\n\nBagging:\n\n\nRandom Forest uses bootstrapping to train each tree on a different sample of the data.\nThis creates diversity among trees, making the model more robust.\n\n\nFeature Randomness:\n\n\nAt each split, Random Forest considers a random subset of features rather than all features.\nThis reduces correlation between trees and improves generalization.\n\n\nOut-of-Bag (OOB) Error:\n\n\nTrees not trained on certain data points (left out during bootstrapping) can be used to validate the model.\nOOB error gives an unbiased estimate of model performance.\n\n\nFeature Importance:\n\n\nRandom Forest provides a ranking of feature importance based on how often features are used for splitting across trees.\nUseful for identifying key predictors in your data. \n\nPros\nImproved Accuracy ‚Äì Combines multiple trees, reducing overfitting. Robust to Noise ‚Äì Handles outliers and noisy data better than individual trees. Handles Large Datasets ‚Äì Can scale well with more data. Feature Selection ‚Äì Provides insights into the importance of features. No Need for Feature Scaling ‚Äì Works with unscaled data, both numerical and categorical. \nCons\nLess Interpretable ‚Äì Harder to visualize compared to a single decision tree. Computationally Intensive ‚Äì Training many trees can take time and memory for large datasets. Overfitting Risk with Too Many Trees ‚Äì Although rare, excessive trees might still overfit without tuning. Slower Inference ‚Äì Predictions may take longer because they aggregate results from multiple trees. \nTips\n\nTune n_estimators ‚Äì Adjust the number of trees to balance accuracy and computational cost.\nLimit Tree Depth ‚Äì Use max_depth to avoid overfitting while maintaining performance.\nOptimize Feature Subset Size ‚Äì Use max_features to control how many features each tree considers at a split.\nUse Feature Importance ‚Äì Rank and prioritize the most important features in your dataset.\nCombine with Other Methods ‚Äì Random Forest pairs well with techniques like PCA for dimensionality reduction. \n\nUseful Articles and Videos\nhttps://www.datacamp.com/tutorial/random-forests-classifier-python\nhttps://www.ibm.com/topics/random-forest\nhttps://www.youtube.com/watch?v=J4Wdy0Wc_xQ\nhttps://www.youtube.com/watch?v=QHOazyP-YlM\n\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n\nCreate Model\n\n\nTrain Model\n\n\nMake Predictions\n\n\nHyperparameter Search\n\n\nEvaluate the Model\n\naccuracy = accuracy_score(y_test, y_pred)*100\nf1_score = f1_score(y_test, y_pred, average='weighted')\nrecall_score = recall_score(y_test, y_pred, average='weighted')\nprecision_score = precision_score(y_test, y_pred, average='weighted')\n\nprint('Accuracy: ' + str(round(accuracy, 2)) + ' %.')\nprint('F1 Score: ' + str(round(f1_score, 2)) + ' %.')\nprint('Recall Score: ' + str(round(recall_score, 2)) + ' %.')\nprint('Precision Score: ' + str(round(precision_score, 2)) + ' %.')"
  },
  {
    "objectID": "Machine Learning projects/3b_Classification_Model/readme.html",
    "href": "Machine Learning projects/3b_Classification_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "XGBoost Classification is a supervised machine learning algorithm that builds an ensemble of decision trees to predict categorical outcomes. It is optimized for speed and performance using gradient boosting techniques.\n\nData For the foundation dataset we will be using the bank customer churn dataset. It is a commonly used dataset for predicting customer turnover in the banking industry. For more information go to the data folder‚Äôs readme file.\n\nDefinition\nXGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:\n\nBoosting Trees: A collection of decision trees built sequentially to reduce errors.\n\nGradient Descent Optimization: Adjusts model weights using the gradient of a loss function.\n\nRegularization: Controls model complexity to prevent overfitting.\n\nFor classification, XGBoost predicts categorical outcomes by minimizing a chosen loss function, such as logistic loss for binary classification or softmax (cross-entropy) loss for multi-class classification.\n\nKey Concepts\n1. Boosting Mechanism:\n- Unlike a single decision tree, XGBoost builds multiple trees in sequence.\n- Each new tree corrects the errors of the previous ones by focusing on misclassified examples.\n\nLoss Functions:\n\nDetermines how errors are measured and minimized.\n\nCommon choices for classification include:\n\nLogistic Loss ‚Äì Used for binary classification tasks.\n\nSoftmax (Cross-Entropy Loss) ‚Äì Used for multi-class classification tasks.\n\n\nRegularization Techniques:\n\nPrevents overfitting by adding penalties to complex models.\n\nL1 Regularization (Lasso) ‚Äì Encourages sparsity by shrinking coefficients.\n\nL2 Regularization (Ridge) ‚Äì Penalizes large coefficients to reduce variance.\n\nFeature Importance & Selection:\n\nXGBoost ranks features by importance, aiding in feature selection.\n\nHelps in eliminating redundant or irrelevant features for better performance.\n\n\n\nPros\n1. High Performance ‚Äì Optimized for speed, scalability, and efficiency.\n2. Handles Missing Data ‚Äì Automatically learns how to deal with missing values.\n3. Regularization Built-in ‚Äì Reduces overfitting with L1 and L2 penalties.\n4. Probabilistic Predictions ‚Äì Provides probability scores for classification, enabling threshold tuning.\n5. Works Well with Large Datasets ‚Äì Efficient memory usage and parallel processing.\n\nCons\n1. Complexity ‚Äì More difficult to tune compared to simpler models.\n2. Computationally Intensive ‚Äì Training can be slow on very large datasets.\n3. Sensitive to Hyperparameters ‚Äì Performance depends on careful tuning of learning rate, tree depth, and regularization.\n4. Less Interpretable ‚Äì Decision boundaries may be challenging to interpret compared to simpler models.\n\nTips\n* Optimize Hyperparameters ‚Äì Use grid search or Bayesian optimization for tuning.\n* Use Early Stopping ‚Äì Stop training if performance ceases to improve on validation data.\n* Scale Features if Needed ‚Äì Although XGBoost can handle unscaled data, standardization might improve performance.\n* Leverage Feature Importance ‚Äì Identify and remove less relevant features to improve efficiency.\n* Adjust Decision Thresholds ‚Äì Fine-tune the probability threshold to balance precision and recall for your specific task.\n\nUseful Articles and Videos\n* XGBoost Official Documentation\n* XGBoost for Classification ‚Äì Machine Learning Mastery\n* Understanding XGBoost ‚Äì Analytics Vidhya\n* XGBoost Explained for Classification ‚Äì YouTube\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine Learning projects/3b_Classification_Model/data/readme.html",
    "href": "Machine Learning projects/3b_Classification_Model/data/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "‚Äì Data Overview ‚Äì\nThe bank customer churn dataset is a commonly used dataset for predicting customer churn in the banking industry. It contains information on bank customers who either left the bank or continue to be a customer.\nThe data dictionary: - Customer ID: A unique identifier for each customer - Surname: The customer‚Äôs surname or last name - Credit Score: A numerical value representing the customer‚Äôs credit score - Geography: The country where the customer resides (France, Spain or Germany) - Gender: The customer‚Äôs gender (Male or Female) - Age: The customer‚Äôs age. - Tenure: The number of years the customer has been with the bank - Balance: The customer‚Äôs account balance - NumOfProducts: The number of bank products the customer uses (e.g., savings account, credit card) - HasCrCard: Whether the customer has a credit card (1 = yes, 0 = no) - IsActiveMember: Whether the customer is an active member (1 = yes, 0 = no) - EstimatedSalary: The estimated salary of the customer - Exited: Whether the customer has churned (1 = yes, 0 = no)\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine Learning projects/3a_Regression_Model/ML_Bootcamp_Regression_Template.html#overview",
    "href": "Machine Learning projects/3a_Regression_Model/ML_Bootcamp_Regression_Template.html#overview",
    "title": "Regression Machine Learning Model üß† üìà",
    "section": "Overview",
    "text": "Overview\nXGBoost Regression is a supervised machine learning algorithm that builds an ensemble of decision trees to predict continuous values. It is optimized for speed and performance using gradient boosting techniques.\n\n\n\nDefinition\nXGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:\n\nBoosting Trees: A collection of decision trees built sequentially to reduce errors.\n\nGradient Descent Optimization: Adjusts model weights using the gradient of a loss function.\n\nRegularization: Controls model complexity to prevent overfitting.\n\nFor regression, XGBoost predicts continuous values by minimizing a chosen loss function, commonly Mean Squared Error (MSE) or Mean Absolute Error (MAE).\n\n\n\nKey Concepts\n1. Boosting Mechanism:\n- Unlike a single decision tree, XGBoost builds multiple trees in sequence.\n- Each new tree corrects the errors of the previous ones by focusing on residuals.\n\nLoss Functions:\n\nDetermines how errors are measured and minimized.\n\nCommon choices:\n\nMean Squared Error (MSE) ‚Äì Penalizes larger errors more heavily.\n\nMean Absolute Error (MAE) ‚Äì Treats all errors equally.\n\nHuber Loss ‚Äì A mix of MSE and MAE to handle outliers.\n\n\nRegularization Techniques:\n\nPrevents overfitting by adding penalties to complex models.\n\nL1 Regularization (Lasso) ‚Äì Shrinks coefficients, promoting sparsity.\n\nL2 Regularization (Ridge) ‚Äì Penalizes large coefficients to reduce variance.\n\nFeature Importance & Selection:\n\nXGBoost ranks features by importance, aiding feature selection.\n\nCan be used to eliminate redundant or irrelevant features.\n\n\n\n\n\nPros\n1. High Performance ‚Äì Optimized for speed, scalability, and efficiency.\n2. Handles Missing Data ‚Äì Automatically learns how to deal with missing values.\n3. Regularization Built-in ‚Äì Reduces overfitting with L1 and L2 penalties.\n4. Works Well with Large Datasets ‚Äì Efficient memory usage and parallel processing.\n\n\n\nCons\n1. Complexity ‚Äì More difficult to tune compared to simpler models.\n2. Computationally Intensive ‚Äì Training can be slow on very large datasets.\n3. Sensitive to Hyperparameters ‚Äì Performance depends on careful tuning of learning rate, tree depth, and regularization.\n\n\n\nTips\n* Optimize Hyperparameters ‚Äì Use grid search or Bayesian optimization for tuning.\n* Use Early Stopping ‚Äì Stops training if performance stops improving on validation data.\n* Scale Features if Needed ‚Äì Although XGBoost can handle unscaled data, standardization may help in some cases.\n* Leverage Feature Importance ‚Äì Identify and remove less relevant features to improve efficiency.\n\n\n\nUseful Articles and Videos\n* XGBoost Official Documentation\n* XGBoost for Regression ‚Äì Machine Learning Mastery\n* Understanding XGBoost ‚Äì Analytics Vidhya\n* XGBoost Explained ‚Äì YouTube"
  },
  {
    "objectID": "Machine Learning projects/3a_Regression_Model/ML_Bootcamp_Regression_Template.html#import-datalibraries",
    "href": "Machine Learning projects/3a_Regression_Model/ML_Bootcamp_Regression_Template.html#import-datalibraries",
    "title": "Regression Machine Learning Model üß† üìà",
    "section": "Import Data/Libraries",
    "text": "Import Data/Libraries\n\n!pip install lets_plot\n\nRequirement already satisfied: lets_plot in /usr/local/lib/python3.11/dist-packages (4.6.1)\nRequirement already satisfied: pypng in /usr/local/lib/python3.11/dist-packages (from lets_plot) (0.20220715.0)\nRequirement already satisfied: palettable in /usr/local/lib/python3.11/dist-packages (from lets_plot) (3.3.3)\n\n\n\n# needed libraries for Regression models\nimport pandas as pd\nfrom sklearn import tree\nimport xgboost as xgb\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, mean_squared_error\nfrom sklearn.model_selection import cross_val_score,  train_test_split , KFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nimport lets_plot as lp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport kagglehub\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# foundation dataset\n#### If this gives an error go into the Data folder in GitHub and click on the baseball data csv and then click \"Raw\" (underneath history in the upper righthand corner) then copy that url to replace the \"data_raw_url\"\ndata_raw_url = \"https://raw.githubusercontent.com/BYUIDSS/DSS-ML-Bootcamp/refs/heads/main/3a_Regression_Model/Data/nyc-east-river-bicycle-counts.csv?token=GHSAT0AAAAAAC6JBYNOYS7FRMQ2VJSQ5HZ6Z6N4UCQ\"\nbicycle_df = pd.read_csv(data_raw_url)"
  },
  {
    "objectID": "Machine Learning projects/3a_Regression_Model/ML_Bootcamp_Regression_Template.html#explore-visualize-and-understand-the-data",
    "href": "Machine Learning projects/3a_Regression_Model/ML_Bootcamp_Regression_Template.html#explore-visualize-and-understand-the-data",
    "title": "Regression Machine Learning Model üß† üìà",
    "section": "Explore, Visualize and Understand the Data",
    "text": "Explore, Visualize and Understand the Data\n\nbicycle_df.head(10)\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nDate\nDay\nHigh Temp (¬∞F)\nLow Temp (¬∞F)\nPrecipitation\nBrooklyn Bridge\nManhattan Bridge\nWilliamsburg Bridge\nQueensboro Bridge\nTotal\n\n\n\n\n0\n0\n2016-04-01 00:00:00\n2016-04-01 00:00:00\n78.1\n66.0\n0.01\n1704.0\n3126\n4115.0\n2552.0\n11497\n\n\n1\n1\n2016-04-02 00:00:00\n2016-04-02 00:00:00\n55.0\n48.9\n0.15\n827.0\n1646\n2565.0\n1884.0\n6922\n\n\n2\n2\n2016-04-03 00:00:00\n2016-04-03 00:00:00\n39.9\n34.0\n0.09\n526.0\n1232\n1695.0\n1306.0\n4759\n\n\n3\n3\n2016-04-04 00:00:00\n2016-04-04 00:00:00\n44.1\n33.1\n0.47 (S)\n521.0\n1067\n1440.0\n1307.0\n4335\n\n\n4\n4\n2016-04-05 00:00:00\n2016-04-05 00:00:00\n42.1\n26.1\n0\n1416.0\n2617\n3081.0\n2357.0\n9471\n\n\n5\n5\n2016-04-06 00:00:00\n2016-04-06 00:00:00\n45.0\n30.0\n0\n1885.0\n3329\n3856.0\n2849.0\n11919\n\n\n6\n6\n2016-04-07 00:00:00\n2016-04-07 00:00:00\n57.0\n53.1\n0.09\n1276.0\n2581\n3282.0\n2457.0\n9596\n\n\n7\n7\n2016-04-08 00:00:00\n2016-04-08 00:00:00\n46.9\n44.1\n0.01\n1982.0\n3455\n4113.0\n3194.0\n12744\n\n\n8\n8\n2016-04-09 00:00:00\n2016-04-09 00:00:00\n43.0\n37.9\n0.09\n504.0\n997\n1507.0\n1502.0\n4510\n\n\n9\n9\n2016-04-10 00:00:00\n2016-04-10 00:00:00\n48.9\n30.9\n0\n1447.0\n2387\n3132.0\n2160.0\n9126\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# try info\n\n\n# investigate describe()"
  },
  {
    "objectID": "Machine Learning projects/3a_Regression_Model/ML_Bootcamp_Regression_Template.html#feature-enginnering-and-data-augmentation",
    "href": "Machine Learning projects/3a_Regression_Model/ML_Bootcamp_Regression_Template.html#feature-enginnering-and-data-augmentation",
    "title": "Regression Machine Learning Model üß† üìà",
    "section": "Feature Enginnering and Data Augmentation",
    "text": "Feature Enginnering and Data Augmentation\n\nData Augmentation\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\nFeature Engineering\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns."
  },
  {
    "objectID": "Machine Learning projects/3a_Regression_Model/ML_Bootcamp_Regression_Template.html#machine-learning-model",
    "href": "Machine Learning projects/3a_Regression_Model/ML_Bootcamp_Regression_Template.html#machine-learning-model",
    "title": "Regression Machine Learning Model üß† üìà",
    "section": "Machine Learning Model",
    "text": "Machine Learning Model\n\nSplit the data to train and test\n\n\nCreate the model\n\n\nTrain the model\n\n\nMake predictions\n\nHyperparameter Search\n\n# Hint: google GridSearchCV()\n\n\n\n\nEvaluate the Model\nMSE (Mean Squared Error) ‚Äì The average of the squared differences between the predicted and actual values.\nExample: If the squared errors for three predictions are 4, 9, and 1, then MSE = (4 + 9 + 1) / 3 ‚âà 4.67.\nRMSE (Root Mean Squared Error) ‚Äì The square root of the MSE, providing an error measure in the same unit as the target variable.\nExample: With an MSE of 4.67, RMSE = ‚àö4.67 ‚âà 2.16.\nMAE (Mean Absolute Error) ‚Äì The average of the absolute differences between the predicted and actual values.\nExample: If the absolute errors for three predictions are 2, 3, and 1, then MAE = (2 + 3 + 1) / 3 = 2.\nR-squared (Coefficient of Determination) ‚Äì The proportion of the variance in the dependent variable that is explained by the independent variables.\nExample: An R-squared value of 0.8 indicates that 80% of the variability in the output is explained by the model, with the remaining 20% unexplained.\n\n# Evaluate the model using regression metrics\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mse)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\n\n# Calculate R-squared (R2)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint('Mean Squared Error (MSE):', mse)\nprint('Root Mean Squared Error (RMSE):', rmse)\nprint('Mean Absolute Error (MAE):', mae)\nprint('R-squared (R2):', r2)"
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "250_projects.html",
    "href": "250_projects.html",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 0\nProject 1\nProject 2\nProject 3\nProject 4\nProject 5\nProject 6"
  },
  {
    "objectID": "250_projects.html#repo-for-all-my-projects",
    "href": "250_projects.html#repo-for-all-my-projects",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 0\nProject 1\nProject 2\nProject 3\nProject 4\nProject 5\nProject 6"
  }
]