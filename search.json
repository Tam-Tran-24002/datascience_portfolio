[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Tam Tran’s Resume",
    "section": "",
    "text": "minhtamtran.work@gmail.com | 208-671-3212 | https://www.linkedin.com/in/tam-tran-b37a9123b/ |\nhttps://github.com/Tam-Tran-24002\nTam Tran is an aspiring Data Scientist currently pursuing a Bachelor of Science in Data Science with a minor in Business Analytics and a Cloud Computing certificate. With a growing interest in data analytics, machine learning, and cloud computing, Tam is passionate about solving real-world problems through data. Eager to learn and gain hands-on experience, Tam has proficiency in Python, SQL, R, Excel, Tableau, and machine learning models. Focused on enhancing skills in data visualization, data analytics, predictive modeling, and database management, Tam is actively seeking opportunities to apply and further develop expertise in a dynamic business environment."
  },
  {
    "objectID": "resume.html#tutor-data-science-lab-tutoring-center",
    "href": "resume.html#tutor-data-science-lab-tutoring-center",
    "title": "Tam Tran’s Resume",
    "section": "Tutor | Data Science Lab & Tutoring Center",
    "text": "Tutor | Data Science Lab & Tutoring Center\nRexburg, Idaho | January 2025 - Present\nBrigham Young University - Idaho\n• Tutor 200+ students in Data Intuition and Insights (Tableau), Data Science Programming (Python, SQL, Machine Learning), and Programming with Functions\n• Break down complex data science concepts for both technical and non-technical students, using clear explanations and practical examples, leading to increased comprehension and engagement"
  },
  {
    "objectID": "resume.html#teaching-assistant",
    "href": "resume.html#teaching-assistant",
    "title": "Tam Tran’s Resume",
    "section": "Teaching Assistant",
    "text": "Teaching Assistant\nRexburg, Idaho | September 2024 - Present\nBrigham Young University - Idaho\n• Provide academic support to 250+ students in Introduction to Database, Intro to Programming, and Data Science Programming courses through tutoring, grading, and constructive feedback\n• Collaborate with professors to refine course materials, enhancing student engagement and improving learning outcomes\n• Facilitate lab sessions and office hours to reinforce key concepts and assist with coding assignments"
  },
  {
    "objectID": "resume.html#new-student-mentor",
    "href": "resume.html#new-student-mentor",
    "title": "Tam Tran’s Resume",
    "section": "New Student Mentor",
    "text": "New Student Mentor\nRexburg, Idaho | September 2024 - December 2024\nBrigham Young University - Idaho\n• Guided weekly gatherings for 40+ new students, providing support for academic and personal challenges\n• Boosted student engagement by 30% through 150+ one-on-one and group consultancy meetings\n• Managed attendance for 40+ students and coordinate 5+ resources to enhance learning outcomes and student success\n• Strengthened course organization and lesson planning to ensure students achieve academic success"
  },
  {
    "objectID": "resume.html#product-manager",
    "href": "resume.html#product-manager",
    "title": "Tam Tran’s Resume",
    "section": "Product Manager",
    "text": "Product Manager\nHo Chi Minh, Vietnam | February 2023 - February 2024\nAIESEC in Vietnam\n• Led a team of 8 members, delivering top-tier customer service for Global Volunteer projects and generating over $1550 in revenue\n• Analyzed 500+ lead and customer data points to generate actionable insights, improve customer experience, and optimize business strategy\n• Collaborated with cross-functional teams across 10+ countries to enhance the customer journey and drive seamless service delivery\n• Achieved an 87.5% retention rate of members through weekly performance data analysis and the implementation of data-driven strategies\n• Awarded Function Excellence with a 90% goal achievement rate"
  },
  {
    "objectID": "resume.html#marketing-team-leader-bac---international-business-club",
    "href": "resume.html#marketing-team-leader-bac---international-business-club",
    "title": "Tam Tran’s Resume",
    "section": "Marketing Team Leader | BAC - International Business Club",
    "text": "Marketing Team Leader | BAC - International Business Club\nHo Chi Minh, Vietnam | December 2021 - December 2022\nTon Duc Thang University\n• Directed and improved a 6-member team by analyzing performance metrics and implementing weekly workflow adjustments, resulting in a 20% increase in operational efficiency and streamlined processes\n• Trained 5 team members on advanced Excel techniques, resulting in a 30% reduction in operational processing time and significantly boosting overall team performance"
  },
  {
    "objectID": "resume.html#machine-learning-bootcamp",
    "href": "resume.html#machine-learning-bootcamp",
    "title": "Tam Tran’s Resume",
    "section": "Machine Learning – Bootcamp",
    "text": "Machine Learning – Bootcamp\nJanuary 2024 - Present\n• Deployed Random Forest, Decision Trees, K-Nearest Neighbors (KNN), and XGBoost on real-world datasets, achieving 90% accuracy\n• Optimized model performance through feature engineering, train-test split, and categorical encoding\n• Extracted actionable insights to support data-driven decisions and enhance model interpretability"
  },
  {
    "objectID": "Machine_Learning_Projects/1_KNN_Model/readme.html",
    "href": "Machine_Learning_Projects/1_KNN_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "&lt;!DOCTYPE html&gt;\n\n\n\n\n\n\nreadme – Tam Tran - Data Science Portfolio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  \n\n\n\n\n\n\n\n\n\n\n  &lt;div class=\"navbar-brand-container mx-auto\"&gt;\n&lt;a class=\"navbar-brand\" href=\"../../index.html\"&gt;\n&lt;span class=\"navbar-title\"&gt;Tam Tran - Data Science Portfolio&lt;/span&gt;\n&lt;/a&gt;\n\n  &lt;div id=\"quarto-search\" class=\"\" title=\"Search\"&gt;&lt;/div&gt;\n      &lt;button class=\"navbar-toggler\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#navbarCollapse\" aria-controls=\"navbarCollapse\" role=\"menu\" aria-expanded=\"false\" aria-label=\"Toggle navigation\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\"&gt;\n \n\n        &lt;ul class=\"navbar-nav navbar-nav-scroll ms-auto\"&gt;\n\n Home\n\n\n My Resume\n\n\n Machine Learning projects\n\n\n      &lt;/div&gt; &lt;!-- /navcollapse --&gt;\n        &lt;div class=\"quarto-navbar-tools\"&gt;\n\n\n Back to top",
    "crumbs": [
      "Machine Learning projects",
      "KNN Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/4_Classification_Model/readme.html",
    "href": "Machine_Learning_Projects/4_Classification_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "&lt;!DOCTYPE html&gt;\n\n\n\n\n\n\nreadme – Tam Tran - Data Science Portfolio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  \n\n\n\n\n\n\n\n\n\n\n  &lt;div class=\"navbar-brand-container mx-auto\"&gt;\n&lt;a class=\"navbar-brand\" href=\"../../index.html\"&gt;\n&lt;span class=\"navbar-title\"&gt;Tam Tran - Data Science Portfolio&lt;/span&gt;\n&lt;/a&gt;\n\n  &lt;div id=\"quarto-search\" class=\"\" title=\"Search\"&gt;&lt;/div&gt;\n      &lt;button class=\"navbar-toggler\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#navbarCollapse\" aria-controls=\"navbarCollapse\" role=\"menu\" aria-expanded=\"false\" aria-label=\"Toggle navigation\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\"&gt;\n \n\n        &lt;ul class=\"navbar-nav navbar-nav-scroll ms-auto\"&gt;\n\n Home\n\n\n My Resume\n\n\n Machine Learning projects\n\n\n      &lt;/div&gt; &lt;!-- /navcollapse --&gt;\n        &lt;div class=\"quarto-navbar-tools\"&gt;\n\n\n Back to top",
    "crumbs": [
      "Machine Learning projects",
      "Classification Model (XGBoost)"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/readme.html",
    "href": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "&lt;!DOCTYPE html&gt;\n\n\n\n\n\n\nreadme – Tam Tran - Data Science Portfolio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  \n\n\n\n\n\n\n\n\n\n\n  &lt;div class=\"navbar-brand-container mx-auto\"&gt;\n&lt;a class=\"navbar-brand\" href=\"../../index.html\"&gt;\n&lt;span class=\"navbar-title\"&gt;Tam Tran - Data Science Portfolio&lt;/span&gt;\n&lt;/a&gt;\n\n  &lt;div id=\"quarto-search\" class=\"\" title=\"Search\"&gt;&lt;/div&gt;\n      &lt;button class=\"navbar-toggler\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#navbarCollapse\" aria-controls=\"navbarCollapse\" role=\"menu\" aria-expanded=\"false\" aria-label=\"Toggle navigation\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\"&gt;\n \n\n        &lt;ul class=\"navbar-nav navbar-nav-scroll ms-auto\"&gt;\n\n Home\n\n\n My Resume\n\n\n Machine Learning projects\n\n\n      &lt;/div&gt; &lt;!-- /navcollapse --&gt;\n        &lt;div class=\"quarto-navbar-tools\"&gt;\n\n\n Back to top",
    "crumbs": [
      "Machine Learning projects",
      "Decision Tree & Random Forests Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#overview",
    "href": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#overview",
    "title": "\nRegression Machine Learning Model 🧠 📈\n",
    "section": "\nOverview\n",
    "text": "Overview\n\n\nXGBoost Regression is a supervised machine learning algorithm that builds an ensemble of decision trees to predict continuous values. It is optimized for speed and performance using gradient boosting techniques.\n\n\n\n\n\n\n\n\n\nDefinition XGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:\n\n\n\nBoosting Trees: A collection of decision trees built sequentially to reduce errors.\n\n\nGradient Descent Optimization: Adjusts model weights using the gradient of a loss function.\n\n\nRegularization: Controls model complexity to prevent overfitting.\n\n\n\nFor regression, XGBoost predicts continuous values by minimizing a chosen loss function, commonly Mean Squared Error (MSE) or Mean Absolute Error (MAE).\n\n\n\n\n\n\n\n\n\nKey Concepts 1. Boosting Mechanism: - Unlike a single decision tree, XGBoost builds multiple trees in sequence. - Each new tree corrects the errors of the previous ones by focusing on residuals.\n\n\n\nLoss Functions:\n\n\nDetermines how errors are measured and minimized.\n\n\nCommon choices:\n\n\nMean Squared Error (MSE) – Penalizes larger errors more heavily.\n\n\nMean Absolute Error (MAE) – Treats all errors equally.\n\n\nHuber Loss – A mix of MSE and MAE to handle outliers.\n\n\n\n\n\n\nRegularization Techniques:\n\n\nPrevents overfitting by adding penalties to complex models.\n\n\nL1 Regularization (Lasso) – Shrinks coefficients, promoting sparsity.\n\n\nL2 Regularization (Ridge) – Penalizes large coefficients to reduce variance.\n\n\n\n\nFeature Importance & Selection:\n\n\nXGBoost ranks features by importance, aiding feature selection.\n\n\nCan be used to eliminate redundant or irrelevant features.\n\n\n\n\n\n\n\n\n\n\n\n\nPros 1. High Performance – Optimized for speed, scalability, and efficiency. 2. Handles Missing Data – Automatically learns how to deal with missing values. 3. Regularization Built-in – Reduces overfitting with L1 and L2 penalties. 4. Works Well with Large Datasets – Efficient memory usage and parallel processing.\n\n\n\n\n\n\n\n\n\nCons 1. Complexity – More difficult to tune compared to simpler models. 2. Computationally Intensive – Training can be slow on very large datasets. 3. Sensitive to Hyperparameters – Performance depends on careful tuning of learning rate, tree depth, and regularization.\n\n\n\n\n\n\n\n\n\nTips * Optimize Hyperparameters – Use grid search or Bayesian optimization for tuning. * Use Early Stopping – Stops training if performance stops improving on validation data. * Scale Features if Needed – Although XGBoost can handle unscaled data, standardization may help in some cases. * Leverage Feature Importance – Identify and remove less relevant features to improve efficiency.\n\n\n\n\n\n\n\n\n\nUseful Articles and Videos * XGBoost Official Documentation * XGBoost for Regression – Machine Learning Mastery * Understanding XGBoost – Analytics Vidhya * XGBoost Explained – YouTube"
  },
  {
    "objectID": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#import-datalibraries",
    "href": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#import-datalibraries",
    "title": "\nRegression Machine Learning Model 🧠 📈\n",
    "section": "\nImport Data/Libraries\n",
    "text": "Import Data/Libraries\n\n\n\n!pip install lets_plot\n\n\nRequirement already satisfied: lets_plot in /usr/local/lib/python3.11/dist-packages (4.6.2)\nRequirement already satisfied: pypng in /usr/local/lib/python3.11/dist-packages (from lets_plot) (0.20220715.0)\nRequirement already satisfied: palettable in /usr/local/lib/python3.11/dist-packages (from lets_plot) (3.3.3)\n\n\n\n\n# needed libraries for Regression models\nimport pandas as pd\nfrom sklearn import tree\nimport xgboost as xgb\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, mean_squared_error\nfrom sklearn.model_selection import cross_val_score,  train_test_split , KFold\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nimport lets_plot as lp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport kagglehub\n\n# foundation dataset\ndata_raw_url = \"https://raw.githubusercontent.com/BYUIDSS/DSS-ML-Bootcamp/refs/heads/main/3a_Regression_Model/Data/nyc-east-river-bicycle-counts.csv?token=GHSAT0AAAAAAC7UQH3WXY3OI4SLVS6O3K6SZ7KFJNQ\"\nbicycle_df = pd.read_csv(data_raw_url)"
  },
  {
    "objectID": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#explore-visualize-and-understand-the-data",
    "href": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#explore-visualize-and-understand-the-data",
    "title": "\nRegression Machine Learning Model 🧠 📈\n",
    "section": "\nExplore, Visualize and Understand the Data\n",
    "text": "Explore, Visualize and Understand the Data\n\n\n\nbicycle_df.head(10)\n\n\n\n&lt;div&gt;\n\n\n\n\n\n\nUnnamed: 0\n\n\nDate\n\n\nDay\n\n\nHigh Temp (°F)\n\n\nLow Temp (°F)\n\n\nPrecipitation\n\n\nBrooklyn Bridge\n\n\nManhattan Bridge\n\n\nWilliamsburg Bridge\n\n\nQueensboro Bridge\n\n\nTotal\n\n\n\n\n\n\n0\n\n\n0\n\n\n2016-04-01 00:00:00\n\n\n2016-04-01 00:00:00\n\n\n78.1\n\n\n66.0\n\n\n0.01\n\n\n1704.0\n\n\n3126\n\n\n4115.0\n\n\n2552.0\n\n\n11497\n\n\n\n\n1\n\n\n1\n\n\n2016-04-02 00:00:00\n\n\n2016-04-02 00:00:00\n\n\n55.0\n\n\n48.9\n\n\n0.15\n\n\n827.0\n\n\n1646\n\n\n2565.0\n\n\n1884.0\n\n\n6922\n\n\n\n\n2\n\n\n2\n\n\n2016-04-03 00:00:00\n\n\n2016-04-03 00:00:00\n\n\n39.9\n\n\n34.0\n\n\n0.09\n\n\n526.0\n\n\n1232\n\n\n1695.0\n\n\n1306.0\n\n\n4759\n\n\n\n\n3\n\n\n3\n\n\n2016-04-04 00:00:00\n\n\n2016-04-04 00:00:00\n\n\n44.1\n\n\n33.1\n\n\n0.47 (S)\n\n\n521.0\n\n\n1067\n\n\n1440.0\n\n\n1307.0\n\n\n4335\n\n\n\n\n4\n\n\n4\n\n\n2016-04-05 00:00:00\n\n\n2016-04-05 00:00:00\n\n\n42.1\n\n\n26.1\n\n\n0\n\n\n1416.0\n\n\n2617\n\n\n3081.0\n\n\n2357.0\n\n\n9471\n\n\n\n\n5\n\n\n5\n\n\n2016-04-06 00:00:00\n\n\n2016-04-06 00:00:00\n\n\n45.0\n\n\n30.0\n\n\n0\n\n\n1885.0\n\n\n3329\n\n\n3856.0\n\n\n2849.0\n\n\n11919\n\n\n\n\n6\n\n\n6\n\n\n2016-04-07 00:00:00\n\n\n2016-04-07 00:00:00\n\n\n57.0\n\n\n53.1\n\n\n0.09\n\n\n1276.0\n\n\n2581\n\n\n3282.0\n\n\n2457.0\n\n\n9596\n\n\n\n\n7\n\n\n7\n\n\n2016-04-08 00:00:00\n\n\n2016-04-08 00:00:00\n\n\n46.9\n\n\n44.1\n\n\n0.01\n\n\n1982.0\n\n\n3455\n\n\n4113.0\n\n\n3194.0\n\n\n12744\n\n\n\n\n8\n\n\n8\n\n\n2016-04-09 00:00:00\n\n\n2016-04-09 00:00:00\n\n\n43.0\n\n\n37.9\n\n\n0.09\n\n\n504.0\n\n\n997\n\n\n1507.0\n\n\n1502.0\n\n\n4510\n\n\n\n\n9\n\n\n9\n\n\n2016-04-10 00:00:00\n\n\n2016-04-10 00:00:00\n\n\n48.9\n\n\n30.9\n\n\n0\n\n\n1447.0\n\n\n2387\n\n\n3132.0\n\n\n2160.0\n\n\n9126\n\n\n\n\n\n&lt;div class=\"colab-df-buttons\"&gt;\n\n&lt;button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40a66470-621f-4d33-9ad1-ac3991723f77')\" title=\"Convert this dataframe to an interactive table.\" style=\"display:none;\"&gt;\n   \n\n&lt;script&gt;\n  const buttonEl =\n    document.querySelector('#df-40a66470-621f-4d33-9ad1-ac3991723f77 button.colab-df-convert');\n  buttonEl.style.display =\n    google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n  async function convertToInteractive(key) {\n    const element = document.querySelector('#df-40a66470-621f-4d33-9ad1-ac3991723f77');\n    const dataTable =\n      await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                [key], {});\n    if (!dataTable) return;\n\n    const docLinkHtml = 'Like what you see? Visit the ' +\n      '&lt;a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;'\n      + ' to learn more about interactive tables.';\n    element.innerHTML = '';\n    dataTable['output_type'] = 'display_data';\n    await google.colab.output.renderOutput(dataTable, element);\n    const docLink = document.createElement('div');\n    docLink.innerHTML = docLinkHtml;\n    element.appendChild(docLink);\n  }\n&lt;/script&gt;\n\n\n\n\n  \n\n\n\n\n\n&lt;/div&gt;"
  },
  {
    "objectID": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#feature-enginnering-and-data-augmentation",
    "href": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#feature-enginnering-and-data-augmentation",
    "title": "\nRegression Machine Learning Model 🧠 📈\n",
    "section": "\nFeature Enginnering and Data Augmentation\n",
    "text": "Feature Enginnering and Data Augmentation\n\n\n\nData Augmentation\n\n\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\n\n\nFeature Engineering\n\n\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns.\n\n\n\nbicycle_df1 = pd.get_dummies(bicycle_df, columns=[\"Date\", \"Day\",\"Precipitation\"])\n\n\n\n\nX = bicycle_df1.drop(columns=[\"Brooklyn Bridge\",    \"Manhattan Bridge\", \"Williamsburg Bridge\",  \"Queensboro Bridge\", \"Total\"])\ny = bicycle_df1[\"Total\"]\n\n\n\n\nbicycle_df1.describe()\n\n\n\n&lt;div&gt;\n\n\n\n\n\n\nUnnamed: 0\n\n\nHigh Temp (°F)\n\n\nLow Temp (°F)\n\n\nBrooklyn Bridge\n\n\nManhattan Bridge\n\n\nWilliamsburg Bridge\n\n\nQueensboro Bridge\n\n\nTotal\n\n\n\n\n\n\ncount\n\n\n210.000000\n\n\n210.000000\n\n\n210.000000\n\n\n210.000000\n\n\n210.000000\n\n\n210.000000\n\n\n210.000000\n\n\n210.000000\n\n\n\n\nmean\n\n\n104.500000\n\n\n60.580000\n\n\n46.413333\n\n\n2269.633333\n\n\n4049.533333\n\n\n4862.466667\n\n\n3352.866667\n\n\n14534.500000\n\n\n\n\nstd\n\n\n60.765944\n\n\n11.183223\n\n\n9.522796\n\n\n981.237786\n\n\n1704.731356\n\n\n1814.039499\n\n\n1099.254419\n\n\n5569.173496\n\n\n\n\nmin\n\n\n0.000000\n\n\n39.900000\n\n\n26.100000\n\n\n504.000000\n\n\n997.000000\n\n\n1440.000000\n\n\n1306.000000\n\n\n4335.000000\n\n\n\n\n25%\n\n\n52.250000\n\n\n55.000000\n\n\n44.100000\n\n\n1447.000000\n\n\n2617.000000\n\n\n3282.000000\n\n\n2457.000000\n\n\n9596.000000\n\n\n\n\n50%\n\n\n104.500000\n\n\n62.100000\n\n\n46.900000\n\n\n2379.500000\n\n\n4165.000000\n\n\n5194.000000\n\n\n3477.000000\n\n\n15292.500000\n\n\n\n\n75%\n\n\n156.750000\n\n\n68.000000\n\n\n50.000000\n\n\n3147.000000\n\n\n5309.000000\n\n\n6030.000000\n\n\n4192.000000\n\n\n18315.000000\n\n\n\n\nmax\n\n\n209.000000\n\n\n81.000000\n\n\n66.000000\n\n\n3871.000000\n\n\n6951.000000\n\n\n7834.000000\n\n\n5032.000000\n\n\n23318.000000\n\n\n\n\n\n&lt;div class=\"colab-df-buttons\"&gt;\n\n&lt;button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4baee9d7-1e95-4299-9935-08038a2510dd')\" title=\"Convert this dataframe to an interactive table.\" style=\"display:none;\"&gt;\n   \n\n&lt;script&gt;\n  const buttonEl =\n    document.querySelector('#df-4baee9d7-1e95-4299-9935-08038a2510dd button.colab-df-convert');\n  buttonEl.style.display =\n    google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n  async function convertToInteractive(key) {\n    const element = document.querySelector('#df-4baee9d7-1e95-4299-9935-08038a2510dd');\n    const dataTable =\n      await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                [key], {});\n    if (!dataTable) return;\n\n    const docLinkHtml = 'Like what you see? Visit the ' +\n      '&lt;a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;'\n      + ' to learn more about interactive tables.';\n    element.innerHTML = '';\n    dataTable['output_type'] = 'display_data';\n    await google.colab.output.renderOutput(dataTable, element);\n    const docLink = document.createElement('div');\n    docLink.innerHTML = docLinkHtml;\n    element.appendChild(docLink);\n  }\n&lt;/script&gt;\n\n\n\n\n  \n\n\n\n\n\n&lt;/div&gt;"
  },
  {
    "objectID": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#machine-learning-model",
    "href": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#machine-learning-model",
    "title": "\nRegression Machine Learning Model 🧠 📈\n",
    "section": "\nMachine Learning Model\n",
    "text": "Machine Learning Model\n\n\n\nSplit the data to train and test\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n\n\n\n\nCreate the model\n\n\n\nparams = {\n    'objective': 'reg:squarederror',  # Regression task\n    'max_depth': 6,                   # Maximum depth of trees\n    'learning_rate': 0.1,             # Learning rate\n    'n_estimators': 100,              # Number of boosting rounds\n}\n\n\n\n\n\nTrain the model\n\n\n\nmodel = xgb.train(params, dtrain, num_boost_round=100)\n\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:53:09] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n\n\n\nMake predictions\n\n\n\n# Predicting the Test set results\ny_pred = model.predict(dtest)\n\n\n\n\nHyperparameter Search\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid to search\nparam_grid = {\n    'max_depth': [3, 4, 5, 6, 7],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [50, 100, 200],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n# Create the XGBoost model\nxgb_model = xgb.XGBRegressor(objective='reg:squarederror', seed=42)\n\n# Create and Configure GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=xgb_model,\n    param_grid=param_grid,\n    scoring='neg_mean_squared_error',  # Since MSE should be minimized, we use its negative (because GridSearchCV maximizes the score)\n    cv=5,  # Uses 5-fold cross-validation to evaluate each parameter set\n    verbose=1, # Prints progress updates\n    n_jobs=-1 # Uses all available CPU cores for parallel computation, making it faster\n)\n# Fit the Grid Search to Training Data\ngrid_search.fit(X_train, y_train)\n\n# Get the Best Model & Hyperparameters\nprint(\"Best hyperparameters:\", grid_search.best_params_) # The optimal hyperparameter combination\nprint(\"Best score:\", grid_search.best_score_) # The highest cross-validation score (negative MSE)\nbest_model = grid_search.best_estimator_ # The trained XGBoost model with the best hyperparameters\nprint(f\"Grid Search: {best_model}\")\n\n\nFitting 5 folds for each of 405 candidates, totalling 2025 fits\n\n\n\n\n\n\nEvaluate the Model\n\n\nAccuracy – The percentage of total predictions that are correct. Example: If a spam filter correctly classifies 90 out of 100 emails (whether spam or not), the accuracy is 90%.\n\n\nF1 Score – Out of all the positive predictions, how many were actually correct. Example: If a spam filter predicts 20 emails as spam, but only 15 are actually spam, precision is 15/20 = 75%.\n\n\nRecall Score – Out of all the actual positive cases, how many did the model correctly identify. Example: If there were 25 spam emails in total, and the model correctly identified 15 of them, recall is 15/25 = 60%.\n\n\nPrecision Score – A balance between precision and recall (harmonic mean). Example: If precision is 75% and recall is 60%, F1 score is (2 × 75 × 60) / (75 + 60) = 66.7%.\n\n\n\n# Evaluate the model using regression metrics\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mse)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\n\n# Calculate R-squared (R2)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint('Mean Squared Error (MSE):', mse)\nprint('Root Mean Squared Error (RMSE):', rmse)\nprint('Mean Absolute Error (MAE):', mae)\nprint('R-squared (R2):', r2)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "View K-Nearest Neighbors Model\nView Decision Tree - Random Forest Model\nView Regression Model\nView Classification Model"
  },
  {
    "objectID": "index.html#machine-learning-projects",
    "href": "index.html#machine-learning-projects",
    "title": "about me",
    "section": "",
    "text": "View K-Nearest Neighbors Model\nView Decision Tree - Random Forest Model\nView Regression Model\nView Classification Model"
  },
  {
    "objectID": "machine_learning.html",
    "href": "machine_learning.html",
    "title": "Machine Learning - Bootcamp",
    "section": "",
    "text": "/Machine Learning projects /1_KNN_Model ML_Bootcamp_KNN.html /2_DecisionTree_RandomForests_Model ML_Bootcamp_DecisionTree_RandomForest.html /3_Regression_Model ML_Bootcamp_Regression.html /4_Classification_Model ML_Bootcamp_Classification_XGBoost.html",
    "crumbs": [
      "Machine Learning projects",
      "Machine Learning Projects"
    ]
  },
  {
    "objectID": "machine_learning.html#my-projects",
    "href": "machine_learning.html#my-projects",
    "title": "Machine Learning - Bootcamp",
    "section": "",
    "text": "/Machine Learning projects /1_KNN_Model ML_Bootcamp_KNN.html /2_DecisionTree_RandomForests_Model ML_Bootcamp_DecisionTree_RandomForest.html /3_Regression_Model ML_Bootcamp_Regression.html /4_Classification_Model ML_Bootcamp_Classification_XGBoost.html",
    "crumbs": [
      "Machine Learning projects",
      "Machine Learning Projects"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/3_Regression_Model/readme.html",
    "href": "Machine_Learning_Projects/3_Regression_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "&lt;!DOCTYPE html&gt;\n\n\n\n\n\n\nreadme – Tam Tran - Data Science Portfolio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  \n\n\n\n\n\n\n\n\n\n\n  &lt;div class=\"navbar-brand-container mx-auto\"&gt;\n&lt;a class=\"navbar-brand\" href=\"../../index.html\"&gt;\n&lt;span class=\"navbar-title\"&gt;Tam Tran - Data Science Portfolio&lt;/span&gt;\n&lt;/a&gt;\n\n  &lt;div id=\"quarto-search\" class=\"\" title=\"Search\"&gt;&lt;/div&gt;\n      &lt;button class=\"navbar-toggler\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#navbarCollapse\" aria-controls=\"navbarCollapse\" role=\"menu\" aria-expanded=\"false\" aria-label=\"Toggle navigation\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\"&gt;\n \n\n        &lt;ul class=\"navbar-nav navbar-nav-scroll ms-auto\"&gt;\n\n Home\n\n\n My Resume\n\n\n Machine Learning projects\n\n\n      &lt;/div&gt; &lt;!-- /navcollapse --&gt;\n        &lt;div class=\"quarto-navbar-tools\"&gt;\n\n\n Back to top",
    "crumbs": [
      "Machine Learning projects",
      "Regression Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#overview",
    "href": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#overview",
    "title": "\nDecision Tree Machine Learning Model 💻 🧠\n",
    "section": "\nOverview\n",
    "text": "Overview\n\n\nDecision Tree (DT) is a supervised machine learning algorithm used for both classification and regression tasks. It splits data into branches based on feature values to make predictions.\n\n\n\n\n\n\n\n\n\nDefinition A Decision Tree is a flowchart-like model where data is split into smaller subsets based on feature conditions. The structure consists of:\n\n\n\nRoot Node: The starting point that represents the entire dataset.\n\n\nDecision Nodes: Points where the data is split based on a feature.\n\n\nLeaves: Terminal nodes that provide the final prediction.\n\n\n\nFor classification, a Decision Tree assigns class labels based on feature splits. For regression, it predicts continuous values using the average or mean of data points in each leaf.\n\n\n\n\n\n\n\n\n\nKey Concepts 1. Splitting Criteria: - Determines how the dataset is divided at each step. - Common methods: * Gini Impurity (Classification) – Measures the likelihood of incorrect classification. * Entropy (Classification) – Uses information gain to decide splits. * Mean Squared Error (MSE) (Regression) – Measures variance within nodes.\n\n\n\nTree Depth & Overfitting:\n\n\nDeeper trees fit training data better but may overfit.\n\n\nPruning (removing unnecessary branches) improves generalization.\n\n\n\n\nFeature Importance:\n\n\nDecision Trees rank features by their impact on predictions.\n\n\nHelps in feature selection for other models.\n\n\n\n\nHandling Missing Data:\n\n\nSome implementations allow surrogate splits to handle missing values.\n\n\n\n\n\n\n\n\n\n\n\n\nPros 1. Easy to Understand & Interpret – Can be visualized as a simple flowchart. 2. No Need for Feature Scaling – Works with both categorical and numerical features. 3. Handles Non-Linearity – Can model complex relationships without requiring transformation. 4. Fast for Small Datasets – Training and inference are relatively quick.\n\n\n\n\n\n\n\n\n\nCons 1. Prone to Overfitting – Deep trees can memorize training data, reducing generalization. 2. Unstable to Small Changes – Small variations in data can change the tree structure significantly. 3. Less Efficient on Large Datasets – Computationally expensive for large datasets.\n\n\n\n\n\n\n\n\n\nTips * Limit Tree Depth – Use max_depth to prevent overfitting. * Pruning Techniques – Use pre-pruning (max_leaf_nodes) or post-pruning to remove unnecessary branches. * Use Feature Importance – Identify the most influential features and remove irrelevant ones. * Consider Ensemble Methods – Random Forest or Gradient Boosting improve Decision Trees by reducing variance.\n\n\n\n\n\n\n\n\n\nUseful Articles and Videos * https://www.datacamp.com/tutorial/decision-tree-classification-python * https://www.ibm.com/think/topics/decision-trees * https://www.youtube.com/watch?v=6DlWndLbk90 * https://www.youtube.com/watch?v=ZOiBe-nrmc4"
  },
  {
    "objectID": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#import-datalibraries",
    "href": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#import-datalibraries",
    "title": "\nDecision Tree Machine Learning Model 💻 🧠\n",
    "section": "\nImport Data/Libraries\n",
    "text": "Import Data/Libraries\n\n\n\n!pip install lets_plot\n\n\nCollecting lets_plot\n  Downloading lets_plot-4.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting pypng (from lets_plot)\n  Downloading pypng-0.20220715.0-py3-none-any.whl.metadata (13 kB)\nCollecting palettable (from lets_plot)\n  Downloading palettable-3.3.3-py2.py3-none-any.whl.metadata (3.3 kB)\nDownloading lets_plot-4.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 19.2 MB/s eta 0:00:00\nDownloading palettable-3.3.3-py2.py3-none-any.whl (332 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 332.3/332.3 kB 7.5 MB/s eta 0:00:00\nDownloading pypng-0.20220715.0-py3-none-any.whl (58 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 kB 3.3 MB/s eta 0:00:00\nInstalling collected packages: pypng, palettable, lets_plot\nSuccessfully installed lets_plot-4.6.2 palettable-3.3.3 pypng-0.20220715.0\n\n\n\n\n# needed libraries for Decision Tree models\nimport pandas as pd\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.model_selection import cross_val_score,  train_test_split , KFold\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nimport lets_plot as lp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# foundation dataset\ntitanic_df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n\n# stretch dataset\ncleveland_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data', header=None)\nhungarian_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data', header=None)\nswitzerland_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data', header=None)"
  },
  {
    "objectID": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#explore-visualize-and-understand-the-data",
    "href": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#explore-visualize-and-understand-the-data",
    "title": "\nDecision Tree Machine Learning Model 💻 🧠\n",
    "section": "\nExplore, Visualize and Understand the Data\n",
    "text": "Explore, Visualize and Understand the Data\n\n\n\nsns.countplot(data=titanic_df, x=\"Pclass\", hue=\"Survived\", palette=\"Set2\")\nplt.title(\"Survival Count by Passenger Class\")\nplt.xlabel(\"Passenger Class\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Survived\", labels=[\"No\", \"Yes\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitanic_df.head()\n\n\n\n&lt;div&gt;\n\n\n\n\n\n\nPassengerId\n\n\nSurvived\n\n\nPclass\n\n\nName\n\n\nSex\n\n\nAge\n\n\nSibSp\n\n\nParch\n\n\nTicket\n\n\nFare\n\n\nCabin\n\n\nEmbarked\n\n\n\n\n\n\n0\n\n\n1\n\n\n0\n\n\n3\n\n\nBraund, Mr. Owen Harris\n\n\nmale\n\n\n22.0\n\n\n1\n\n\n0\n\n\nA/5 21171\n\n\n7.2500\n\n\nNaN\n\n\nS\n\n\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\nCumings, Mrs. John Bradley (Florence Briggs Th…\n\n\nfemale\n\n\n38.0\n\n\n1\n\n\n0\n\n\nPC 17599\n\n\n71.2833\n\n\nC85\n\n\nC\n\n\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\nHeikkinen, Miss. Laina\n\n\nfemale\n\n\n26.0\n\n\n0\n\n\n0\n\n\nSTON/O2. 3101282\n\n\n7.9250\n\n\nNaN\n\n\nS\n\n\n\n\n3\n\n\n4\n\n\n1\n\n\n1\n\n\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n\n\nfemale\n\n\n35.0\n\n\n1\n\n\n0\n\n\n113803\n\n\n53.1000\n\n\nC123\n\n\nS\n\n\n\n\n4\n\n\n5\n\n\n0\n\n\n3\n\n\nAllen, Mr. William Henry\n\n\nmale\n\n\n35.0\n\n\n0\n\n\n0\n\n\n373450\n\n\n8.0500\n\n\nNaN\n\n\nS\n\n\n\n\n\n&lt;div class=\"colab-df-buttons\"&gt;\n\n&lt;button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-283a5f21-7b50-4336-99ae-b01e96df8536')\" title=\"Convert this dataframe to an interactive table.\" style=\"display:none;\"&gt;\n   \n\n&lt;script&gt;\n  const buttonEl =\n    document.querySelector('#df-283a5f21-7b50-4336-99ae-b01e96df8536 button.colab-df-convert');\n  buttonEl.style.display =\n    google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n  async function convertToInteractive(key) {\n    const element = document.querySelector('#df-283a5f21-7b50-4336-99ae-b01e96df8536');\n    const dataTable =\n      await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                [key], {});\n    if (!dataTable) return;\n\n    const docLinkHtml = 'Like what you see? Visit the ' +\n      '&lt;a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;'\n      + ' to learn more about interactive tables.';\n    element.innerHTML = '';\n    dataTable['output_type'] = 'display_data';\n    await google.colab.output.renderOutput(dataTable, element);\n    const docLink = document.createElement('div');\n    docLink.innerHTML = docLinkHtml;\n    element.appendChild(docLink);\n  }\n&lt;/script&gt;\n\n\n\n\n  \n\n\n\n\n\n&lt;/div&gt;"
  },
  {
    "objectID": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#feature-enginnering-and-data-augmentation",
    "href": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#feature-enginnering-and-data-augmentation",
    "title": "\nDecision Tree Machine Learning Model 💻 🧠\n",
    "section": "\nFeature Enginnering and Data Augmentation\n",
    "text": "Feature Enginnering and Data Augmentation\n\n\n\nData Augmentation\n\n\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\n\n\nFeature Engineering\n\n\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns.\n\n\n\nconditions = [\n    titanic_df[\"Name\"].str.contains(r'(?i)(Mr)|(Mrs)|(Ms)|(Miss)|(Mlle)', na=False),  # Condition 1: If \"Name\" contains \"Mr\"\n    titanic_df[\"Name\"].str.contains(r'(?i)(Rev)|(Dr)|(General)|(Col)|(Major)|(Capt)', na=False), # Condition 2: If \"Name\" contains \"Mrs\"\n    titanic_df[\"Name\"].str.contains(r'(?i)(Master)|(Countess)|(Jonkheer)|(Don)', na=False) # Condition 3: If \"Name\" contains \"Miss\"\n]\n\noutputs = [\n    1,\n    2,\n    3\n]\n\ntitanic_df[\"Name_group\"] = np.select(conditions, outputs, default=5)\n\n\nUserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  titanic_df[\"Name\"].str.contains(r'(?i)(Mr)|(Mrs)|(Ms)|(Miss)|(Mlle)', na=False),  # Condition 1: If \"Name\" contains \"Mr\"\n&lt;ipython-input-6-1b68d7e0032b&gt;:3: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  titanic_df[\"Name\"].str.contains(r'(?i)(Rev)|(Dr)|(General)|(Col)|(Major)|(Capt)', na=False), # Condition 2: If \"Name\" contains \"Mrs\"\n&lt;ipython-input-6-1b68d7e0032b&gt;:4: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  titanic_df[\"Name\"].str.contains(r'(?i)(Master)|(Countess)|(Jonkheer)|(Don)', na=False) # Condition 3: If \"Name\" contains \"Miss\""
  },
  {
    "objectID": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#machine-learning-model-decision-tree",
    "href": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#machine-learning-model-decision-tree",
    "title": "\nDecision Tree Machine Learning Model 💻 🧠\n",
    "section": "\nMachine Learning Model: Decision Tree\n",
    "text": "Machine Learning Model: Decision Tree\n\n\n\nSplit the data\n\n\n\n# Fitting the model\ntitanic_df2 = pd.get_dummies(titanic_df, columns=['Sex', 'Embarked', 'Name', 'Fare', 'Ticket', 'Cabin'], drop_first=True)\n\n\n\n\nX = titanic_df2.drop('Survived', axis=1)\ny = titanic_df2['Survived']\n\n\n\n\n\nCreate the model\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n\ny.head()\n\n\n\n\n\n\n\n\n\nSurvived\n\n\n\n\n\n\n0\n\n\n0\n\n\n\n\n1\n\n\n1\n\n\n\n\n2\n\n\n1\n\n\n\n\n3\n\n\n1\n\n\n\n\n4\n\n\n0\n\n\n\n\n\ndtype: int64\n\n\n\n\n\nTrain the model\n\n\n\n\nMake predictions\n\n\n\ntitanic_df2.head()\n\n\n\n&lt;div&gt;\n\n\n\n\n\n\nPassengerId\n\n\nSurvived\n\n\nPclass\n\n\nAge\n\n\nSibSp\n\n\nParch\n\n\nName_group\n\n\nSex_male\n\n\nEmbarked_Q\n\n\nEmbarked_S\n\n\n…\n\n\nCabin_E8\n\n\nCabin_F E69\n\n\nCabin_F G63\n\n\nCabin_F G73\n\n\nCabin_F2\n\n\nCabin_F33\n\n\nCabin_F38\n\n\nCabin_F4\n\n\nCabin_G6\n\n\nCabin_T\n\n\n\n\n\n\n0\n\n\n1\n\n\n0\n\n\n3\n\n\n22.0\n\n\n1\n\n\n0\n\n\n1\n\n\nTrue\n\n\nFalse\n\n\nTrue\n\n\n…\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n38.0\n\n\n1\n\n\n0\n\n\n1\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n…\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n26.0\n\n\n0\n\n\n0\n\n\n1\n\n\nFalse\n\n\nFalse\n\n\nTrue\n\n\n…\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n3\n\n\n4\n\n\n1\n\n\n1\n\n\n35.0\n\n\n1\n\n\n0\n\n\n1\n\n\nFalse\n\n\nFalse\n\n\nTrue\n\n\n…\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n4\n\n\n5\n\n\n0\n\n\n3\n\n\n35.0\n\n\n0\n\n\n0\n\n\n1\n\n\nTrue\n\n\nFalse\n\n\nTrue\n\n\n…\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n\n5 rows × 1973 columns\n\n\n&lt;div class=\"colab-df-buttons\"&gt;\n\n&lt;button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f084445-c813-44fc-a792-e3e295183397')\" title=\"Convert this dataframe to an interactive table.\" style=\"display:none;\"&gt;\n   \n\n&lt;script&gt;\n  const buttonEl =\n    document.querySelector('#df-7f084445-c813-44fc-a792-e3e295183397 button.colab-df-convert');\n  buttonEl.style.display =\n    google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n  async function convertToInteractive(key) {\n    const element = document.querySelector('#df-7f084445-c813-44fc-a792-e3e295183397');\n    const dataTable =\n      await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                [key], {});\n    if (!dataTable) return;\n\n    const docLinkHtml = 'Like what you see? Visit the ' +\n      '&lt;a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;'\n      + ' to learn more about interactive tables.';\n    element.innerHTML = '';\n    dataTable['output_type'] = 'display_data';\n    await google.colab.output.renderOutput(dataTable, element);\n    const docLink = document.createElement('div');\n    docLink.innerHTML = docLinkHtml;\n    element.appendChild(docLink);\n  }\n&lt;/script&gt;\n\n\n\n\n  \n\n\n\n\n\n&lt;/div&gt;\n\n\n\n\n\n# Predicting the Test set results\nDT = DecisionTreeClassifier()\nDT.fit(X_train, y_train)\n\n\n\n\n\nDecisionTreeClassifier()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\n\n\nDecisionTreeClassifier\n\n\n\n?Documentation for DecisionTreeClassifieriFitted\n\n\n\nDecisionTreeClassifier()\n\n\n\n\n\n\n\n\n\ny_pred = DT.predict(X_test)\n\n\n\n\n\nEvaluate the Model\n\n\nAccuracy – The percentage of total predictions that are correct. Example: If a spam filter correctly classifies 90 out of 100 emails (whether spam or not), the accuracy is 90%.\n\n\nF1 Score – Out of all the positive predictions, how many were actually correct. Example: If a spam filter predicts 20 emails as spam, but only 15 are actually spam, precision is 15/20 = 75%.\n\n\nRecall Score – Out of all the actual positive cases, how many did the model correctly identify. Example: If there were 25 spam emails in total, and the model correctly identified 15 of them, recall is 15/25 = 60%.\n\n\nPrecision Score – A balance between precision and recall (harmonic mean). Example: If precision is 75% and recall is 60%, F1 score is (2 × 75 × 60) / (75 + 60) = 66.7%.\n\n\nAccuracy: 82.68 %. F1 Score: 0.82 %. Recall Score: 0.83 %. Precision Score: 0.83 %.\n\n\n\n# Accuracy, F1 Score, Recall score, Precision score\naccuracy = accuracy_score(y_test, y_pred)*100\nf1_score = f1_score(y_test, y_pred, average='weighted')\nrecall_score = recall_score(y_test, y_pred, average='weighted')\nprecision_score = precision_score(y_test, y_pred, average='weighted')\n\nprint('Accuracy: ' + str(round(accuracy, 2)) + ' %.')\nprint('F1 Score: ' + str(round(f1_score, 2)) + ' %.')\nprint('Recall Score: ' + str(round(recall_score, 2)) + ' %.')\nprint('Precision Score: ' + str(round(precision_score, 2)) + ' %.')\n\n\nAccuracy: 82.12 %.\nF1 Score: 0.82 %.\nRecall Score: 0.82 %.\nPrecision Score: 0.82 %."
  },
  {
    "objectID": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#overview-of-random-forests",
    "href": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#overview-of-random-forests",
    "title": "\nDecision Tree Machine Learning Model 💻 🧠\n",
    "section": "\nOverview of Random Forests\n",
    "text": "Overview of Random Forests\n\n\nRandom Forest (RF) is a supervised machine learning algorithm used for both classification and regression tasks. It builds multiple decision trees and combines their outputs for better accuracy and stability.\n\n\n\n\n\nDefinition A Random Forest is an ensemble learning method that creates a “forest” of decision trees during training. Instead of relying on a single tree, Random Forest combines the predictions of multiple trees to improve performance. The structure consists of:\n\n\n\nDecision Trees: Individual trees that make predictions based on subsets of data and features.\n\n\nBagging (Bootstrap Aggregating): Each tree is trained on a randomly sampled subset of the data.\n\n\nVoting/Averaging: Combines the predictions from all trees:\n\n\nFor classification, the majority vote decides the class.\n\n\nFor regression, the average of all tree predictions is used. Random Forest reduces the risk of overfitting compared to individual decision trees and works well for both categorical and numerical data.\n\n\n\n\n\n\nKey Concepts\n\n\n\nBagging:\n\n\n\n\nRandom Forest uses bootstrapping to train each tree on a different sample of the data.\n\n\nThis creates diversity among trees, making the model more robust.\n\n\n\n\nFeature Randomness:\n\n\n\n\nAt each split, Random Forest considers a random subset of features rather than all features.\n\n\nThis reduces correlation between trees and improves generalization.\n\n\n\n\nOut-of-Bag (OOB) Error:\n\n\n\n\nTrees not trained on certain data points (left out during bootstrapping) can be used to validate the model.\n\n\nOOB error gives an unbiased estimate of model performance.\n\n\n\n\nFeature Importance:\n\n\n\n\nRandom Forest provides a ranking of feature importance based on how often features are used for splitting across trees.\n\n\nUseful for identifying key predictors in your data. \n\n\n\nPros\n\n\nImproved Accuracy – Combines multiple trees, reducing overfitting. Robust to Noise – Handles outliers and noisy data better than individual trees. Handles Large Datasets – Can scale well with more data. Feature Selection – Provides insights into the importance of features. No Need for Feature Scaling – Works with unscaled data, both numerical and categorical. \n\n\nCons\n\n\nLess Interpretable – Harder to visualize compared to a single decision tree. Computationally Intensive – Training many trees can take time and memory for large datasets. Overfitting Risk with Too Many Trees – Although rare, excessive trees might still overfit without tuning. Slower Inference – Predictions may take longer because they aggregate results from multiple trees. \n\n\nTips\n\n\n\nTune n_estimators – Adjust the number of trees to balance accuracy and computational cost.\n\n\nLimit Tree Depth – Use max_depth to avoid overfitting while maintaining performance.\n\n\nOptimize Feature Subset Size – Use max_features to control how many features each tree considers at a split.\n\n\nUse Feature Importance – Rank and prioritize the most important features in your dataset.\n\n\nCombine with Other Methods – Random Forest pairs well with techniques like PCA for dimensionality reduction. \n\n\n\nUseful Articles and Videos\n\n\nhttps://www.datacamp.com/tutorial/random-forests-classifier-python https://www.ibm.com/topics/random-forest https://www.youtube.com/watch?v=J4Wdy0Wc_xQ https://www.youtube.com/watch?v=QHOazyP-YlM\n\n\n\n\n\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n\n\n\nCreate Model\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n\n\nTrain Model\n\n\n\ny.head()\n\n\n\n\n\n\n\n\n\nSurvived\n\n\n\n\n\n\n0\n\n\n0\n\n\n\n\n1\n\n\n1\n\n\n\n\n2\n\n\n1\n\n\n\n\n3\n\n\n1\n\n\n\n\n4\n\n\n0\n\n\n\n\n\ndtype: int64\n\n\n\n\n\nMake Predictions\n\n\n\n# Predicting the Test set results\nRF = RandomForestClassifier()\nRF.fit(X_train, y_train)\n\n\n\n\n\nRandomForestClassifier()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\n\n\nRandomForestClassifier\n\n\n\n?Documentation for RandomForestClassifieriFitted\n\n\n\nRandomForestClassifier()\n\n\n\n\n\n\n\n\n\n\nHyperparameter Search\n\n\n\ny_pred = RF.predict(X_test)\n\n\n\n\n\nEvaluate the Model\n\n\n\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n\n# Assuming y_test and y_pred are defined somewhere in your code\naccuracy = accuracy_score(y_test, y_pred) * 100\nf1_value = f1_score(y_test, y_pred, average='weighted') * 100\nrecall_value = recall_score(y_test, y_pred, average='weighted') * 100\nprecision_value = precision_score(y_test, y_pred, average='weighted') * 100\n\nprint('Accuracy: ' + str(round(accuracy, 2)) + ' %.')\nprint('F1 Score: ' + str(round(f1_value, 2)) + ' %.')\nprint('Recall Score: ' + str(round(recall_value, 2)) + ' %.')\nprint('Precision Score: ' + str(round(precision_value, 2)) + ' %.')\n\n\nAccuracy: 81.56 %.\nF1 Score: 80.97 %.\nRecall Score: 81.56 %.\nPrecision Score: 82.44 %."
  },
  {
    "objectID": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#overview",
    "href": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#overview",
    "title": "\nClassification Machine Learning Model 🧠 📈\n",
    "section": "\nOverview\n",
    "text": "Overview\n\n\nXGBoost Classification is a supervised machine learning algorithm that builds an ensemble of decision trees to predict categorical outcomes. It is optimized for speed and performance using gradient boosting techniques.\n\n\n\n\n\n\n\n\n\nDefinition XGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:\n\n\n\nBoosting Trees: A collection of decision trees built sequentially to reduce errors.\n\n\nGradient Descent Optimization: Adjusts model weights using the gradient of a loss function.\n\n\nRegularization: Controls model complexity to prevent overfitting.\n\n\n\nFor classification, XGBoost predicts categorical outcomes by minimizing a chosen loss function, such as logistic loss for binary classification or softmax (cross-entropy) loss for multi-class classification.\n\n\n\n\n\n\n\n\n\nKey Concepts 1. Boosting Mechanism: - Unlike a single decision tree, XGBoost builds multiple trees in sequence. - Each new tree corrects the errors of the previous ones by focusing on misclassified examples.\n\n\n\nLoss Functions:\n\n\nDetermines how errors are measured and minimized.\n\n\nCommon choices for classification include:\n\n\nLogistic Loss – Used for binary classification tasks.\n\n\nSoftmax (Cross-Entropy Loss) – Used for multi-class classification tasks.\n\n\n\n\n\n\nRegularization Techniques:\n\n\nPrevents overfitting by adding penalties to complex models.\n\n\nL1 Regularization (Lasso) – Encourages sparsity by shrinking coefficients.\n\n\nL2 Regularization (Ridge) – Penalizes large coefficients to reduce variance.\n\n\n\n\nFeature Importance & Selection:\n\n\nXGBoost ranks features by importance, aiding in feature selection.\n\n\nHelps in eliminating redundant or irrelevant features for better performance.\n\n\n\n\n\n\n\n\n\n\n\n\nPros 1. High Performance – Optimized for speed, scalability, and efficiency. 2. Handles Missing Data – Automatically learns how to deal with missing values. 3. Regularization Built-in – Reduces overfitting with L1 and L2 penalties. 4. Probabilistic Predictions – Provides probability scores for classification, enabling threshold tuning. 5. Works Well with Large Datasets – Efficient memory usage and parallel processing.\n\n\n\n\n\n\n\n\n\nCons 1. Complexity – More difficult to tune compared to simpler models. 2. Computationally Intensive – Training can be slow on very large datasets. 3. Sensitive to Hyperparameters – Performance depends on careful tuning of learning rate, tree depth, and regularization. 4. Less Interpretable – Decision boundaries may be challenging to interpret compared to simpler models.\n\n\n\n\n\n\n\n\n\nTips * Optimize Hyperparameters – Use grid search or Bayesian optimization for tuning. * Use Early Stopping – Stop training if performance ceases to improve on validation data. * Scale Features if Needed – Although XGBoost can handle unscaled data, standardization might improve performance. * Leverage Feature Importance – Identify and remove less relevant features to improve efficiency. * Adjust Decision Thresholds – Fine-tune the probability threshold to balance precision and recall for your specific task.\n\n\n\n\n\n\n\n\n\nUseful Articles and Videos * XGBoost Official Documentation * XGBoost for Classification – Machine Learning Mastery * Understanding XGBoost – Analytics Vidhya * XGBoost Explained for Classification – YouTube"
  },
  {
    "objectID": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#import-datalibraries",
    "href": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#import-datalibraries",
    "title": "\nClassification Machine Learning Model 🧠 📈\n",
    "section": "\nImport Data/Libraries\n",
    "text": "Import Data/Libraries\n\n\n\n!pip install lets_plot\n\n\nRequirement already satisfied: lets_plot in /usr/local/lib/python3.11/dist-packages (4.6.2)\nRequirement already satisfied: pypng in /usr/local/lib/python3.11/dist-packages (from lets_plot) (0.20220715.0)\nRequirement already satisfied: palettable in /usr/local/lib/python3.11/dist-packages (from lets_plot) (3.3.3)\n\n\n\n\n# needed libraries for Classification models\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import f1_score\nfrom xgboost import XGBClassifier\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Load the training dataset\n#### If this gives an error go into the Data folder in GitHub and click on the data csv and then \"Raw\"\n#### (underneath history in the upper righthand corner) then copy that url to replace the \"data_raw_url\"\ndata_raw_url = 'https://raw.githubusercontent.com/BYUIDSS/DSS-ML-Bootcamp/refs/heads/main/3b_Classification_Model/data/Churn_Modelling.csv?token=GHSAT0AAAAAAC7UQH3WQQ4HLOI3GC7L7EVIZ7KGDGQ'\nbanking_df = pd.read_csv(data_raw_url)"
  },
  {
    "objectID": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#explore-visualize-and-understand-the-data",
    "href": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#explore-visualize-and-understand-the-data",
    "title": "\nClassification Machine Learning Model 🧠 📈\n",
    "section": "\nExplore, Visualize and Understand the Data\n",
    "text": "Explore, Visualize and Understand the Data\n\n\n\nbanking_df.head(10)\n\n\n\n&lt;div&gt;\n\n\n\n\n\n\nRowNumber\n\n\nCustomerId\n\n\nSurname\n\n\nCreditScore\n\n\nGeography\n\n\nGender\n\n\nAge\n\n\nTenure\n\n\nBalance\n\n\nNumOfProducts\n\n\nHasCrCard\n\n\nIsActiveMember\n\n\nEstimatedSalary\n\n\nExited\n\n\n\n\n\n\n0\n\n\n1\n\n\n15634602\n\n\nHargrave\n\n\n619\n\n\nFrance\n\n\nFemale\n\n\n42.0\n\n\n2\n\n\n0.00\n\n\n1\n\n\n1.0\n\n\n1.0\n\n\n101348.88\n\n\n1\n\n\n\n\n1\n\n\n2\n\n\n15647311\n\n\nHill\n\n\n608\n\n\nSpain\n\n\nFemale\n\n\n41.0\n\n\n1\n\n\n83807.86\n\n\n1\n\n\n0.0\n\n\n1.0\n\n\n112542.58\n\n\n0\n\n\n\n\n2\n\n\n3\n\n\n15619304\n\n\nOnio\n\n\n502\n\n\nFrance\n\n\nFemale\n\n\n42.0\n\n\n8\n\n\n159660.80\n\n\n3\n\n\n1.0\n\n\n0.0\n\n\n113931.57\n\n\n1\n\n\n\n\n3\n\n\n4\n\n\n15701354\n\n\nBoni\n\n\n699\n\n\nFrance\n\n\nFemale\n\n\n39.0\n\n\n1\n\n\n0.00\n\n\n2\n\n\n0.0\n\n\n0.0\n\n\n93826.63\n\n\n0\n\n\n\n\n4\n\n\n5\n\n\n15737888\n\n\nMitchell\n\n\n850\n\n\nSpain\n\n\nFemale\n\n\n43.0\n\n\n2\n\n\n125510.82\n\n\n1\n\n\nNaN\n\n\n1.0\n\n\n79084.10\n\n\n0\n\n\n\n\n5\n\n\n6\n\n\n15574012\n\n\nChu\n\n\n645\n\n\nSpain\n\n\nMale\n\n\n44.0\n\n\n8\n\n\n113755.78\n\n\n2\n\n\n1.0\n\n\n0.0\n\n\n149756.71\n\n\n1\n\n\n\n\n6\n\n\n7\n\n\n15592531\n\n\nBartlett\n\n\n822\n\n\nNaN\n\n\nMale\n\n\n50.0\n\n\n7\n\n\n0.00\n\n\n2\n\n\n1.0\n\n\n1.0\n\n\n10062.80\n\n\n0\n\n\n\n\n7\n\n\n8\n\n\n15656148\n\n\nObinna\n\n\n376\n\n\nGermany\n\n\nFemale\n\n\n29.0\n\n\n4\n\n\n115046.74\n\n\n4\n\n\n1.0\n\n\n0.0\n\n\n119346.88\n\n\n1\n\n\n\n\n8\n\n\n9\n\n\n15792365\n\n\nHe\n\n\n501\n\n\nFrance\n\n\nMale\n\n\n44.0\n\n\n4\n\n\n142051.07\n\n\n2\n\n\n0.0\n\n\nNaN\n\n\n74940.50\n\n\n0\n\n\n\n\n9\n\n\n10\n\n\n15592389\n\n\nH?\n\n\n684\n\n\nFrance\n\n\nMale\n\n\nNaN\n\n\n2\n\n\n134603.88\n\n\n1\n\n\n1.0\n\n\n1.0\n\n\n71725.73\n\n\n0\n\n\n\n\n\n&lt;div class=\"colab-df-buttons\"&gt;\n\n&lt;button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35961408-bb9e-43f2-b0cc-3b542bdf210b')\" title=\"Convert this dataframe to an interactive table.\" style=\"display:none;\"&gt;\n   \n\n&lt;script&gt;\n  const buttonEl =\n    document.querySelector('#df-35961408-bb9e-43f2-b0cc-3b542bdf210b button.colab-df-convert');\n  buttonEl.style.display =\n    google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n  async function convertToInteractive(key) {\n    const element = document.querySelector('#df-35961408-bb9e-43f2-b0cc-3b542bdf210b');\n    const dataTable =\n      await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                [key], {});\n    if (!dataTable) return;\n\n    const docLinkHtml = 'Like what you see? Visit the ' +\n      '&lt;a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;'\n      + ' to learn more about interactive tables.';\n    element.innerHTML = '';\n    dataTable['output_type'] = 'display_data';\n    await google.colab.output.renderOutput(dataTable, element);\n    const docLink = document.createElement('div');\n    docLink.innerHTML = docLinkHtml;\n    element.appendChild(docLink);\n  }\n&lt;/script&gt;\n\n\n\n\n  \n\n\n\n\n\n&lt;/div&gt;"
  },
  {
    "objectID": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#feature-enginnering-and-data-augmentation",
    "href": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#feature-enginnering-and-data-augmentation",
    "title": "\nClassification Machine Learning Model 🧠 📈\n",
    "section": "\nFeature Enginnering and Data Augmentation\n",
    "text": "Feature Enginnering and Data Augmentation\n\n\n\nX = banking_df.drop(['Exited', 'Surname'], axis=1)\ny = banking_df['Exited']\n\n\n\n\nData Augmentation\n\n\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\n\n\nFeature Engineering\n\n\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns.\n\n\n\ncategorical_features = ['Geography', 'Gender']"
  },
  {
    "objectID": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#machine-learning-model",
    "href": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#machine-learning-model",
    "title": "\nClassification Machine Learning Model 🧠 📈\n",
    "section": "\nMachine Learning Model\n",
    "text": "Machine Learning Model\n\n\n\nSplit the data to train and test\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n\n\nCreate the model\n\n\n\n# Create an ordinal encoder\nordinal_encoder = OrdinalEncoder()\n\n# Create a column transformer to apply ordinal encoding to categorical features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', ordinal_encoder, categorical_features)\n    ],\n    remainder='passthrough'  # Keep other columns unchanged\n)\n\n\n\n\n\nTrain the model\n\n\n\n# Fit and transform the training data\nX_train_encoded = preprocessor.fit_transform(X_train)\n\n# Transform the test data using the fitted preprocessor\nX_test_encoded = preprocessor.transform(X_test)\n\n\n\n\n\nMake predictions\n\n\n\n# Initialize the XGBoost classifier\nxgb_classifier = XGBClassifier(random_state=42)\n\n\n\n\nHyperparameter Search\n\n\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid = {\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7],\n}\n\n# Perform GridSearchCV\ngrid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=3, scoring='f1')\ngrid_search.fit(X_train_encoded, y_train)\n\n# Get the best model from GridSearchCV\nbest_xgb_classifier = grid_search.best_estimator_\n\n# Make predictions using the best model\ny_pred = best_xgb_classifier.predict(X_test_encoded)\n\n\n\n\n\n\nEvaluate the Model\n\n\nAccuracy – The percentage of total predictions that are correct. Example: If a spam filter correctly classifies 90 out of 100 emails (whether spam or not), the accuracy is 90%.\n\n\nF1 Score – Out of all the positive predictions, how many were actually correct. Example: If a spam filter predicts 20 emails as spam, but only 15 are actually spam, precision is 15/20 = 75%.\n\n\nRecall Score – Out of all the actual positive cases, how many did the model correctly identify. Example: If there were 25 spam emails in total, and the model correctly identified 15 of them, recall is 15/25 = 60%.\n\n\nPrecision Score – A balance between precision and recall (harmonic mean). Example: If precision is 75% and recall is 60%, F1 score is (2 × 75 × 60) / (75 + 60) = 66.7%.\n\n\n\n# Evaluate the model using classification metrics\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mse)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\n\n# Calculate R-squared (R2)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint('Mean Squared Error (MSE):', mse)\nprint('Root Mean Squared Error (RMSE):', rmse)\nprint('Mean Absolute Error (MAE):', mae)\nprint('R-squared (R2):', r2)\n\n\nMean Squared Error (MSE): 0.13693153423288357\nRoot Mean Squared Error (RMSE): 0.37004261137453287\nMean Absolute Error (MAE): 0.13693153423288357\nR-squared (R2): 0.14705086201263828"
  },
  {
    "objectID": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#import-datalibraries",
    "href": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#import-datalibraries",
    "title": "\nK-Nearest Neighbors Machine Leaning Model 💻 🧠\n",
    "section": "\nImport Data/Libraries\n",
    "text": "Import Data/Libraries\n\n\nid=“cell-3” class=“cell” data-outputid=“99085247-c014-4cca-ea53-d401b21c68c1”&gt;\n\nclass=“sourceCode cell-code” id=“cb1”&gt;\n!pip3 install lets-plot\n\n\nclass=“cell-output cell-output-stdout”&gt;\nCollecting lets-plot\n  Downloading lets_plot-4.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting pypng (from lets-plot)\n  Downloading pypng-0.20220715.0-py3-none-any.whl.metadata (13 kB)\nCollecting palettable (from lets-plot)\n  Downloading palettable-3.3.3-py2.py3-none-any.whl.metadata (3.3 kB)\nDownloading lets_plot-4.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 28.8 MB/s eta 0:00:00\nDownloading palettable-3.3.3-py2.py3-none-any.whl (332 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 332.3/332.3 kB 16.1 MB/s eta 0:00:00\nDownloading pypng-0.20220715.0-py3-none-any.whl (58 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 kB 3.9 MB/s eta 0:00:00\nInstalling collected packages: pypng, palettable, lets-plot\nSuccessfully installed lets-plot-4.6.2 palettable-3.3.3 pypng-0.20220715.0\n\n\n\nid=“cell-4” class=“cell”&gt;\n\nclass=“sourceCode cell-code” id=“cb3”&gt;\n\n# needed libraries for KNN models\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.model_selection import cross_val_score,  train_test_split , KFold\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nimport lets_plot as lp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n\n# foundation dataset\nfrom sklearn.datasets import load_iris\n\n# stretch dataset\ncleveland_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data', header=None)\nhungarian_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data', header=None)\nswitzerland_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data', header=None)\n\n\n\nid=“cell-5” class=“cell” data-outputid=“9e426d0c-c14e-463f-9262-bb246416abde”&gt;\n\nclass=“sourceCode cell-code” id=“cb4”&gt;\n# Code to set up access to the foundational dataset\niris = load_iris()\niris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\niris_df['target'] = iris.target_names[iris.target]\n# iris_df is now the dataframe to be used for the foundational model\niris_df.head()\n\n\nclass=“cell-output cell-output-display” data-execution_count=“3”&gt;\n\nid=“df-6ff9e14d-290b-4c6a-bbb7-0e44444cbcb1” class=“colab-df-container”&gt;\n\n\n\n\n\n\n\nsepal length (cm)\n\n\nsepal width (cm)\n\n\npetal length (cm)\n\n\npetal width (cm)\n\n\ntarget\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\nsetosa\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\nsetosa\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n\n&lt;/div&gt; class=\"colab-df-buttons\"&gt;\n\nclass=“colab-df-container”&gt; \n   \n\n&lt;script&gt;\n  const buttonEl =\n    document.querySelector('#df-6ff9e14d-290b-4c6a-bbb7-0e44444cbcb1 button.colab-df-convert');\n  buttonEl.style.display =\n    google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n  async function convertToInteractive(key) {\n    const element = document.querySelector('#df-6ff9e14d-290b-4c6a-bbb7-0e44444cbcb1');\n    const dataTable =\n      await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                [key], {});\n    if (!dataTable) return;\n\n    const docLinkHtml = 'Like what you see? Visit the ' +\n      '&lt;a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;'\n      + ' to learn more about interactive tables.';\n    element.innerHTML = '';\n    dataTable['output_type'] = 'display_data';\n    await google.colab.output.renderOutput(dataTable, element);\n    const docLink = document.createElement('div');\n    docLink.innerHTML = docLinkHtml;\n    element.appendChild(docLink);\n  }\n&lt;/script&gt;\n\n\nid=“df-42b42b17-221c-4d09-9655-87d86d79080c”&gt; \n\n  \n\n\n\n\n\n&lt;/div&gt;"
  },
  {
    "objectID": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#explore-visualize-and-understand-the-data",
    "href": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#explore-visualize-and-understand-the-data",
    "title": "\nK-Nearest Neighbors Machine Leaning Model 💻 🧠\n",
    "section": "\nExplore, Visualize and Understand the Data\n",
    "text": "Explore, Visualize and Understand the Data\n\n\nid=“cell-7” class=“cell” data-outputid=“9404300e-0acc-4be9-b01f-cf5381ca91c4”&gt;\n\nclass=“sourceCode cell-code” id=“cb5”&gt;\n\nsns.pairplot(iris_df, hue = \"target\", size=3, markers=[\"o\", \"s\", \"D\"])\nplt.figure()\nplt.show()\n\n\nclass=“cell-output cell-output-stderr”&gt;\n/usr/local/lib/python3.11/dist-packages/seaborn/axisgrid.py:2100: UserWarning: The `size` parameter has been renamed to `height`; please update your code.\n  warnings.warn(msg, UserWarning)\n\n\nclass=“cell-output cell-output-display”&gt;\n\n\n\n\n\n\n\n\n\nclass=“cell-output cell-output-display”&gt;\n&lt;Figure size 640x480 with 0 Axes&gt;"
  },
  {
    "objectID": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#feature-enginnering-and-data-augmentation",
    "href": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#feature-enginnering-and-data-augmentation",
    "title": "\nK-Nearest Neighbors Machine Leaning Model 💻 🧠\n",
    "section": "\nFeature Enginnering and Data Augmentation\n",
    "text": "Feature Enginnering and Data Augmentation\n\n\n\nData Augmentation\n\n\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\n\n\nFeature Engineering\n\n\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns.\n\n\nid=“cell-10” class=“cell”&gt;\n\nclass=“sourceCode cell-code” id=“cb8”&gt;"
  },
  {
    "objectID": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#machine-learning-model",
    "href": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#machine-learning-model",
    "title": "\nK-Nearest Neighbors Machine Leaning Model 💻 🧠\n",
    "section": "\nMachine Learning Model\n",
    "text": "Machine Learning Model\n\n\n\nSplit the data\n\n\nid=“cell-15” class=“cell”&gt;\n\nclass=“sourceCode cell-code” id=“cb9”&gt;\nx= iris_df.drop(columns=['target'])\ny = iris_df['target']\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42) #Training 80% and testing 20%\n\n\n\nid=“cell-17” class=“cell” data-outputid=“77648695-f286-456c-9603-5e2e110c5ea0”&gt;\n\nclass=“sourceCode cell-code” id=“cb11”&gt;\ny.head()\n\n\nclass=“cell-output cell-output-display” data-execution_count=“7”&gt;\n\n\n\n\n\n\n\ntarget\n\n\n\n\n\n\n0\n\n\nsetosa\n\n\n\n\n1\n\n\nsetosa\n\n\n\n\n2\n\n\nsetosa\n\n\n\n\n3\n\n\nsetosa\n\n\n\n\n4\n\n\nsetosa\n\n\n\n\n\ndtype: object\n\n\n\n\n\nCreate the model\n\n\n\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(x, y)\n\n\n\n\n\nKNeighborsClassifier(n_neighbors=3)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\n\n\nKNeighborsClassifier\n\n\n\n?Documentation for KNeighborsClassifieriFitted\n\n\n\nKNeighborsClassifier(n_neighbors=3)\n\n\n\n\n\n\n\n\n\n\nTrain the model\n\n\n\nneigh.fit(x, y)\n\n\n\n\n\nKNeighborsClassifier(n_neighbors=3)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\n\n\nKNeighborsClassifier\n\n\n\n?Documentation for KNeighborsClassifieriFitted\n\n\n\nKNeighborsClassifier(n_neighbors=3)\n\n\n\n\n\n\n\n\n\n\nMake predictions\n\n\n\ny_pred = neigh.predict(X_test)"
  },
  {
    "objectID": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#evaluate-the-model",
    "href": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#evaluate-the-model",
    "title": "\nK-Nearest Neighbors Machine Leaning Model 💻 🧠\n",
    "section": "\nEvaluate the Model\n",
    "text": "Evaluate the Model\n\n\n\naccuracy = accuracy_score(y_test, y_pred)*100\n\nprint('Accuracy: ' + str(round(accuracy, 2)) + ' %.')\n\n\nAccuracy: 100.0 %."
  }
]