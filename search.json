[
  {
    "objectID": "data_visualization.html",
    "href": "data_visualization.html",
    "title": "Machine Learning - Bootcamp",
    "section": "",
    "text": "Developed an interactive Power BI dashboard to monitor and analyze student account holds with filters for hold type, date range, and student demographics (international vs.¬†domestic students).\n\n\n\nDeveloped an interactive Power BI dashboard to track and analyze active academic programs across colleges and departments. The report includes search and filter options for program name, code, department, program type, college, and academic year (UG19‚ÄìUG25), enabling quick identification of program availability and status.\n\n\n\nDeveloped an interactive Power BI dashboard to track and evaluate transfer course accuracy across schools and departments. The dashboard includes filters for date range, I-Number (student lookup), and school/transfer type categories (e.g., duplicated transfer, waivers, invalid grade)."
  },
  {
    "objectID": "data_visualization.html#my-projects",
    "href": "data_visualization.html#my-projects",
    "title": "Machine Learning - Bootcamp",
    "section": "",
    "text": "Developed an interactive Power BI dashboard to monitor and analyze student account holds with filters for hold type, date range, and student demographics (international vs.¬†domestic students).\n\n\n\nDeveloped an interactive Power BI dashboard to track and analyze active academic programs across colleges and departments. The report includes search and filter options for program name, code, department, program type, college, and academic year (UG19‚ÄìUG25), enabling quick identification of program availability and status.\n\n\n\nDeveloped an interactive Power BI dashboard to track and evaluate transfer course accuracy across schools and departments. The dashboard includes filters for date range, I-Number (student lookup), and school/transfer type categories (e.g., duplicated transfer, waivers, invalid grade)."
  },
  {
    "objectID": "Data_Visualization_Projects/active_program.html",
    "href": "Data_Visualization_Projects/active_program.html",
    "title": "Data Visualization projects",
    "section": "",
    "text": "Active Program Report\nNote: All sensitive data has been removed for privacy. This project was reviewed and approved for portfolio use.\n\n\n\nActive Program Report\n\n\n\nWhat is it?\nDeveloped an interactive Power BI dashboard to track and analyze active academic programs across colleges and departments. The report includes search and filter options for program name, code, department, program type, college, and academic year (UG19‚ÄìUG25), enabling quick identification of program availability and status.\nDesigned a program lookup table with highlighted Yes/No indicators to clearly show whether programs were active in each academic year.\nBuilt dynamic filters to improve usability and streamline program searches for staff.\n\n\nImpact\nSimplified program tracking through clear Yes/No highlights, improved accessibility of program information, and enabled faster decision-making for academic planning.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Data_Visualization_Projects/hold_report.html",
    "href": "Data_Visualization_Projects/hold_report.html",
    "title": "Data Visualization projects",
    "section": "",
    "text": "Hold Report\nNote: All sensitive data has been removed for privacy. This project was reviewed and approved for portfolio use.\n{width=100%}\n\nWhat is it?\nDeveloped an interactive Power BI dashboard to monitor and analyze student account holds with filters for hold type, date range, and student demographics (international vs.¬†domestic students).\nBuilt key visualizations including monthly resolved holds (using DAX for time-based calculations), top hold types, risk-tier distribution, unresolved hold comparisons, and semester-aligned reference lines.\nDesigned dynamic filters and clear visualizations to support decision-making by the registration team.\n\n\nImpact\nImproved transparency of student hold trends, enabled actionable insights for decision-makers, and enhanced reporting flexibility.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning_Projects/1_KNN_Model/readme.html",
    "href": "Machine_Learning_Projects/1_KNN_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "This folder‚Äôs purpose is to give an example of using KNN machine learning model.\nFoundation datset: iris dataset Stretch dataset: the Heart diseases - UCI (https://archive.ics.uci.edu/dataset/45/heart+disease).\nK-Nearest Neighbors (KNN) is a supervised machine learning algorithm commonly used for classification and regression tasks. It‚Äôs one of the simplest and most intuitive models in machine learning.\n\nDefinition: KNN works by finding the ‚Äòk‚Äô closest data points (neighbors) to a given input based on some distance metric (e.g., Euclidean distance). The predicted value or class is determined by these neighbors:\n\nFor classification, the input is assigned the class most common among its neighbors (majority vote).\nFor regression, the predicted value is the average (or sometimes weighted average) of the neighbors‚Äô values.\n\n\nKey Concepts:\n\nK (Number of Neighbors): The algorithm uses ‚Äòk‚Äô neighbors to make predictions. Choosing the right ‚Äòk‚Äô is crucial:\n\n\nSmall ‚Äòk‚Äô (e.g., 1 or 3) makes the model sensitive to noise.\nLarge ‚Äòk‚Äô smooths out predictions but may overlook local patterns.\n\n\nDistance Metrics: Determines how ‚Äúclose‚Äù neighbors are. Common metrics include:\n\n\nEuclidean Distance: Straight-line distance between points.\nManhattan Distance: Distance measured along axes at right angles.\nCosine Similarity: Measures the cosine of the angle between two vectors (useful for text or high-dimensional data).\n\n\nLaziness: KNN is a lazy learner, meaning it doesn‚Äôt learn a model during training. Instead, it stores the data and makes predictions when queried. This is why it‚Äôs called a ‚Äúmemory-based‚Äù approach.\n\nUseful Articles and Videos: * https://www.w3schools.com/python/python_ml_knn.asp * https://realpython.com/knn-python/ * https://www.geeksforgeeks.org/k-nearest-neighbor-algorithm-in-python/ * https://www.youtube.com/watch?v=CQveSaMyEwM * https://www.youtube.com/watch?v=b6uHw7QW_n4 * https://www.youtube.com/watch?v=w6bOBZX-1kY\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#overview",
    "href": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#overview",
    "title": "Regression Machine Learning Model üß† üìà",
    "section": "Overview",
    "text": "Overview\nXGBoost Regression is a supervised machine learning algorithm that builds an ensemble of decision trees to predict continuous values. It is optimized for speed and performance using gradient boosting techniques.\n\n\n\nDefinition\nXGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:\n\nBoosting Trees: A collection of decision trees built sequentially to reduce errors.\n\nGradient Descent Optimization: Adjusts model weights using the gradient of a loss function.\n\nRegularization: Controls model complexity to prevent overfitting.\n\nFor regression, XGBoost predicts continuous values by minimizing a chosen loss function, commonly Mean Squared Error (MSE) or Mean Absolute Error (MAE).\n\n\n\nKey Concepts\n1. Boosting Mechanism:\n- Unlike a single decision tree, XGBoost builds multiple trees in sequence.\n- Each new tree corrects the errors of the previous ones by focusing on residuals.\n\nLoss Functions:\n\nDetermines how errors are measured and minimized.\n\nCommon choices:\n\nMean Squared Error (MSE) ‚Äì Penalizes larger errors more heavily.\n\nMean Absolute Error (MAE) ‚Äì Treats all errors equally.\n\nHuber Loss ‚Äì A mix of MSE and MAE to handle outliers.\n\n\nRegularization Techniques:\n\nPrevents overfitting by adding penalties to complex models.\n\nL1 Regularization (Lasso) ‚Äì Shrinks coefficients, promoting sparsity.\n\nL2 Regularization (Ridge) ‚Äì Penalizes large coefficients to reduce variance.\n\nFeature Importance & Selection:\n\nXGBoost ranks features by importance, aiding feature selection.\n\nCan be used to eliminate redundant or irrelevant features.\n\n\n\n\n\nPros\n1. High Performance ‚Äì Optimized for speed, scalability, and efficiency.\n2. Handles Missing Data ‚Äì Automatically learns how to deal with missing values.\n3. Regularization Built-in ‚Äì Reduces overfitting with L1 and L2 penalties.\n4. Works Well with Large Datasets ‚Äì Efficient memory usage and parallel processing.\n\n\n\nCons\n1. Complexity ‚Äì More difficult to tune compared to simpler models.\n2. Computationally Intensive ‚Äì Training can be slow on very large datasets.\n3. Sensitive to Hyperparameters ‚Äì Performance depends on careful tuning of learning rate, tree depth, and regularization.\n\n\n\nTips\n* Optimize Hyperparameters ‚Äì Use grid search or Bayesian optimization for tuning.\n* Use Early Stopping ‚Äì Stops training if performance stops improving on validation data.\n* Scale Features if Needed ‚Äì Although XGBoost can handle unscaled data, standardization may help in some cases.\n* Leverage Feature Importance ‚Äì Identify and remove less relevant features to improve efficiency.\n\n\n\nUseful Articles and Videos\n* XGBoost Official Documentation\n* XGBoost for Regression ‚Äì Machine Learning Mastery\n* Understanding XGBoost ‚Äì Analytics Vidhya\n* XGBoost Explained ‚Äì YouTube",
    "crumbs": [
      "Machine Learning projects",
      "Regression Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#import-datalibraries",
    "href": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#import-datalibraries",
    "title": "Regression Machine Learning Model üß† üìà",
    "section": "Import Data/Libraries",
    "text": "Import Data/Libraries\n\n!pip install lets_plot\n\nRequirement already satisfied: lets_plot in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (4.7.1)\nRequirement already satisfied: pypng in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (from lets_plot) (0.20220715.0)\nRequirement already satisfied: palettable in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (from lets_plot) (3.3.3)\nRequirement already satisfied: pillow in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (from lets_plot) (11.3.0)\n\n\n\n# needed libraries for Regression models\nimport pandas as pd\nfrom sklearn import tree\nimport xgboost as xgb\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, mean_squared_error\nfrom sklearn.model_selection import cross_val_score,  train_test_split , KFold\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nimport lets_plot as lp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n# import kagglehub\n\n# foundation dataset\ndata_raw_url = \"nyc-east-river-bicycle-counts.csv\"\nbicycle_df = pd.read_csv(data_raw_url)",
    "crumbs": [
      "Machine Learning projects",
      "Regression Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#explore-visualize-and-understand-the-data",
    "href": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#explore-visualize-and-understand-the-data",
    "title": "Regression Machine Learning Model üß† üìà",
    "section": "Explore, Visualize and Understand the Data",
    "text": "Explore, Visualize and Understand the Data\n\nbicycle_df.head(10)\n\n\n\n\n\n\n\n\nUnnamed: 0\nDate\nDay\nHigh Temp (¬∞F)\nLow Temp (¬∞F)\nPrecipitation\nBrooklyn Bridge\nManhattan Bridge\nWilliamsburg Bridge\nQueensboro Bridge\nTotal\n\n\n\n\n0\n0\n2016-04-01 00:00:00\n2016-04-01 00:00:00\n78.1\n66.0\n0.01\n1704.0\n3126\n4115.0\n2552.0\n11497\n\n\n1\n1\n2016-04-02 00:00:00\n2016-04-02 00:00:00\n55.0\n48.9\n0.15\n827.0\n1646\n2565.0\n1884.0\n6922\n\n\n2\n2\n2016-04-03 00:00:00\n2016-04-03 00:00:00\n39.9\n34.0\n0.09\n526.0\n1232\n1695.0\n1306.0\n4759\n\n\n3\n3\n2016-04-04 00:00:00\n2016-04-04 00:00:00\n44.1\n33.1\n0.47 (S)\n521.0\n1067\n1440.0\n1307.0\n4335\n\n\n4\n4\n2016-04-05 00:00:00\n2016-04-05 00:00:00\n42.1\n26.1\n0\n1416.0\n2617\n3081.0\n2357.0\n9471\n\n\n5\n5\n2016-04-06 00:00:00\n2016-04-06 00:00:00\n45.0\n30.0\n0\n1885.0\n3329\n3856.0\n2849.0\n11919\n\n\n6\n6\n2016-04-07 00:00:00\n2016-04-07 00:00:00\n57.0\n53.1\n0.09\n1276.0\n2581\n3282.0\n2457.0\n9596\n\n\n7\n7\n2016-04-08 00:00:00\n2016-04-08 00:00:00\n46.9\n44.1\n0.01\n1982.0\n3455\n4113.0\n3194.0\n12744\n\n\n8\n8\n2016-04-09 00:00:00\n2016-04-09 00:00:00\n43.0\n37.9\n0.09\n504.0\n997\n1507.0\n1502.0\n4510\n\n\n9\n9\n2016-04-10 00:00:00\n2016-04-10 00:00:00\n48.9\n30.9\n0\n1447.0\n2387\n3132.0\n2160.0\n9126\n\n\n\n\n\n\n\n\nbicycle_df.describe()\n\n\n\n\n\n\n\n\nUnnamed: 0\nHigh Temp (¬∞F)\nLow Temp (¬∞F)\nBrooklyn Bridge\nManhattan Bridge\nWilliamsburg Bridge\nQueensboro Bridge\nTotal\n\n\n\n\ncount\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n\n\nmean\n104.500000\n60.580000\n46.413333\n2269.633333\n4049.533333\n4862.466667\n3352.866667\n14534.500000\n\n\nstd\n60.765944\n11.183223\n9.522796\n981.237786\n1704.731356\n1814.039499\n1099.254419\n5569.173496\n\n\nmin\n0.000000\n39.900000\n26.100000\n504.000000\n997.000000\n1440.000000\n1306.000000\n4335.000000\n\n\n25%\n52.250000\n55.000000\n44.100000\n1447.000000\n2617.000000\n3282.000000\n2457.000000\n9596.000000\n\n\n50%\n104.500000\n62.100000\n46.900000\n2379.500000\n4165.000000\n5194.000000\n3477.000000\n15292.500000\n\n\n75%\n156.750000\n68.000000\n50.000000\n3147.000000\n5309.000000\n6030.000000\n4192.000000\n18315.000000\n\n\nmax\n209.000000\n81.000000\n66.000000\n3871.000000\n6951.000000\n7834.000000\n5032.000000\n23318.000000\n\n\n\n\n\n\n\n\nbicycle_df.dtypes\n\nUnnamed: 0               int64\nDate                    object\nDay                     object\nHigh Temp (¬∞F)         float64\nLow Temp (¬∞F)          float64\nPrecipitation           object\nBrooklyn Bridge        float64\nManhattan Bridge         int64\nWilliamsburg Bridge    float64\nQueensboro Bridge      float64\nTotal                    int64\ndtype: object\n\n\n\nsns.barplot(data=bicycle_df, x=\"Brooklyn Bridge\", y =\"Total\", color=\"blue\")\nplt.title(\"Plot\")\nplt.legend(loc='upper right')\nplt.show()\n\n/tmp/ipykernel_2690/3523077237.py:3: UserWarning:\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\n\n\nsns.barplot(data=bicycle_df, x=\"Manhattan Bridge\", y =\"Total\", hue=\"Total\", palette=\"Set2\")\nplt.title(\"Plot\")\nplt.show()",
    "crumbs": [
      "Machine Learning projects",
      "Regression Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#feature-enginnering-and-data-augmentation",
    "href": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#feature-enginnering-and-data-augmentation",
    "title": "Regression Machine Learning Model üß† üìà",
    "section": "Feature Enginnering and Data Augmentation",
    "text": "Feature Enginnering and Data Augmentation\n\nData Augmentation\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\nFeature Engineering\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns.\n\nbicycle_df1 = pd.get_dummies(bicycle_df, columns=[\"Date\", \"Day\",\"Precipitation\"])\n\n\nX = bicycle_df1.drop(columns=[\"Brooklyn Bridge\",    \"Manhattan Bridge\", \"Williamsburg Bridge\",  \"Queensboro Bridge\", \"Total\"])\ny = bicycle_df1[\"Total\"]\n\n\nbicycle_df1.describe()\n\n\n\n\n\n\n\n\nUnnamed: 0\nHigh Temp (¬∞F)\nLow Temp (¬∞F)\nBrooklyn Bridge\nManhattan Bridge\nWilliamsburg Bridge\nQueensboro Bridge\nTotal\n\n\n\n\ncount\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n\n\nmean\n104.500000\n60.580000\n46.413333\n2269.633333\n4049.533333\n4862.466667\n3352.866667\n14534.500000\n\n\nstd\n60.765944\n11.183223\n9.522796\n981.237786\n1704.731356\n1814.039499\n1099.254419\n5569.173496\n\n\nmin\n0.000000\n39.900000\n26.100000\n504.000000\n997.000000\n1440.000000\n1306.000000\n4335.000000\n\n\n25%\n52.250000\n55.000000\n44.100000\n1447.000000\n2617.000000\n3282.000000\n2457.000000\n9596.000000\n\n\n50%\n104.500000\n62.100000\n46.900000\n2379.500000\n4165.000000\n5194.000000\n3477.000000\n15292.500000\n\n\n75%\n156.750000\n68.000000\n50.000000\n3147.000000\n5309.000000\n6030.000000\n4192.000000\n18315.000000\n\n\nmax\n209.000000\n81.000000\n66.000000\n3871.000000\n6951.000000\n7834.000000\n5032.000000\n23318.000000",
    "crumbs": [
      "Machine Learning projects",
      "Regression Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#machine-learning-model",
    "href": "Machine_Learning_Projects/3_Regression_Model/ML_Bootcamp_Regression.html#machine-learning-model",
    "title": "Regression Machine Learning Model üß† üìà",
    "section": "Machine Learning Model",
    "text": "Machine Learning Model\n\nSplit the data to train and test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n\n\nCreate the model\n\nparams = {\n    'objective': 'reg:squarederror',  # Regression task\n    'max_depth': 6,                   # Maximum depth of trees\n    'learning_rate': 0.1,             # Learning rate\n    'n_estimators': 100,              # Number of boosting rounds\n}\n\n\n\nTrain the model\n\nmodel = xgb.train(params, dtrain, num_boost_round=100)\n\n/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning:\n\n[06:46:58] WARNING: /workspace/src/learner.cc:738: \nParameters: { \"n_estimators\" } are not used.\n\n\n\n\n\n\nMake predictions\n\n# Predicting the Test set results\ny_pred = model.predict(dtest)\n\n\nHyperparameter Search\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid to search\nparam_grid = {\n    'max_depth': [3, 4, 5, 6, 7],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [50, 100, 200],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n# Create the XGBoost model\nxgb_model = xgb.XGBRegressor(objective='reg:squarederror', seed=42)\n\n# Create and Configure GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=xgb_model,\n    param_grid=param_grid,\n    scoring='neg_mean_squared_error',  # Since MSE should be minimized, we use its negative (because GridSearchCV maximizes the score)\n    cv=5,  # Uses 5-fold cross-validation to evaluate each parameter set\n    verbose=1, # Prints progress updates\n    n_jobs=-1 # Uses all available CPU cores for parallel computation, making it faster\n)\n# Fit the Grid Search to Training Data\ngrid_search.fit(X_train, y_train)\n\n# Get the Best Model & Hyperparameters\nprint(\"Best hyperparameters:\", grid_search.best_params_) # The optimal hyperparameter combination\nprint(\"Best score:\", grid_search.best_score_) # The highest cross-validation score (negative MSE)\nbest_model = grid_search.best_estimator_ # The trained XGBoost model with the best hyperparameters\nprint(f\"Grid Search: {best_model}\")\n\nFitting 5 folds for each of 405 candidates, totalling 2025 fits\nBest hyperparameters: {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 1.0}\nBest score: -1.5210765195661224e-05\nGrid Search: XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=1.0, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=0.2, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=7,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=200,\n             n_jobs=None, num_parallel_tree=None, ...)\n\n\n\n\n\nEvaluate the Model\nAccuracy ‚Äì The percentage of total predictions that are correct.\nExample: If a spam filter correctly classifies 90 out of 100 emails (whether spam or not), the accuracy is 90%.\nF1 Score ‚Äì Out of all the positive predictions, how many were actually correct.\nExample: If a spam filter predicts 20 emails as spam, but only 15 are actually spam, precision is 15/20 = 75%.\nRecall Score ‚Äì Out of all the actual positive cases, how many did the model correctly identify.\nExample: If there were 25 spam emails in total, and the model correctly identified 15 of them, recall is 15/25 = 60%.\nPrecision Score ‚Äì A balance between precision and recall (harmonic mean).\nExample: If precision is 75% and recall is 60%, F1 score is (2 √ó 75 √ó 60) / (75 + 60) = 66.7%.\n\n# Evaluate the model using regression metrics\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mse)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\n\n# Calculate R-squared (R2)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint('Mean Squared Error (MSE):', mse)\nprint('Root Mean Squared Error (RMSE):', rmse)\nprint('Mean Absolute Error (MAE):', mae)\nprint('R-squared (R2):', r2)\n\nMean Squared Error (MSE): 1.5638288259506226\nRoot Mean Squared Error (RMSE): 1.2505314174184599\nMean Absolute Error (MAE): 1.0395972728729248\nR-squared (R2): 0.9999999403953552",
    "crumbs": [
      "Machine Learning projects",
      "Regression Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#overview",
    "href": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#overview",
    "title": "Decision Tree Machine Learning Model üíª üß†",
    "section": "Overview",
    "text": "Overview\nDecision Tree (DT) is a supervised machine learning algorithm used for both classification and regression tasks. It splits data into branches based on feature values to make predictions.\n\n\n\nDefinition\nA Decision Tree is a flowchart-like model where data is split into smaller subsets based on feature conditions. The structure consists of:\n\nRoot Node: The starting point that represents the entire dataset.\n\nDecision Nodes: Points where the data is split based on a feature.\n\nLeaves: Terminal nodes that provide the final prediction.\n\nFor classification, a Decision Tree assigns class labels based on feature splits.\nFor regression, it predicts continuous values using the average or mean of data points in each leaf.\n\n\n\nKey Concepts 1. Splitting Criteria:\n- Determines how the dataset is divided at each step.\n- Common methods: * Gini Impurity (Classification) ‚Äì Measures the likelihood of incorrect classification.\n* Entropy (Classification) ‚Äì Uses information gain to decide splits.\n* Mean Squared Error (MSE) (Regression) ‚Äì Measures variance within nodes.\n\nTree Depth & Overfitting:\n\nDeeper trees fit training data better but may overfit.\n\nPruning (removing unnecessary branches) improves generalization.\n\nFeature Importance:\n\nDecision Trees rank features by their impact on predictions.\n\nHelps in feature selection for other models.\n\nHandling Missing Data:\n\nSome implementations allow surrogate splits to handle missing values.\n\n\n\n\n\nPros 1. Easy to Understand & Interpret ‚Äì Can be visualized as a simple flowchart.\n2. No Need for Feature Scaling ‚Äì Works with both categorical and numerical features.\n3. Handles Non-Linearity ‚Äì Can model complex relationships without requiring transformation.\n4. Fast for Small Datasets ‚Äì Training and inference are relatively quick.\n\n\n\nCons 1. Prone to Overfitting ‚Äì Deep trees can memorize training data, reducing generalization.\n2. Unstable to Small Changes ‚Äì Small variations in data can change the tree structure significantly.\n3. Less Efficient on Large Datasets ‚Äì Computationally expensive for large datasets.\n\n\n\nTips * Limit Tree Depth ‚Äì Use max_depth to prevent overfitting.\n* Pruning Techniques ‚Äì Use pre-pruning (max_leaf_nodes) or post-pruning to remove unnecessary branches.\n* Use Feature Importance ‚Äì Identify the most influential features and remove irrelevant ones.\n* Consider Ensemble Methods ‚Äì Random Forest or Gradient Boosting improve Decision Trees by reducing variance.\n\n\n\nUseful Articles and Videos * https://www.datacamp.com/tutorial/decision-tree-classification-python * https://www.ibm.com/think/topics/decision-trees * https://www.youtube.com/watch?v=6DlWndLbk90 * https://www.youtube.com/watch?v=ZOiBe-nrmc4",
    "crumbs": [
      "Machine Learning projects",
      "Decision Tree & Random Forests Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#import-datalibraries",
    "href": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#import-datalibraries",
    "title": "Decision Tree Machine Learning Model üíª üß†",
    "section": "Import Data/Libraries",
    "text": "Import Data/Libraries\n\n!pip install lets_plot\n\nRequirement already satisfied: lets_plot in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (4.7.1)\nRequirement already satisfied: pypng in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (from lets_plot) (0.20220715.0)\nRequirement already satisfied: palettable in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (from lets_plot) (3.3.3)\nRequirement already satisfied: pillow in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (from lets_plot) (11.3.0)\n\n\n\n# needed libraries for Decision Tree models\nimport pandas as pd\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.model_selection import cross_val_score,  train_test_split , KFold\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nimport lets_plot as lp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# foundation dataset\ntitanic_df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n\n# stretch dataset\ncleveland_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data', header=None)\nhungarian_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data', header=None)\nswitzerland_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data', header=None)",
    "crumbs": [
      "Machine Learning projects",
      "Decision Tree & Random Forests Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#explore-visualize-and-understand-the-data",
    "href": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#explore-visualize-and-understand-the-data",
    "title": "Decision Tree Machine Learning Model üíª üß†",
    "section": "Explore, Visualize and Understand the Data",
    "text": "Explore, Visualize and Understand the Data\n\nsns.countplot(data=titanic_df, x=\"Pclass\", hue=\"Survived\", palette=\"Set2\")\nplt.title(\"Survival Count by Passenger Class\")\nplt.xlabel(\"Passenger Class\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Survived\", labels=[\"No\", \"Yes\"])\nplt.show()\n\n\n\n\n\n\n\n\n\ntitanic_df.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom lets_plot import *\nLetsPlot.setup_html()\nggplot(mapping=aes(x='Pclass', fill='Survived'), data=titanic_df) + geom_bar()",
    "crumbs": [
      "Machine Learning projects",
      "Decision Tree & Random Forests Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#feature-enginnering-and-data-augmentation",
    "href": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#feature-enginnering-and-data-augmentation",
    "title": "Decision Tree Machine Learning Model üíª üß†",
    "section": "Feature Enginnering and Data Augmentation",
    "text": "Feature Enginnering and Data Augmentation\n\nData Augmentation\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\nFeature Engineering\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns.\n\nconditions = [\n    titanic_df[\"Name\"].str.contains(r'(?i)(Mr)|(Mrs)|(Ms)|(Miss)|(Mlle)', na=False),  # Condition 1: If \"Name\" contains \"Mr\"\n    titanic_df[\"Name\"].str.contains(r'(?i)(Rev)|(Dr)|(General)|(Col)|(Major)|(Capt)', na=False), # Condition 2: If \"Name\" contains \"Mrs\"\n    titanic_df[\"Name\"].str.contains(r'(?i)(Master)|(Countess)|(Jonkheer)|(Don)', na=False) # Condition 3: If \"Name\" contains \"Miss\"\n]\n\noutputs = [\n    1,\n    2,\n    3\n]\n\ntitanic_df[\"Name_group\"] = np.select(conditions, outputs, default=5)\n\n/tmp/ipykernel_2622/2687717826.py:2: UserWarning:\n\nThis pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n\n/tmp/ipykernel_2622/2687717826.py:3: UserWarning:\n\nThis pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n\n/tmp/ipykernel_2622/2687717826.py:4: UserWarning:\n\nThis pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.",
    "crumbs": [
      "Machine Learning projects",
      "Decision Tree & Random Forests Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#machine-learning-model-decision-tree",
    "href": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#machine-learning-model-decision-tree",
    "title": "Decision Tree Machine Learning Model üíª üß†",
    "section": "Machine Learning Model: Decision Tree",
    "text": "Machine Learning Model: Decision Tree\n\nSplit the data\n\n# Fitting the model\ntitanic_df2 = pd.get_dummies(titanic_df, columns=['Sex', 'Embarked', 'Name', 'Fare', 'Ticket', 'Cabin'], drop_first=True)\n\n\nX = titanic_df2.drop('Survived', axis=1)\ny = titanic_df2['Survived']\n\n\n\nCreate the model\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\ny.head()\n\n0    0\n1    1\n2    1\n3    1\n4    0\nName: Survived, dtype: int64\n\n\n\n\nTrain the model\n\n\nMake predictions\n\ntitanic_df2.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nName_group\nSex_male\nEmbarked_Q\nEmbarked_S\n...\nCabin_E8\nCabin_F E69\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n1\nTrue\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n2\n1\n1\n38.0\n1\n0\n1\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n3\n1\n3\n26.0\n0\n0\n1\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n4\n1\n1\n35.0\n1\n0\n1\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n5\n0\n3\n35.0\n0\n0\n1\nTrue\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows √ó 1973 columns\n\n\n\n\n# Predicting the Test set results\nDT = DecisionTreeClassifier()\nDT.fit(X_train, y_train)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\ncriterion¬†\n'gini'\n\n\n\nsplitter¬†\n'best'\n\n\n\nmax_depth¬†\nNone\n\n\n\nmin_samples_split¬†\n2\n\n\n\nmin_samples_leaf¬†\n1\n\n\n\nmin_weight_fraction_leaf¬†\n0.0\n\n\n\nmax_features¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\nmax_leaf_nodes¬†\nNone\n\n\n\nmin_impurity_decrease¬†\n0.0\n\n\n\nclass_weight¬†\nNone\n\n\n\nccp_alpha¬†\n0.0\n\n\n\nmonotonic_cst¬†\nNone\n\n\n\n\n            \n        \n    \n\n\n\ny_pred = DT.predict(X_test)\n\n\n\nEvaluate the Model\nAccuracy ‚Äì The percentage of total predictions that are correct.\nExample: If a spam filter correctly classifies 90 out of 100 emails (whether spam or not), the accuracy is 90%.\nF1 Score ‚Äì Out of all the positive predictions, how many were actually correct.\nExample: If a spam filter predicts 20 emails as spam, but only 15 are actually spam, precision is 15/20 = 75%.\nRecall Score ‚Äì Out of all the actual positive cases, how many did the model correctly identify.\nExample: If there were 25 spam emails in total, and the model correctly identified 15 of them, recall is 15/25 = 60%.\nPrecision Score ‚Äì A balance between precision and recall (harmonic mean).\nExample: If precision is 75% and recall is 60%, F1 score is (2 √ó 75 √ó 60) / (75 + 60) = 66.7%.\nAccuracy: 82.68 %. F1 Score: 0.82 %. Recall Score: 0.83 %. Precision Score: 0.83 %.\n\n# Accuracy, F1 Score, Recall score, Precision score\naccuracy = accuracy_score(y_test, y_pred)*100\nf1_score = f1_score(y_test, y_pred, average='weighted')\nrecall_score = recall_score(y_test, y_pred, average='weighted')\nprecision_score = precision_score(y_test, y_pred, average='weighted')\n\nprint('Accuracy: ' + str(round(accuracy, 2)) + ' %.')\nprint('F1 Score: ' + str(round(f1_score, 2)) + ' %.')\nprint('Recall Score: ' + str(round(recall_score, 2)) + ' %.')\nprint('Precision Score: ' + str(round(precision_score, 2)) + ' %.')\n\nAccuracy: 83.8 %.\nF1 Score: 0.84 %.\nRecall Score: 0.84 %.\nPrecision Score: 0.84 %.",
    "crumbs": [
      "Machine Learning projects",
      "Decision Tree & Random Forests Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#overview-of-random-forests",
    "href": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#overview-of-random-forests",
    "title": "Decision Tree Machine Learning Model üíª üß†",
    "section": "Overview of Random Forests",
    "text": "Overview of Random Forests\nRandom Forest (RF) is a supervised machine learning algorithm used for both classification and regression tasks. It builds multiple decision trees and combines their outputs for better accuracy and stability.\n\nDefinition\nA Random Forest is an ensemble learning method that creates a ‚Äúforest‚Äù of decision trees during training. Instead of relying on a single tree, Random Forest combines the predictions of multiple trees to improve performance. The structure consists of:\n\nDecision Trees: Individual trees that make predictions based on subsets of data and features.\nBagging (Bootstrap Aggregating): Each tree is trained on a randomly sampled subset of the data.\nVoting/Averaging: Combines the predictions from all trees:\nFor classification, the majority vote decides the class.\nFor regression, the average of all tree predictions is used.\nRandom Forest reduces the risk of overfitting compared to individual decision trees and works well for both categorical and numerical data.\n\n\nKey Concepts\n\nBagging:\n\n\nRandom Forest uses bootstrapping to train each tree on a different sample of the data.\nThis creates diversity among trees, making the model more robust.\n\n\nFeature Randomness:\n\n\nAt each split, Random Forest considers a random subset of features rather than all features.\nThis reduces correlation between trees and improves generalization.\n\n\nOut-of-Bag (OOB) Error:\n\n\nTrees not trained on certain data points (left out during bootstrapping) can be used to validate the model.\nOOB error gives an unbiased estimate of model performance.\n\n\nFeature Importance:\n\n\nRandom Forest provides a ranking of feature importance based on how often features are used for splitting across trees.\nUseful for identifying key predictors in your data. \n\nPros\nImproved Accuracy ‚Äì Combines multiple trees, reducing overfitting. Robust to Noise ‚Äì Handles outliers and noisy data better than individual trees. Handles Large Datasets ‚Äì Can scale well with more data. Feature Selection ‚Äì Provides insights into the importance of features. No Need for Feature Scaling ‚Äì Works with unscaled data, both numerical and categorical. \nCons\nLess Interpretable ‚Äì Harder to visualize compared to a single decision tree. Computationally Intensive ‚Äì Training many trees can take time and memory for large datasets. Overfitting Risk with Too Many Trees ‚Äì Although rare, excessive trees might still overfit without tuning. Slower Inference ‚Äì Predictions may take longer because they aggregate results from multiple trees. \nTips\n\nTune n_estimators ‚Äì Adjust the number of trees to balance accuracy and computational cost.\nLimit Tree Depth ‚Äì Use max_depth to avoid overfitting while maintaining performance.\nOptimize Feature Subset Size ‚Äì Use max_features to control how many features each tree considers at a split.\nUse Feature Importance ‚Äì Rank and prioritize the most important features in your dataset.\nCombine with Other Methods ‚Äì Random Forest pairs well with techniques like PCA for dimensionality reduction. \n\nUseful Articles and Videos\nhttps://www.datacamp.com/tutorial/random-forests-classifier-python\nhttps://www.ibm.com/topics/random-forest\nhttps://www.youtube.com/watch?v=J4Wdy0Wc_xQ\nhttps://www.youtube.com/watch?v=QHOazyP-YlM\n\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n\nCreate Model\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nTrain Model\n\ny.head()\n\n0    0\n1    1\n2    1\n3    1\n4    0\nName: Survived, dtype: int64\n\n\n\n\nMake Predictions\n\n# Predicting the Test set results\nRF = RandomForestClassifier()\nRF.fit(X_train, y_train)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators¬†\n100\n\n\n\ncriterion¬†\n'gini'\n\n\n\nmax_depth¬†\nNone\n\n\n\nmin_samples_split¬†\n2\n\n\n\nmin_samples_leaf¬†\n1\n\n\n\nmin_weight_fraction_leaf¬†\n0.0\n\n\n\nmax_features¬†\n'sqrt'\n\n\n\nmax_leaf_nodes¬†\nNone\n\n\n\nmin_impurity_decrease¬†\n0.0\n\n\n\nbootstrap¬†\nTrue\n\n\n\noob_score¬†\nFalse\n\n\n\nn_jobs¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\nverbose¬†\n0\n\n\n\nwarm_start¬†\nFalse\n\n\n\nclass_weight¬†\nNone\n\n\n\nccp_alpha¬†\n0.0\n\n\n\nmax_samples¬†\nNone\n\n\n\nmonotonic_cst¬†\nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nHyperparameter Search\n\ny_pred = RF.predict(X_test)\n\n\n# Get feature importance\nfeature_importances = RF.feature_importances_\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Get feature importance from the RandomForest model\nfeature_importances = RF.feature_importances_\n\n# Create a DataFrame for better visualization\nimportance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': feature_importances\n})\n\n# Sort the features by importance\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n# Plot the feature importances as a bar chart\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=importance_df, orient='v')\nplt.title(\"Feature Importance - Random Forest\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEvaluate the Model\n\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n\n# Assuming y_test and y_pred are defined somewhere in your code\naccuracy = accuracy_score(y_test, y_pred) * 100\nf1_value = f1_score(y_test, y_pred, average='weighted') * 100\nrecall_value = recall_score(y_test, y_pred, average='weighted') * 100\nprecision_value = precision_score(y_test, y_pred, average='weighted') * 100\n\nprint('Accuracy: ' + str(round(accuracy, 2)) + ' %.')\nprint('F1 Score: ' + str(round(f1_value, 2)) + ' %.')\nprint('Recall Score: ' + str(round(recall_value, 2)) + ' %.')\nprint('Precision Score: ' + str(round(precision_value, 2)) + ' %.')\n\nAccuracy: 81.01 %.\nF1 Score: 80.25 %.\nRecall Score: 81.01 %.\nPrecision Score: 82.3 %.",
    "crumbs": [
      "Machine Learning projects",
      "Decision Tree & Random Forests Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/4_Classification_Model/readme_data.html",
    "href": "Machine_Learning_Projects/4_Classification_Model/readme_data.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "‚Äì Data Overview ‚Äì\nThe bank customer churn dataset is a commonly used dataset for predicting customer churn in the banking industry. It contains information on bank customers who either left the bank or continue to be a customer.\nThe data dictionary: - Customer ID: A unique identifier for each customer - Surname: The customer‚Äôs surname or last name - Credit Score: A numerical value representing the customer‚Äôs credit score - Geography: The country where the customer resides (France, Spain or Germany) - Gender: The customer‚Äôs gender (Male or Female) - Age: The customer‚Äôs age. - Tenure: The number of years the customer has been with the bank - Balance: The customer‚Äôs account balance - NumOfProducts: The number of bank products the customer uses (e.g., savings account, credit card) - HasCrCard: Whether the customer has a credit card (1 = yes, 0 = no) - IsActiveMember: Whether the customer is an active member (1 = yes, 0 = no) - EstimatedSalary: The estimated salary of the customer - Exited: Whether the customer has churned (1 = yes, 0 = no)\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning_Projects/4_Classification_Model/readme.html",
    "href": "Machine_Learning_Projects/4_Classification_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "XGBoost Classification is a supervised machine learning algorithm that builds an ensemble of decision trees to predict categorical outcomes. It is optimized for speed and performance using gradient boosting techniques.\n\nData For the foundation dataset we will be using the bank customer churn dataset. It is a commonly used dataset for predicting customer turnover in the banking industry. For more information go to the data folder‚Äôs readme file.\n\nDefinition\nXGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:\n\nBoosting Trees: A collection of decision trees built sequentially to reduce errors.\n\nGradient Descent Optimization: Adjusts model weights using the gradient of a loss function.\n\nRegularization: Controls model complexity to prevent overfitting.\n\nFor classification, XGBoost predicts categorical outcomes by minimizing a chosen loss function, such as logistic loss for binary classification or softmax (cross-entropy) loss for multi-class classification.\n\nKey Concepts\n1. Boosting Mechanism:\n- Unlike a single decision tree, XGBoost builds multiple trees in sequence.\n- Each new tree corrects the errors of the previous ones by focusing on misclassified examples.\n\nLoss Functions:\n\nDetermines how errors are measured and minimized.\n\nCommon choices for classification include:\n\nLogistic Loss ‚Äì Used for binary classification tasks.\n\nSoftmax (Cross-Entropy Loss) ‚Äì Used for multi-class classification tasks.\n\n\nRegularization Techniques:\n\nPrevents overfitting by adding penalties to complex models.\n\nL1 Regularization (Lasso) ‚Äì Encourages sparsity by shrinking coefficients.\n\nL2 Regularization (Ridge) ‚Äì Penalizes large coefficients to reduce variance.\n\nFeature Importance & Selection:\n\nXGBoost ranks features by importance, aiding in feature selection.\n\nHelps in eliminating redundant or irrelevant features for better performance.\n\n\n\nPros\n1. High Performance ‚Äì Optimized for speed, scalability, and efficiency.\n2. Handles Missing Data ‚Äì Automatically learns how to deal with missing values.\n3. Regularization Built-in ‚Äì Reduces overfitting with L1 and L2 penalties.\n4. Probabilistic Predictions ‚Äì Provides probability scores for classification, enabling threshold tuning.\n5. Works Well with Large Datasets ‚Äì Efficient memory usage and parallel processing.\n\nCons\n1. Complexity ‚Äì More difficult to tune compared to simpler models.\n2. Computationally Intensive ‚Äì Training can be slow on very large datasets.\n3. Sensitive to Hyperparameters ‚Äì Performance depends on careful tuning of learning rate, tree depth, and regularization.\n4. Less Interpretable ‚Äì Decision boundaries may be challenging to interpret compared to simpler models.\n\nTips\n* Optimize Hyperparameters ‚Äì Use grid search or Bayesian optimization for tuning.\n* Use Early Stopping ‚Äì Stop training if performance ceases to improve on validation data.\n* Scale Features if Needed ‚Äì Although XGBoost can handle unscaled data, standardization might improve performance.\n* Leverage Feature Importance ‚Äì Identify and remove less relevant features to improve efficiency.\n* Adjust Decision Thresholds ‚Äì Fine-tune the probability threshold to balance precision and recall for your specific task.\n\nUseful Articles and Videos\n* XGBoost Official Documentation\n* XGBoost for Classification ‚Äì Machine Learning Mastery\n* Understanding XGBoost ‚Äì Analytics Vidhya\n* XGBoost Explained for Classification ‚Äì YouTube\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/healthcare_machine_learning.html",
    "href": "notebooks/healthcare_machine_learning.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "!pip install xgboost imbalanced-learn\n!pip install scikeras\n!pip install imbalanced-learn\n\nRequirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\nRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\nRequirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\nRequirement already satisfied: scikit-learn&lt;2,&gt;=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\nRequirement already satisfied: sklearn-compat&lt;1,&gt;=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\nRequirement already satisfied: joblib&lt;2,&gt;=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl&lt;4,&gt;=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\nRequirement already satisfied: scikeras in /usr/local/lib/python3.11/dist-packages (0.13.0)\nRequirement already satisfied: keras&gt;=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\nRequirement already satisfied: scikit-learn&gt;=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.6.1)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras&gt;=3.2.0-&gt;scikeras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras&gt;=3.2.0-&gt;scikeras) (2.0.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras&gt;=3.2.0-&gt;scikeras) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras&gt;=3.2.0-&gt;scikeras) (0.1.0)\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras&gt;=3.2.0-&gt;scikeras) (3.14.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras&gt;=3.2.0-&gt;scikeras) (0.16.0)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras&gt;=3.2.0-&gt;scikeras) (0.4.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras&gt;=3.2.0-&gt;scikeras) (25.0)\nRequirement already satisfied: scipy&gt;=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn&gt;=1.4.2-&gt;scikeras) (1.15.3)\nRequirement already satisfied: joblib&gt;=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn&gt;=1.4.2-&gt;scikeras) (1.5.1)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn&gt;=1.4.2-&gt;scikeras) (3.6.0)\nRequirement already satisfied: typing-extensions&gt;=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree-&gt;keras&gt;=3.2.0-&gt;scikeras) (4.14.1)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich-&gt;keras&gt;=3.2.0-&gt;scikeras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich-&gt;keras&gt;=3.2.0-&gt;scikeras) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras&gt;=3.2.0-&gt;scikeras) (0.1.2)\nRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\nRequirement already satisfied: numpy&lt;3,&gt;=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\nRequirement already satisfied: scipy&lt;2,&gt;=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.3)\nRequirement already satisfied: scikit-learn&lt;2,&gt;=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\nRequirement already satisfied: sklearn-compat&lt;1,&gt;=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\nRequirement already satisfied: joblib&lt;2,&gt;=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl&lt;4,&gt;=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv('/content/drive/MyDrive/diabetes.csv')\nprint(df.head())\n\n   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0            6      148             72             35        0  33.6   \n1            1       85             66             29        0  26.6   \n2            8      183             64              0        0  23.3   \n3            1       89             66             23       94  28.1   \n4            0      137             40             35      168  43.1   \n\n   DiabetesPedigreeFunction  Age  Outcome  \n0                     0.627   50        1  \n1                     0.351   31        0  \n2                     0.672   32        1  \n3                     0.167   21        0  \n4                     2.288   33        1  \n\n\n\nzero_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\ndf[zero_cols] = df[zero_cols].replace(0, np.nan)\ndf[zero_cols] = df[zero_cols].fillna(df[zero_cols].median())\n\n# Feature Engineering\ndf['Age_BMI'] = df['Age'] * df['BMI']\ndf['Glucose2'] = df['Glucose'] ** 2\ndf['Is_Obese'] = (df['BMI'] &gt; 30).astype(int)\n\n# Prepare features\nX = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\n# Polynomial features\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\nX_poly = poly.fit_transform(X)\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_poly)\n\n# Apply SMOTE\nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(X_scaled, y)\n\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42, stratify=y_res)\n\n# Compute class weights from original y (not y_res)\nfrom sklearn.utils.class_weight import compute_class_weight\nclasses = np.unique(y)\nweights = compute_class_weight(class_weight='balanced', classes=classes, y=y)\nclass_weights = dict(zip(classes, weights))\n\n# XGBoost tuning\nxgb_params = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5],\n    'learning_rate': [0.01, 0.1],\n    'subsample': [0.8, 1.0]\n}\nxgb_base = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, scale_pos_weight=weights[1]/weights[0])\nxgb_grid = GridSearchCV(xgb_base, xgb_params, cv=3, scoring='roc_auc', n_jobs=-1)\nxgb_grid.fit(X_train, y_train)\nbest_xgb = xgb_grid.best_estimator_\n\n# Random Forest tuning\nrf_params = {\n    'n_estimators': [100, 200],\n    'max_depth': [6, 8],\n    'max_features': ['sqrt', 'log2']\n}\nrf = RandomForestClassifier(random_state=42, class_weight=class_weights)\nrf_grid = GridSearchCV(rf, rf_params, cv=3, scoring='roc_auc', n_jobs=-1)\nrf_grid.fit(X_train, y_train)\nbest_rf = rf_grid.best_estimator_\n\n# Logistic Regression meta learner tuning\nmeta_params = {\n    'C': [0.1, 1, 10],\n    'solver': ['lbfgs'],\n    'penalty': ['l2']\n}\nmeta = LogisticRegression(max_iter=1000, class_weight=class_weights)\nmeta_grid = GridSearchCV(meta, meta_params, cv=3, scoring='roc_auc', n_jobs=-1)\nmeta_grid.fit(X_train, y_train)\nbest_meta = meta_grid.best_estimator_\n\n# Stacking ensemble\nstack = StackingClassifier(\n    estimators=[('xgb', best_xgb), ('rf', best_rf)],\n    final_estimator=best_meta,\n    passthrough=True,\n    n_jobs=-1\n)\n\n# Cross-validation on stacking ensemble\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nprobs = np.zeros(len(y_test))\npreds = np.zeros(len(y_test))\n\nfor train_idx, val_idx in kf.split(X_train, y_train):\n    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n    stack.fit(X_tr, y_tr)\n    probs += stack.predict_proba(X_test)[:, 1] / kf.n_splits\n    preds += stack.predict(X_test) / kf.n_splits\n\nfinal_preds = (preds &gt;= 0.5).astype(int)\n\n\n# Results\nprint(\"Cross-Validated Stacking Ensemble Results\")\nprint(classification_report(y_test, final_preds))\nprint(\"ROC AUC (CV ensemble):\", roc_auc_score(y_test, probs))\n\n\n\n\n Back to top"
  },
  {
    "objectID": "machine_learning.html",
    "href": "machine_learning.html",
    "title": "Machine Learning - Bootcamp",
    "section": "",
    "text": "Data Import:\n‚Ä¢ Uses the Iris dataset and heart disease datasets from UCI for classification tasks.\nData Preprocessing:\n‚Ä¢ Includes splitting the data into training and test sets.\nModel Construction:\n‚Ä¢ Applies the K-Nearest Neighbors algorithm with n_neighbors=3 to classify data.\nEvaluation:\n‚Ä¢ Measures model accuracy using accuracy score on the test set.\nVisualization:\n‚Ä¢ Utilizes pairplots for data exploration and visualizes feature relationships.\n\n\n\nData Import:\n‚Ä¢ Imports Titanic dataset from Data Science Dojo for classification tasks.\nData Preprocessing:\n‚Ä¢ Processes the data using one-hot encoding for categorical features and feature extraction (e.g., grouping names into categories).\n‚Ä¢ Splits the data into training and test sets.\nModel Construction:\n‚Ä¢ Applies a Decision Tree Classifier to predict passenger survival (Survived) based on various features such as passenger class, sex, age, etc.\n‚Ä¢ Hyperparameters such as max_depth and pruning are essential to control tree complexity.\nEvaluation:\n‚Ä¢ Evaluates the model performance using accuracy, F1 score, recall, and precision scores. These metrics help assess the model‚Äôs ability to predict survival correctly and balance false positives/negatives.\nVisualization:\n‚Ä¢ Visualizes the survival distribution by passenger class using seaborn count plots and explores relationships with survival using the LetsPlot library.\n\n\n\nData Import:\n‚Ä¢ Imports data from a CSV file containing bicycle count data.\nData Preprocessing:\n‚Ä¢ Converts categorical columns (‚ÄúDate‚Äù, ‚ÄúDay‚Äù, ‚ÄúPrecipitation‚Äù) into dummy variables using one-hot encoding.\n‚Ä¢ Splits the dataset into features (X) and target variable (y), where the target variable is the Total bicycle count.\n‚Ä¢ Further splits the dataset into training and testing sets for model evaluation.\nModel Construction:\n‚Ä¢ Creates an XGBoost regression model using reg:squarederror as the objective function.\n‚Ä¢ Builds the model by training it with the training dataset using 100 boosting rounds, a learning rate of 0.1, and a maximum tree depth of 6.\nEvaluation:\n‚Ä¢ Evaluates the model performance using regression metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared (R2) to assess the model‚Äôs prediction accuracy.\nHyperparameter Tuning:\n‚Ä¢ Uses GridSearchCV to tune hyperparameters like max_depth, learning_rate, n_estimators, subsample, and colsample_bytree to optimize the model for better performance. The grid search utilizes cross-validation to find the best hyperparameter combination.\nVisualization:\n‚Ä¢ Use feature importance scores helps identify which features have the most impact on predictions.\n\n\n\nData Import:\n‚Ä¢ Uses the Titanic dataset, similar to the Decision Tree example, for classification tasks.\nData Preprocessing:\n‚Ä¢ Applies one-hot encoding and splits the dataset into training and test sets.\nModel Construction:\n‚Ä¢ Builds a Random Forest Classifier consisting of multiple decision trees that aggregate results to improve predictive accuracy.\nEvaluation:\n‚Ä¢ Measures model performance using accuracy, F1 score, recall, and precision metrics, ensuring that the model is well-calibrated and robust.\nHyperparameter Tuning:\n‚Ä¢ Uses hyperparameter tuning (e.g., n_estimators, max_depth) to optimize the Random Forest model for better performance.\nVisualization:\n‚Ä¢ Use plots to explore the dataset and understand the relationships between variables.\n\n\n\nData Import:\n‚Ä¢ Loads a banking dataset to predict customer churn (whether a customer leaves the bank or not).\nData Preprocessing:\n‚Ä¢ Drops irrelevant columns like ‚ÄòExited‚Äô and ‚ÄòSurname‚Äô from the dataset.\n‚Ä¢ Applies ordinal encoding to categorical features such as ‚ÄòGeography‚Äô and ‚ÄòGender‚Äô.\n‚Ä¢ Splits the dataset into training and test sets for model evaluation.\nFeature Engineering:\n‚Ä¢ Creates a list of categorical features and prepares them for transformation using a column transformer.\n‚Ä¢ Includes feature augmentation to enhance the dataset‚Äôs quality and make it more suitable for model training.\nModel Construction:\n‚Ä¢ Builds an XGBoost classification model, an ensemble of decision trees optimized for accuracy and performance using gradient boosting.\n‚Ä¢ Initializes the model with specific hyperparameters, followed by training using the processed data.\nEvaluation:\n‚Ä¢ Evaluates the model‚Äôs performance using various metrics like accuracy, F1 score, precision, and recall to assess its predictive power and reliability.\n‚Ä¢ Prints out model performance measures such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared (R2) for a comprehensive evaluation.\nHyperparameter Tuning:\n‚Ä¢ Uses GridSearchCV for hyperparameter tuning, searching for optimal values for parameters such as learning_rate, n_estimators, and max_depth.\nVisualization:\n‚Ä¢ Uses Seaborn to visualize feature relationships and the impact of various features (e.g., Geography, IsActiveMember) on the ‚ÄúExited‚Äù outcome.",
    "crumbs": [
      "Machine Learning projects",
      "Machine Learning Projects"
    ]
  },
  {
    "objectID": "machine_learning.html#my-projects",
    "href": "machine_learning.html#my-projects",
    "title": "Machine Learning - Bootcamp",
    "section": "",
    "text": "Data Import:\n‚Ä¢ Uses the Iris dataset and heart disease datasets from UCI for classification tasks.\nData Preprocessing:\n‚Ä¢ Includes splitting the data into training and test sets.\nModel Construction:\n‚Ä¢ Applies the K-Nearest Neighbors algorithm with n_neighbors=3 to classify data.\nEvaluation:\n‚Ä¢ Measures model accuracy using accuracy score on the test set.\nVisualization:\n‚Ä¢ Utilizes pairplots for data exploration and visualizes feature relationships.\n\n\n\nData Import:\n‚Ä¢ Imports Titanic dataset from Data Science Dojo for classification tasks.\nData Preprocessing:\n‚Ä¢ Processes the data using one-hot encoding for categorical features and feature extraction (e.g., grouping names into categories).\n‚Ä¢ Splits the data into training and test sets.\nModel Construction:\n‚Ä¢ Applies a Decision Tree Classifier to predict passenger survival (Survived) based on various features such as passenger class, sex, age, etc.\n‚Ä¢ Hyperparameters such as max_depth and pruning are essential to control tree complexity.\nEvaluation:\n‚Ä¢ Evaluates the model performance using accuracy, F1 score, recall, and precision scores. These metrics help assess the model‚Äôs ability to predict survival correctly and balance false positives/negatives.\nVisualization:\n‚Ä¢ Visualizes the survival distribution by passenger class using seaborn count plots and explores relationships with survival using the LetsPlot library.\n\n\n\nData Import:\n‚Ä¢ Imports data from a CSV file containing bicycle count data.\nData Preprocessing:\n‚Ä¢ Converts categorical columns (‚ÄúDate‚Äù, ‚ÄúDay‚Äù, ‚ÄúPrecipitation‚Äù) into dummy variables using one-hot encoding.\n‚Ä¢ Splits the dataset into features (X) and target variable (y), where the target variable is the Total bicycle count.\n‚Ä¢ Further splits the dataset into training and testing sets for model evaluation.\nModel Construction:\n‚Ä¢ Creates an XGBoost regression model using reg:squarederror as the objective function.\n‚Ä¢ Builds the model by training it with the training dataset using 100 boosting rounds, a learning rate of 0.1, and a maximum tree depth of 6.\nEvaluation:\n‚Ä¢ Evaluates the model performance using regression metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared (R2) to assess the model‚Äôs prediction accuracy.\nHyperparameter Tuning:\n‚Ä¢ Uses GridSearchCV to tune hyperparameters like max_depth, learning_rate, n_estimators, subsample, and colsample_bytree to optimize the model for better performance. The grid search utilizes cross-validation to find the best hyperparameter combination.\nVisualization:\n‚Ä¢ Use feature importance scores helps identify which features have the most impact on predictions.\n\n\n\nData Import:\n‚Ä¢ Uses the Titanic dataset, similar to the Decision Tree example, for classification tasks.\nData Preprocessing:\n‚Ä¢ Applies one-hot encoding and splits the dataset into training and test sets.\nModel Construction:\n‚Ä¢ Builds a Random Forest Classifier consisting of multiple decision trees that aggregate results to improve predictive accuracy.\nEvaluation:\n‚Ä¢ Measures model performance using accuracy, F1 score, recall, and precision metrics, ensuring that the model is well-calibrated and robust.\nHyperparameter Tuning:\n‚Ä¢ Uses hyperparameter tuning (e.g., n_estimators, max_depth) to optimize the Random Forest model for better performance.\nVisualization:\n‚Ä¢ Use plots to explore the dataset and understand the relationships between variables.\n\n\n\nData Import:\n‚Ä¢ Loads a banking dataset to predict customer churn (whether a customer leaves the bank or not).\nData Preprocessing:\n‚Ä¢ Drops irrelevant columns like ‚ÄòExited‚Äô and ‚ÄòSurname‚Äô from the dataset.\n‚Ä¢ Applies ordinal encoding to categorical features such as ‚ÄòGeography‚Äô and ‚ÄòGender‚Äô.\n‚Ä¢ Splits the dataset into training and test sets for model evaluation.\nFeature Engineering:\n‚Ä¢ Creates a list of categorical features and prepares them for transformation using a column transformer.\n‚Ä¢ Includes feature augmentation to enhance the dataset‚Äôs quality and make it more suitable for model training.\nModel Construction:\n‚Ä¢ Builds an XGBoost classification model, an ensemble of decision trees optimized for accuracy and performance using gradient boosting.\n‚Ä¢ Initializes the model with specific hyperparameters, followed by training using the processed data.\nEvaluation:\n‚Ä¢ Evaluates the model‚Äôs performance using various metrics like accuracy, F1 score, precision, and recall to assess its predictive power and reliability.\n‚Ä¢ Prints out model performance measures such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared (R2) for a comprehensive evaluation.\nHyperparameter Tuning:\n‚Ä¢ Uses GridSearchCV for hyperparameter tuning, searching for optimal values for parameters such as learning_rate, n_estimators, and max_depth.\nVisualization:\n‚Ä¢ Uses Seaborn to visualize feature relationships and the impact of various features (e.g., Geography, IsActiveMember) on the ‚ÄúExited‚Äù outcome.",
    "crumbs": [
      "Machine Learning projects",
      "Machine Learning Projects"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#overview",
    "href": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#overview",
    "title": "Classification Machine Learning Model üß† üìà",
    "section": "Overview",
    "text": "Overview\nXGBoost Classification is a supervised machine learning algorithm that builds an ensemble of decision trees to predict categorical outcomes. It is optimized for speed and performance using gradient boosting techniques.\n\n\n\nDefinition\nXGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:\n\nBoosting Trees: A collection of decision trees built sequentially to reduce errors.\n\nGradient Descent Optimization: Adjusts model weights using the gradient of a loss function.\n\nRegularization: Controls model complexity to prevent overfitting.\n\nFor classification, XGBoost predicts categorical outcomes by minimizing a chosen loss function, such as logistic loss for binary classification or softmax (cross-entropy) loss for multi-class classification.\n\n\n\nKey Concepts\n1. Boosting Mechanism:\n- Unlike a single decision tree, XGBoost builds multiple trees in sequence.\n- Each new tree corrects the errors of the previous ones by focusing on misclassified examples.\n\nLoss Functions:\n\nDetermines how errors are measured and minimized.\n\nCommon choices for classification include:\n\nLogistic Loss ‚Äì Used for binary classification tasks.\n\nSoftmax (Cross-Entropy Loss) ‚Äì Used for multi-class classification tasks.\n\n\nRegularization Techniques:\n\nPrevents overfitting by adding penalties to complex models.\n\nL1 Regularization (Lasso) ‚Äì Encourages sparsity by shrinking coefficients.\n\nL2 Regularization (Ridge) ‚Äì Penalizes large coefficients to reduce variance.\n\nFeature Importance & Selection:\n\nXGBoost ranks features by importance, aiding in feature selection.\n\nHelps in eliminating redundant or irrelevant features for better performance.\n\n\n\n\n\nPros\n1. High Performance ‚Äì Optimized for speed, scalability, and efficiency.\n2. Handles Missing Data ‚Äì Automatically learns how to deal with missing values.\n3. Regularization Built-in ‚Äì Reduces overfitting with L1 and L2 penalties.\n4. Probabilistic Predictions ‚Äì Provides probability scores for classification, enabling threshold tuning.\n5. Works Well with Large Datasets ‚Äì Efficient memory usage and parallel processing.\n\n\n\nCons\n1. Complexity ‚Äì More difficult to tune compared to simpler models.\n2. Computationally Intensive ‚Äì Training can be slow on very large datasets.\n3. Sensitive to Hyperparameters ‚Äì Performance depends on careful tuning of learning rate, tree depth, and regularization.\n4. Less Interpretable ‚Äì Decision boundaries may be challenging to interpret compared to simpler models.\n\n\n\nTips\n* Optimize Hyperparameters ‚Äì Use grid search or Bayesian optimization for tuning.\n* Use Early Stopping ‚Äì Stop training if performance ceases to improve on validation data.\n* Scale Features if Needed ‚Äì Although XGBoost can handle unscaled data, standardization might improve performance.\n* Leverage Feature Importance ‚Äì Identify and remove less relevant features to improve efficiency.\n* Adjust Decision Thresholds ‚Äì Fine-tune the probability threshold to balance precision and recall for your specific task.\n\n\n\nUseful Articles and Videos\n* XGBoost Official Documentation\n* XGBoost for Classification ‚Äì Machine Learning Mastery\n* Understanding XGBoost ‚Äì Analytics Vidhya\n* XGBoost Explained for Classification ‚Äì YouTube",
    "crumbs": [
      "Machine Learning projects",
      "Classification Model (XGBoost)"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#import-datalibraries",
    "href": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#import-datalibraries",
    "title": "Classification Machine Learning Model üß† üìà",
    "section": "Import Data/Libraries",
    "text": "Import Data/Libraries\n\n!pip install lets_plot\n\nRequirement already satisfied: lets_plot in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (4.7.1)\nRequirement already satisfied: pypng in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (from lets_plot) (0.20220715.0)\nRequirement already satisfied: palettable in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (from lets_plot) (3.3.3)\nRequirement already satisfied: pillow in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (from lets_plot) (11.3.0)\n\n\n\n# needed libraries for Classification models\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import f1_score\nfrom xgboost import XGBClassifier\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Load the training dataset\n#### If this gives an error go into the Data folder in GitHub and click on the data csv and then \"Raw\"\n#### (underneath history in the upper righthand corner) then copy that url to replace the \"data_raw_url\"\ndata_raw_url = 'Churn_Modelling.csv'\nbanking_df = pd.read_csv(data_raw_url)",
    "crumbs": [
      "Machine Learning projects",
      "Classification Model (XGBoost)"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#explore-visualize-and-understand-the-data",
    "href": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#explore-visualize-and-understand-the-data",
    "title": "Classification Machine Learning Model üß† üìà",
    "section": "Explore, Visualize and Understand the Data",
    "text": "Explore, Visualize and Understand the Data\n\nbanking_df.head(10)\n\n\n\n\n\n\n\n\nRowNumber\nCustomerId\nSurname\nCreditScore\nGeography\nGender\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\n\n\n\n\n0\n1\n15634602\nHargrave\n619\nFrance\nFemale\n42.0\n2\n0.00\n1\n1.0\n1.0\n101348.88\n1\n\n\n1\n2\n15647311\nHill\n608\nSpain\nFemale\n41.0\n1\n83807.86\n1\n0.0\n1.0\n112542.58\n0\n\n\n2\n3\n15619304\nOnio\n502\nFrance\nFemale\n42.0\n8\n159660.80\n3\n1.0\n0.0\n113931.57\n1\n\n\n3\n4\n15701354\nBoni\n699\nFrance\nFemale\n39.0\n1\n0.00\n2\n0.0\n0.0\n93826.63\n0\n\n\n4\n5\n15737888\nMitchell\n850\nSpain\nFemale\n43.0\n2\n125510.82\n1\nNaN\n1.0\n79084.10\n0\n\n\n5\n6\n15574012\nChu\n645\nSpain\nMale\n44.0\n8\n113755.78\n2\n1.0\n0.0\n149756.71\n1\n\n\n6\n7\n15592531\nBartlett\n822\nNaN\nMale\n50.0\n7\n0.00\n2\n1.0\n1.0\n10062.80\n0\n\n\n7\n8\n15656148\nObinna\n376\nGermany\nFemale\n29.0\n4\n115046.74\n4\n1.0\n0.0\n119346.88\n1\n\n\n8\n9\n15792365\nHe\n501\nFrance\nMale\n44.0\n4\n142051.07\n2\n0.0\nNaN\n74940.50\n0\n\n\n9\n10\n15592389\nH?\n684\nFrance\nMale\nNaN\n2\n134603.88\n1\n1.0\n1.0\n71725.73\n0\n\n\n\n\n\n\n\n\nbanking_df.describe()\n\n\n\n\n\n\n\n\nRowNumber\nCustomerId\nCreditScore\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\n\n\n\n\ncount\n10002.000000\n1.000200e+04\n10002.000000\n10001.000000\n10002.000000\n10002.000000\n10002.000000\n10001.000000\n10001.000000\n10002.000000\n10002.000000\n\n\nmean\n5001.499600\n1.569093e+07\n650.555089\n38.922311\n5.012498\n76491.112875\n1.530194\n0.705529\n0.514949\n100083.331145\n0.203759\n\n\nstd\n2887.472338\n7.193177e+04\n96.661615\n10.487200\n2.891973\n62393.474144\n0.581639\n0.455827\n0.499801\n57508.117802\n0.402812\n\n\nmin\n1.000000\n1.556570e+07\n350.000000\n18.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n11.580000\n0.000000\n\n\n25%\n2501.250000\n1.562852e+07\n584.000000\n32.000000\n3.000000\n0.000000\n1.000000\n0.000000\n0.000000\n50983.750000\n0.000000\n\n\n50%\n5001.500000\n1.569073e+07\n652.000000\n37.000000\n5.000000\n97198.540000\n1.000000\n1.000000\n1.000000\n100185.240000\n0.000000\n\n\n75%\n7501.750000\n1.575323e+07\n718.000000\n44.000000\n7.000000\n127647.840000\n2.000000\n1.000000\n1.000000\n149383.652500\n0.000000\n\n\nmax\n10000.000000\n1.581569e+07\n850.000000\n92.000000\n10.000000\n250898.090000\n4.000000\n1.000000\n1.000000\n199992.480000\n1.000000\n\n\n\n\n\n\n\n\nbanking_df.dtypes\n\nRowNumber            int64\nCustomerId           int64\nSurname             object\nCreditScore          int64\nGeography           object\nGender              object\nAge                float64\nTenure               int64\nBalance            float64\nNumOfProducts        int64\nHasCrCard          float64\nIsActiveMember     float64\nEstimatedSalary    float64\nExited               int64\ndtype: object\n\n\n\n# 0 means staying and 1 means leaving.\n# Most customers who left the bank were from Germany compared to Spain and France. Does this indicate that geography significantly influences the decision to stay or leave?\nplt.figure(figsize=(10, 6))\nsns.countplot(x='Geography', hue='Exited', data=banking_df)\nplt.title('Geography vs Exited')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.countplot(data = banking_df, x = \"IsActiveMember\", hue = \"Exited\", palette= \"Set2\")\nplt.title(\"Exited by Status\")\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.boxplot(data = banking_df, x = \"Geography\", y = \"Exited\", hue = \"Geography\", palette= \"Set2\")\nplt.title(\"Exited by Geography\")\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.boxplot(data = banking_df, x = \"Geography\", y = \"Exited\", hue = \"Exited\", palette= \"Set2\")\nplt.title(\"Exited by Estimated Salary\")\nplt.show()",
    "crumbs": [
      "Machine Learning projects",
      "Classification Model (XGBoost)"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#feature-enginnering-and-data-augmentation",
    "href": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#feature-enginnering-and-data-augmentation",
    "title": "Classification Machine Learning Model üß† üìà",
    "section": "Feature Enginnering and Data Augmentation",
    "text": "Feature Enginnering and Data Augmentation\n\nX = banking_df.drop(['Exited', 'Surname'], axis=1)\ny = banking_df['Exited']\n\n\nData Augmentation\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\nFeature Engineering\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns.\n\ncategorical_features = ['Geography', 'Gender']",
    "crumbs": [
      "Machine Learning projects",
      "Classification Model (XGBoost)"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#machine-learning-model",
    "href": "Machine_Learning_Projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#machine-learning-model",
    "title": "Classification Machine Learning Model üß† üìà",
    "section": "Machine Learning Model",
    "text": "Machine Learning Model\n\nSplit the data to train and test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nCreate the model\n\n# Create an ordinal encoder\nordinal_encoder = OrdinalEncoder()\n\n# Create a column transformer to apply ordinal encoding to categorical features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', ordinal_encoder, categorical_features)\n    ],\n    remainder='passthrough'  # Keep other columns unchanged\n)\n\n\n\nTrain the model\n\n# Fit and transform the training data\nX_train_encoded = preprocessor.fit_transform(X_train)\n\n# Transform the test data using the fitted preprocessor\nX_test_encoded = preprocessor.transform(X_test)\n\n\n\nMake predictions\n\n# Initialize the XGBoost classifier\nxgb_classifier = XGBClassifier(random_state=42)\n\n\nHyperparameter Search\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid = {\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7],\n}\n\n# Perform GridSearchCV\ngrid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=3, scoring='f1')\ngrid_search.fit(X_train_encoded, y_train)\n\n# Get the best model from GridSearchCV\nbest_xgb_classifier = grid_search.best_estimator_\n\n# Make predictions using the best model\ny_pred = best_xgb_classifier.predict(X_test_encoded)\n\n\n\n\nEvaluate the Model\nAccuracy ‚Äì The percentage of total predictions that are correct.\nExample: If a spam filter correctly classifies 90 out of 100 emails (whether spam or not), the accuracy is 90%.\nF1 Score ‚Äì Out of all the positive predictions, how many were actually correct.\nExample: If a spam filter predicts 20 emails as spam, but only 15 are actually spam, precision is 15/20 = 75%.\nRecall Score ‚Äì Out of all the actual positive cases, how many did the model correctly identify.\nExample: If there were 25 spam emails in total, and the model correctly identified 15 of them, recall is 15/25 = 60%.\nPrecision Score ‚Äì A balance between precision and recall (harmonic mean).\nExample: If precision is 75% and recall is 60%, F1 score is (2 √ó 75 √ó 60) / (75 + 60) = 66.7%.\n\n# Evaluate the model using classification metrics\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mse)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\n\n# Calculate R-squared (R2)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint('Mean Squared Error (MSE):', mse)\nprint('Root Mean Squared Error (RMSE):', rmse)\nprint('Mean Absolute Error (MAE):', mae)\nprint('R-squared (R2):', r2)\n\nMean Squared Error (MSE): 0.14642678660669664\nRoot Mean Squared Error (RMSE): 0.38265753175221395\nMean Absolute Error (MAE): 0.14642678660669664\nR-squared (R2): 0.08790475390402563",
    "crumbs": [
      "Machine Learning projects",
      "Classification Model (XGBoost)"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/readme.html",
    "href": "Machine_Learning_Projects/2_DecisionTree_RandomForests_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "Decision Tree (DT) is a supervised machine learning algorithm used for both classification and regression tasks. It splits data into branches based on feature values to make predictions.\nDefinition A Decision Tree is a flowchart-like model where data is split into smaller subsets based on feature conditions. The structure consists of:\nRoot Node: The starting point that represents the entire dataset. Decision Nodes: Points where the data is split based on a feature. Leaves: Terminal nodes that provide the final prediction. For classification, a Decision Tree assigns class labels based on feature splits. For regression, it predicts continuous values using the average or mean of data points in each leaf.\nKey Concepts\nSplitting Criteria:\nDetermines how the dataset is divided at each step. Common methods: Gini Impurity (Classification) ‚Äì Measures the likelihood of incorrect classification. Entropy (Classification) ‚Äì Uses information gain to decide splits. Mean Squared Error (MSE) (Regression) ‚Äì Measures variance within nodes. Tree Depth & Overfitting:\nDeeper trees fit training data better but may overfit. Pruning (removing unnecessary branches) improves generalization. Feature Importance:\nDecision Trees rank features by their impact on predictions. Helps in feature selection for other models. Handling Missing Data:\nSome implementations allow surrogate splits to handle missing values.\nPros\nEasy to Understand & Interpret ‚Äì Can be visualized as a simple flowchart. No Need for Feature Scaling ‚Äì Works with both categorical and numerical features. Handles Non-Linearity ‚Äì Can model complex relationships without requiring transformation. Fast for Small Datasets ‚Äì Training and inference are relatively quick.\nCons\nProne to Overfitting ‚Äì Deep trees can memorize training data, reducing generalization. Unstable to Small Changes ‚Äì Small variations in data can change the tree structure significantly. Less Efficient on Large Datasets ‚Äì Computationally expensive for large datasets.\nTips\nLimit Tree Depth ‚Äì Use max_depth to prevent overfitting. Pruning Techniques ‚Äì Use pre-pruning (max_leaf_nodes) or post-pruning to remove unnecessary branches. Use Feature Importance ‚Äì Identify the most influential features and remove irrelevant ones. Consider Ensemble Methods ‚Äì Random Forest or Gradient Boosting improve Decision Trees by reducing variance.\nUseful Articles and Videos\nhttps://www.datacamp.com/tutorial/decision-tree-classification-python https://www.ibm.com/think/topics/decision-trees https://www.youtube.com/watch?v=6DlWndLbk90 https://www.youtube.com/watch?v=ZOiBe-nrmc4\nRandom Forest (RF) is a supervised machine learning algorithm used for both classification and regression tasks. It builds multiple decision trees and combines their outputs for better accuracy and stability.\nDefinition A Random Forest is an ensemble learning method that creates a ‚Äúforest‚Äù of decision trees during training. Instead of relying on a single tree, Random Forest combines the predictions of multiple trees to improve performance. The structure consists of:\nDecision Trees: Individual trees that make predictions based on subsets of data and features. Bagging (Bootstrap Aggregating): Each tree is trained on a randomly sampled subset of the data. Voting/Averaging: Combines the predictions from all trees: For classification, the majority vote decides the class. For regression, the average of all tree predictions is used. Random Forest reduces the risk of overfitting compared to individual decision trees and works well for both categorical and numerical data.\nKey Concepts\nBagging: Random Forest uses bootstrapping to train each tree on a different sample of the data. This creates diversity among trees, making the model more robust. Feature Randomness: At each split, Random Forest considers a random subset of features rather than all features. This reduces correlation between trees and improves generalization. Out-of-Bag (OOB) Error: Trees not trained on certain data points (left out during bootstrapping) can be used to validate the model. OOB error gives an unbiased estimate of model performance. Feature Importance: Random Forest provides a ranking of feature importance based on how often features are used for splitting across trees. Useful for identifying key predictors in your data. Pros\nImproved Accuracy ‚Äì Combines multiple trees, reducing overfitting. Robust to Noise ‚Äì Handles outliers and noisy data better than individual trees. Handles Large Datasets ‚Äì Can scale well with more data. Feature Selection ‚Äì Provides insights into the importance of features. No Need for Feature Scaling ‚Äì Works with unscaled data, both numerical and categorical.\nCons\nLess Interpretable ‚Äì Harder to visualize compared to a single decision tree. Computationally Intensive ‚Äì Training many trees can take time and memory for large datasets. Overfitting Risk with Too Many Trees ‚Äì Although rare, excessive trees might still overfit without tuning. Slower Inference ‚Äì Predictions may take longer because they aggregate results from multiple trees.\nTips\nTune n_estimators ‚Äì Adjust the number of trees to balance accuracy and computational cost. Limit Tree Depth ‚Äì Use max_depth to avoid overfitting while maintaining performance. Optimize Feature Subset Size ‚Äì Use max_features to control how many features each tree considers at a split. Use Feature Importance ‚Äì Rank and prioritize the most important features in your dataset. Combine with Other Methods ‚Äì Random Forest pairs well with techniques like PCA for dimensionality reduction. Useful Articles and Videos\nhttps://www.datacamp.com/tutorial/random-forests-classifier-python https://www.ibm.com/topics/random-forest https://www.youtube.com/watch?v=J4Wdy0Wc_xQ https://www.youtube.com/watch?v=QHOazyP-YlM\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning_Projects/3_Regression_Model/readme.html",
    "href": "Machine_Learning_Projects/3_Regression_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "There are two datasets in the Data folder that will be used to learn regression. 1. New York City bicycle routes through bridges 2. Idaho Falls Chukars (Pioneer League Baseball) data to find the optimal amount of pitches to throw out a batter based on the teamID (player is a trench goal).\nOverview: XGBoost Regression is a supervised machine learning algorithm that builds an ensemble of decision trees to predict continuous values. It is optimized for speed and performance using gradient boosting techniques.\n\nDefinition XGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:\n\nBoosting Trees: A collection of decision trees built sequentially to reduce errors.\n\nGradient Descent Optimization: Adjusts model weights using the gradient of a loss function.\n\nRegularization: Controls model complexity to prevent overfitting.\n\nFor regression, XGBoost predicts continuous values by minimizing a chosen loss function, commonly Mean Squared Error (MSE) or Mean Absolute Error (MAE).\n\nKey Concepts 1. Boosting Mechanism:\n- Unlike a single decision tree, XGBoost builds multiple trees in sequence.\n- Each new tree corrects the errors of the previous ones by focusing on residuals.\n\nLoss Functions:\n\nDetermines how errors are measured and minimized.\n\nCommon choices:\n\nMean Squared Error (MSE) ‚Äì Penalizes larger errors more heavily.\n\nMean Absolute Error (MAE) ‚Äì Treats all errors equally.\n\nHuber Loss ‚Äì A mix of MSE and MAE to handle outliers.\n\n\nRegularization Techniques:\n\nPrevents overfitting by adding penalties to complex models.\n\nL1 Regularization (Lasso) ‚Äì Shrinks coefficients, promoting sparsity.\n\nL2 Regularization (Ridge) ‚Äì Penalizes large coefficients to reduce variance.\n\nFeature Importance & Selection:\n\nXGBoost ranks features by importance, aiding feature selection.\n\nCan be used to eliminate redundant or irrelevant features.\n\n\n\nPros 1. High Performance ‚Äì Optimized for speed, scalability, and efficiency.\n2. Handles Missing Data ‚Äì Automatically learns how to deal with missing values.\n3. Regularization Built-in ‚Äì Reduces overfitting with L1 and L2 penalties.\n4. Works Well with Large Datasets ‚Äì Efficient memory usage and parallel processing.\n\nCons 1. Complexity ‚Äì More difficult to tune compared to simpler models.\n2. Computationally Intensive ‚Äì Training can be slow on very large datasets.\n3. Sensitive to Hyperparameters ‚Äì Performance depends on careful tuning of learning rate, tree depth, and regularization.\n\nTips * Optimize Hyperparameters ‚Äì Use grid search or Bayesian optimization for tuning.\n* Use Early Stopping ‚Äì Stops training if performance stops improving on validation data.\n* Scale Features if Needed ‚Äì Although XGBoost can handle unscaled data, standardization may help in some cases.\n* Leverage Feature Importance ‚Äì Identify and remove less relevant features to improve efficiency.\n\nUseful Articles and Videos\n* XGBoost Official Documentation\n* XGBoost for Regression ‚Äì Machine Learning Mastery\n* Understanding XGBoost ‚Äì Analytics Vidhya\n* XGBoost Explained ‚Äì YouTube\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#import-datalibraries",
    "href": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#import-datalibraries",
    "title": "K-Nearest Neighbors Machine Leaning Model üíª üß†",
    "section": "Import Data/Libraries",
    "text": "Import Data/Libraries\n\n!pip3 install lets-plot\n\nRequirement already satisfied: lets-plot in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (4.7.1)\nRequirement already satisfied: pypng in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (from lets-plot) (0.20220715.0)\nRequirement already satisfied: palettable in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (from lets-plot) (3.3.3)\nRequirement already satisfied: pillow in /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages (from lets-plot) (11.3.0)\n\n\n\n# needed libraries for KNN models\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.model_selection import cross_val_score,  train_test_split , KFold\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nimport lets_plot as lp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n\n# foundation dataset\nfrom sklearn.datasets import load_iris\n\n# stretch dataset\ncleveland_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data', header=None)\nhungarian_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data', header=None)\nswitzerland_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data', header=None)\n\n\n# Code to set up access to the foundational dataset\niris = load_iris()\niris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\niris_df['target'] = iris.target_names[iris.target]\n# iris_df is now the dataframe to be used for the foundational model\niris_df.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa",
    "crumbs": [
      "Machine Learning projects",
      "KNN Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#explore-visualize-and-understand-the-data",
    "href": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#explore-visualize-and-understand-the-data",
    "title": "K-Nearest Neighbors Machine Leaning Model üíª üß†",
    "section": "Explore, Visualize and Understand the Data",
    "text": "Explore, Visualize and Understand the Data\n\nsns.pairplot(iris_df, hue = \"target\", size=3, markers=[\"o\", \"s\", \"D\"])\nplt.figure()\nplt.show()\n\n/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/seaborn/axisgrid.py:2100: UserWarning:\n\nThe `size` parameter has been renamed to `height`; please update your code.\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Machine Learning projects",
      "KNN Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#feature-enginnering-and-data-augmentation",
    "href": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#feature-enginnering-and-data-augmentation",
    "title": "K-Nearest Neighbors Machine Leaning Model üíª üß†",
    "section": "Feature Enginnering and Data Augmentation",
    "text": "Feature Enginnering and Data Augmentation\n\nData Augmentation\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\nFeature Engineering\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns.",
    "crumbs": [
      "Machine Learning projects",
      "KNN Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#machine-learning-model",
    "href": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#machine-learning-model",
    "title": "K-Nearest Neighbors Machine Leaning Model üíª üß†",
    "section": "Machine Learning Model",
    "text": "Machine Learning Model\n\nSplit the data\n\nx= iris_df.drop(columns=['target'])\ny = iris_df['target']\n\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2) #Training 80% and testing 20%\n\n\ny.head()\n\n0    setosa\n1    setosa\n2    setosa\n3    setosa\n4    setosa\nName: target, dtype: object\n\n\n\n\nCreate the model\n\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(x, y)\n\nKNeighborsClassifier(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_neighbors¬†\n3\n\n\n\nweights¬†\n'uniform'\n\n\n\nalgorithm¬†\n'auto'\n\n\n\nleaf_size¬†\n30\n\n\n\np¬†\n2\n\n\n\nmetric¬†\n'minkowski'\n\n\n\nmetric_params¬†\nNone\n\n\n\nn_jobs¬†\nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nTrain the model\n\nneigh.fit(x, y)\n\nKNeighborsClassifier(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_neighbors¬†\n3\n\n\n\nweights¬†\n'uniform'\n\n\n\nalgorithm¬†\n'auto'\n\n\n\nleaf_size¬†\n30\n\n\n\np¬†\n2\n\n\n\nmetric¬†\n'minkowski'\n\n\n\nmetric_params¬†\nNone\n\n\n\nn_jobs¬†\nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nMake predictions\n\ny_pred = neigh.predict(X_test)",
    "crumbs": [
      "Machine Learning projects",
      "KNN Model"
    ]
  },
  {
    "objectID": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#evaluate-the-model",
    "href": "Machine_Learning_Projects/1_KNN_Model/ML_Bootcamp_KNN.html#evaluate-the-model",
    "title": "K-Nearest Neighbors Machine Leaning Model üíª üß†",
    "section": "Evaluate the Model",
    "text": "Evaluate the Model\n\naccuracy = accuracy_score(y_test, y_pred)*100\n\nprint('Accuracy: ' + str(round(accuracy, 2)) + ' %.')\n\nAccuracy: 93.33 %.",
    "crumbs": [
      "Machine Learning projects",
      "KNN Model"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "About me",
    "section": "",
    "text": "Hi! I‚Äôm Tam Tran, a Data Visualization Analyst and aspiring Data Scientist currently pursuing a B.S. in Data Science with a minor in Statistics and a Cloud Computing certificate at BYU-Idaho.\nI‚Äôm passionate about transforming data into meaningful insights and compelling visual stories that drive informed decisions. With hands-on experience in Python, SQL, R, Power BI, Tableau, and DAX, I enjoy working at the intersection of statistics, analytics, and communication.\nMy academic and professional journey has sharpened my ability to analyze complex data, think critically, and collaborate across teams. I‚Äôm also an active contributor to the data science community at BYU-Idaho, mentoring peers and leading project-based learning.\nI‚Äôm excited to keep growing in data science and contribute to innovative, data-driven teams solving real-world challenges.\n\nEducation\nBrigham Young University - Idaho | Expected Graduation: December 2027\nBachelor of Science in Data Science\nMinor in Statistics & Cloud Computing Certificate\n\n\nKey skills\n‚Ä¢ Data Analysis: Python (Pandas, NumPy, Matplotlib, LetsPlot, Seaborn, Scikit-learn), SQL, R\n‚Ä¢ Machine Learning: Data preprocessing (train-test split, feature engineering, encoding categorical data), Classification Models (Decision Trees, Random Forest, KNN, XGBoost), Regression Models\n‚Ä¢ Business Intelligence Tools: PowerBI, Tableau\n‚Ä¢ Microsoft Office Suite: Word, Excel, PowerPoint, Outlook\nDownload My Resume\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Data_Visualization_Projects/accuracy_report.html",
    "href": "Data_Visualization_Projects/accuracy_report.html",
    "title": "Data Visualization projects",
    "section": "",
    "text": "Accuracy Report\nNote: All sensitive data has been removed for privacy. This project was reviewed and approved for portfolio use.\n\n\n\nAccuracy Report\n\n\n\nWhat is it?\nDeveloped an interactive Power BI dashboard to track and evaluate transfer course accuracy across schools and departments. The dashboard includes filters for date range, I-Number (student lookup), and school/transfer type categories (e.g., duplicated transfer, waivers, invalid grade).\n\n\nImpact\nImproved transparency and accountability in transfer credit evaluation, ensured data accuracy for student records, and provided actionable insights to support policy compliance and academic decision-making.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tam Tran",
    "section": "",
    "text": "Resume\nView My Resume\n\n\nData Visualization projects\nView My Hold Report View My Active Program Report View My Accuracy Report\n\n\nMachine Learning projects\nView K-Nearest Neighbors Model\nView Decision Tree - Random Forest Model\nView Regression Model\nView Classification Model\n\n\n\n\n Back to top"
  }
]