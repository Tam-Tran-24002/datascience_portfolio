[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Tam Tran’s Resume",
    "section": "",
    "text": "minhtamtran.work@gmail.com | 208-671-3212 | https://www.linkedin.com/in/tam-tran-b37a9123b/ |\nhttps://github.com/Tam-Tran-24002\nTam Tran is an aspiring Data Scientist currently pursuing a Bachelor of Science in Data Science with a minor in Business Analytics and a Cloud Computing certificate. With a growing interest in data analytics, machine learning, and cloud computing, Tam is passionate about solving real-world problems through data. Eager to learn and gain hands-on experience, Tam has proficiency in Python, SQL, R, Excel, Tableau, and machine learning models. Focused on enhancing skills in data visualization, data analytics, predictive modeling, and database management, Tam is actively seeking opportunities to apply and further develop expertise in a dynamic business environment."
  },
  {
    "objectID": "resume.html#tutor-data-science-lab-tutoring-center",
    "href": "resume.html#tutor-data-science-lab-tutoring-center",
    "title": "Tam Tran’s Resume",
    "section": "Tutor | Data Science Lab & Tutoring Center",
    "text": "Tutor | Data Science Lab & Tutoring Center\nRexburg, Idaho | January 2025 - Present\nBrigham Young University - Idaho\n• Tutor 200+ students in Data Intuition and Insights (Tableau), Data Science Programming (Python, SQL, Machine Learning), and Programming with Functions\n• Break down complex data science concepts for both technical and non-technical students, using clear explanations and practical examples, leading to increased comprehension and engagement"
  },
  {
    "objectID": "resume.html#teaching-assistant",
    "href": "resume.html#teaching-assistant",
    "title": "Tam Tran’s Resume",
    "section": "Teaching Assistant",
    "text": "Teaching Assistant\nRexburg, Idaho | September 2024 - Present\nBrigham Young University - Idaho\n• Provide academic support to 250+ students in Introduction to Database, Intro to Programming, and Data Science Programming courses through tutoring, grading, and constructive feedback\n• Collaborate with professors to refine course materials, enhancing student engagement and improving learning outcomes\n• Facilitate lab sessions and office hours to reinforce key concepts and assist with coding assignments"
  },
  {
    "objectID": "resume.html#new-student-mentor",
    "href": "resume.html#new-student-mentor",
    "title": "Tam Tran’s Resume",
    "section": "New Student Mentor",
    "text": "New Student Mentor\nRexburg, Idaho | September 2024 - December 2024\nBrigham Young University - Idaho\n• Guided weekly gatherings for 40+ new students, providing support for academic and personal challenges\n• Boosted student engagement by 30% through 150+ one-on-one and group consultancy meetings\n• Managed attendance for 40+ students and coordinate 5+ resources to enhance learning outcomes and student success\n• Strengthened course organization and lesson planning to ensure students achieve academic success"
  },
  {
    "objectID": "resume.html#product-manager",
    "href": "resume.html#product-manager",
    "title": "Tam Tran’s Resume",
    "section": "Product Manager",
    "text": "Product Manager\nHo Chi Minh, Vietnam | February 2023 - February 2024\nAIESEC in Vietnam\n• Led a team of 8 members, delivering top-tier customer service for Global Volunteer projects and generating over $1550 in revenue\n• Analyzed 500+ lead and customer data points to generate actionable insights, improve customer experience, and optimize business strategy\n• Collaborated with cross-functional teams across 10+ countries to enhance the customer journey and drive seamless service delivery\n• Achieved an 87.5% retention rate of members through weekly performance data analysis and the implementation of data-driven strategies\n• Awarded Function Excellence with a 90% goal achievement rate"
  },
  {
    "objectID": "resume.html#marketing-team-leader-bac---international-business-club",
    "href": "resume.html#marketing-team-leader-bac---international-business-club",
    "title": "Tam Tran’s Resume",
    "section": "Marketing Team Leader | BAC - International Business Club",
    "text": "Marketing Team Leader | BAC - International Business Club\nHo Chi Minh, Vietnam | December 2021 - December 2022\nTon Duc Thang University\n• Directed and improved a 6-member team by analyzing performance metrics and implementing weekly workflow adjustments, resulting in a 20% increase in operational efficiency and streamlined processes\n• Trained 5 team members on advanced Excel techniques, resulting in a 30% reduction in operational processing time and significantly boosting overall team performance"
  },
  {
    "objectID": "resume.html#machine-learning-bootcamp",
    "href": "resume.html#machine-learning-bootcamp",
    "title": "Tam Tran’s Resume",
    "section": "Machine Learning – Bootcamp",
    "text": "Machine Learning – Bootcamp\nJanuary 2024 - Present\n• Deployed Random Forest, Decision Trees, K-Nearest Neighbors (KNN), and XGBoost on real-world datasets, achieving 90% accuracy\n• Optimized model performance through feature engineering, train-test split, and categorical encoding\n• Extracted actionable insights to support data-driven decisions and enhance model interpretability"
  },
  {
    "objectID": "Machine Learning projects/1_KNN_Model/readme.html",
    "href": "Machine Learning projects/1_KNN_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "This folder’s purpose is to give an example of using KNN machine learning model.\nFoundation datset: iris dataset Stretch dataset: the Heart diseases - UCI (https://archive.ics.uci.edu/dataset/45/heart+disease).\nK-Nearest Neighbors (KNN) is a supervised machine learning algorithm commonly used for classification and regression tasks. It’s one of the simplest and most intuitive models in machine learning.\n\nDefinition: KNN works by finding the ‘k’ closest data points (neighbors) to a given input based on some distance metric (e.g., Euclidean distance). The predicted value or class is determined by these neighbors:\n\nFor classification, the input is assigned the class most common among its neighbors (majority vote).\nFor regression, the predicted value is the average (or sometimes weighted average) of the neighbors’ values.\n\n\nKey Concepts:\n\nK (Number of Neighbors): The algorithm uses ‘k’ neighbors to make predictions. Choosing the right ‘k’ is crucial:\n\n\nSmall ‘k’ (e.g., 1 or 3) makes the model sensitive to noise.\nLarge ‘k’ smooths out predictions but may overlook local patterns.\n\n\nDistance Metrics: Determines how “close” neighbors are. Common metrics include:\n\n\nEuclidean Distance: Straight-line distance between points.\nManhattan Distance: Distance measured along axes at right angles.\nCosine Similarity: Measures the cosine of the angle between two vectors (useful for text or high-dimensional data).\n\n\nLaziness: KNN is a lazy learner, meaning it doesn’t learn a model during training. Instead, it stores the data and makes predictions when queried. This is why it’s called a “memory-based” approach.\n\nUseful Articles and Videos: * https://www.w3schools.com/python/python_ml_knn.asp * https://realpython.com/knn-python/ * https://www.geeksforgeeks.org/k-nearest-neighbor-algorithm-in-python/ * https://www.youtube.com/watch?v=CQveSaMyEwM * https://www.youtube.com/watch?v=b6uHw7QW_n4 * https://www.youtube.com/watch?v=w6bOBZX-1kY\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning projects/1_KNN_Model"
    ]
  },
  {
    "objectID": "Machine Learning projects/4_Classification_Model/readme.html",
    "href": "Machine Learning projects/4_Classification_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "XGBoost Classification is a supervised machine learning algorithm that builds an ensemble of decision trees to predict categorical outcomes. It is optimized for speed and performance using gradient boosting techniques.\n\nData For the foundation dataset we will be using the bank customer churn dataset. It is a commonly used dataset for predicting customer turnover in the banking industry. For more information go to the data folder’s readme file.\n\nDefinition\nXGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:\n\nBoosting Trees: A collection of decision trees built sequentially to reduce errors.\n\nGradient Descent Optimization: Adjusts model weights using the gradient of a loss function.\n\nRegularization: Controls model complexity to prevent overfitting.\n\nFor classification, XGBoost predicts categorical outcomes by minimizing a chosen loss function, such as logistic loss for binary classification or softmax (cross-entropy) loss for multi-class classification.\n\nKey Concepts\n1. Boosting Mechanism:\n- Unlike a single decision tree, XGBoost builds multiple trees in sequence.\n- Each new tree corrects the errors of the previous ones by focusing on misclassified examples.\n\nLoss Functions:\n\nDetermines how errors are measured and minimized.\n\nCommon choices for classification include:\n\nLogistic Loss – Used for binary classification tasks.\n\nSoftmax (Cross-Entropy Loss) – Used for multi-class classification tasks.\n\n\nRegularization Techniques:\n\nPrevents overfitting by adding penalties to complex models.\n\nL1 Regularization (Lasso) – Encourages sparsity by shrinking coefficients.\n\nL2 Regularization (Ridge) – Penalizes large coefficients to reduce variance.\n\nFeature Importance & Selection:\n\nXGBoost ranks features by importance, aiding in feature selection.\n\nHelps in eliminating redundant or irrelevant features for better performance.\n\n\n\nPros\n1. High Performance – Optimized for speed, scalability, and efficiency.\n2. Handles Missing Data – Automatically learns how to deal with missing values.\n3. Regularization Built-in – Reduces overfitting with L1 and L2 penalties.\n4. Probabilistic Predictions – Provides probability scores for classification, enabling threshold tuning.\n5. Works Well with Large Datasets – Efficient memory usage and parallel processing.\n\nCons\n1. Complexity – More difficult to tune compared to simpler models.\n2. Computationally Intensive – Training can be slow on very large datasets.\n3. Sensitive to Hyperparameters – Performance depends on careful tuning of learning rate, tree depth, and regularization.\n4. Less Interpretable – Decision boundaries may be challenging to interpret compared to simpler models.\n\nTips\n* Optimize Hyperparameters – Use grid search or Bayesian optimization for tuning.\n* Use Early Stopping – Stop training if performance ceases to improve on validation data.\n* Scale Features if Needed – Although XGBoost can handle unscaled data, standardization might improve performance.\n* Leverage Feature Importance – Identify and remove less relevant features to improve efficiency.\n* Adjust Decision Thresholds – Fine-tune the probability threshold to balance precision and recall for your specific task.\n\nUseful Articles and Videos\n* XGBoost Official Documentation\n* XGBoost for Classification – Machine Learning Mastery\n* Understanding XGBoost – Analytics Vidhya\n* XGBoost Explained for Classification – YouTube\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning projects/4_Classification_Model"
    ]
  },
  {
    "objectID": "Machine Learning projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#overview",
    "href": "Machine Learning projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#overview",
    "title": "Decision Tree Machine Learning Model 💻 🧠",
    "section": "Overview",
    "text": "Overview\nDecision Tree (DT) is a supervised machine learning algorithm used for both classification and regression tasks. It splits data into branches based on feature values to make predictions.\n\n\n\nDefinition\nA Decision Tree is a flowchart-like model where data is split into smaller subsets based on feature conditions. The structure consists of:\n\nRoot Node: The starting point that represents the entire dataset.\n\nDecision Nodes: Points where the data is split based on a feature.\n\nLeaves: Terminal nodes that provide the final prediction.\n\nFor classification, a Decision Tree assigns class labels based on feature splits.\nFor regression, it predicts continuous values using the average or mean of data points in each leaf.\n\n\n\nKey Concepts 1. Splitting Criteria:\n- Determines how the dataset is divided at each step.\n- Common methods: * Gini Impurity (Classification) – Measures the likelihood of incorrect classification.\n* Entropy (Classification) – Uses information gain to decide splits.\n* Mean Squared Error (MSE) (Regression) – Measures variance within nodes.\n\nTree Depth & Overfitting:\n\nDeeper trees fit training data better but may overfit.\n\nPruning (removing unnecessary branches) improves generalization.\n\nFeature Importance:\n\nDecision Trees rank features by their impact on predictions.\n\nHelps in feature selection for other models.\n\nHandling Missing Data:\n\nSome implementations allow surrogate splits to handle missing values.\n\n\n\n\n\nPros 1. Easy to Understand & Interpret – Can be visualized as a simple flowchart.\n2. No Need for Feature Scaling – Works with both categorical and numerical features.\n3. Handles Non-Linearity – Can model complex relationships without requiring transformation.\n4. Fast for Small Datasets – Training and inference are relatively quick.\n\n\n\nCons 1. Prone to Overfitting – Deep trees can memorize training data, reducing generalization.\n2. Unstable to Small Changes – Small variations in data can change the tree structure significantly.\n3. Less Efficient on Large Datasets – Computationally expensive for large datasets.\n\n\n\nTips * Limit Tree Depth – Use max_depth to prevent overfitting.\n* Pruning Techniques – Use pre-pruning (max_leaf_nodes) or post-pruning to remove unnecessary branches.\n* Use Feature Importance – Identify the most influential features and remove irrelevant ones.\n* Consider Ensemble Methods – Random Forest or Gradient Boosting improve Decision Trees by reducing variance.\n\n\n\nUseful Articles and Videos * https://www.datacamp.com/tutorial/decision-tree-classification-python * https://www.ibm.com/think/topics/decision-trees * https://www.youtube.com/watch?v=6DlWndLbk90 * https://www.youtube.com/watch?v=ZOiBe-nrmc4"
  },
  {
    "objectID": "Machine Learning projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#import-datalibraries",
    "href": "Machine Learning projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#import-datalibraries",
    "title": "Decision Tree Machine Learning Model 💻 🧠",
    "section": "Import Data/Libraries",
    "text": "Import Data/Libraries\n\n!pip install lets_plot\n\nCollecting lets_plot\n  Downloading lets_plot-4.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting pypng (from lets_plot)\n  Downloading pypng-0.20220715.0-py3-none-any.whl.metadata (13 kB)\nCollecting palettable (from lets_plot)\n  Downloading palettable-3.3.3-py2.py3-none-any.whl.metadata (3.3 kB)\nDownloading lets_plot-4.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 19.2 MB/s eta 0:00:00\nDownloading palettable-3.3.3-py2.py3-none-any.whl (332 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 332.3/332.3 kB 7.5 MB/s eta 0:00:00\nDownloading pypng-0.20220715.0-py3-none-any.whl (58 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 kB 3.3 MB/s eta 0:00:00\nInstalling collected packages: pypng, palettable, lets_plot\nSuccessfully installed lets_plot-4.6.2 palettable-3.3.3 pypng-0.20220715.0\n\n\n\n# needed libraries for Decision Tree models\nimport pandas as pd\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.model_selection import cross_val_score,  train_test_split , KFold\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nimport lets_plot as lp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# foundation dataset\ntitanic_df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n\n# stretch dataset\ncleveland_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data', header=None)\nhungarian_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data', header=None)\nswitzerland_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data', header=None)"
  },
  {
    "objectID": "Machine Learning projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#explore-visualize-and-understand-the-data",
    "href": "Machine Learning projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#explore-visualize-and-understand-the-data",
    "title": "Decision Tree Machine Learning Model 💻 🧠",
    "section": "Explore, Visualize and Understand the Data",
    "text": "Explore, Visualize and Understand the Data\n\nsns.countplot(data=titanic_df, x=\"Pclass\", hue=\"Survived\", palette=\"Set2\")\nplt.title(\"Survival Count by Passenger Class\")\nplt.xlabel(\"Passenger Class\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Survived\", labels=[\"No\", \"Yes\"])\nplt.show()\n\n\n\n\n\n\n\n\n\ntitanic_df.head()\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nimport numpy as np\nfrom lets_plot import *\nLetsPlot.setup_html()\nggplot(mapping=aes(x='Pclass', fill='Survived'), data=titanic_df) + geom_bar()"
  },
  {
    "objectID": "Machine Learning projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#feature-enginnering-and-data-augmentation",
    "href": "Machine Learning projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#feature-enginnering-and-data-augmentation",
    "title": "Decision Tree Machine Learning Model 💻 🧠",
    "section": "Feature Enginnering and Data Augmentation",
    "text": "Feature Enginnering and Data Augmentation\n\nData Augmentation\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\nFeature Engineering\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns.\n\nconditions = [\n    titanic_df[\"Name\"].str.contains(r'(?i)(Mr)|(Mrs)|(Ms)|(Miss)|(Mlle)', na=False),  # Condition 1: If \"Name\" contains \"Mr\"\n    titanic_df[\"Name\"].str.contains(r'(?i)(Rev)|(Dr)|(General)|(Col)|(Major)|(Capt)', na=False), # Condition 2: If \"Name\" contains \"Mrs\"\n    titanic_df[\"Name\"].str.contains(r'(?i)(Master)|(Countess)|(Jonkheer)|(Don)', na=False) # Condition 3: If \"Name\" contains \"Miss\"\n]\n\noutputs = [\n    1,\n    2,\n    3\n]\n\ntitanic_df[\"Name_group\"] = np.select(conditions, outputs, default=5)\n\nUserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  titanic_df[\"Name\"].str.contains(r'(?i)(Mr)|(Mrs)|(Ms)|(Miss)|(Mlle)', na=False),  # Condition 1: If \"Name\" contains \"Mr\"\n&lt;ipython-input-6-1b68d7e0032b&gt;:3: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  titanic_df[\"Name\"].str.contains(r'(?i)(Rev)|(Dr)|(General)|(Col)|(Major)|(Capt)', na=False), # Condition 2: If \"Name\" contains \"Mrs\"\n&lt;ipython-input-6-1b68d7e0032b&gt;:4: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  titanic_df[\"Name\"].str.contains(r'(?i)(Master)|(Countess)|(Jonkheer)|(Don)', na=False) # Condition 3: If \"Name\" contains \"Miss\""
  },
  {
    "objectID": "Machine Learning projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#machine-learning-model-decision-tree",
    "href": "Machine Learning projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#machine-learning-model-decision-tree",
    "title": "Decision Tree Machine Learning Model 💻 🧠",
    "section": "Machine Learning Model: Decision Tree",
    "text": "Machine Learning Model: Decision Tree\n\nSplit the data\n\n# Fitting the model\ntitanic_df2 = pd.get_dummies(titanic_df, columns=['Sex', 'Embarked', 'Name', 'Fare', 'Ticket', 'Cabin'], drop_first=True)\n\n\nX = titanic_df2.drop('Survived', axis=1)\ny = titanic_df2['Survived']\n\n\n\nCreate the model\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\ny.head()\n\n\n\n\n\n\n\n\nSurvived\n\n\n\n\n0\n0\n\n\n1\n1\n\n\n2\n1\n\n\n3\n1\n\n\n4\n0\n\n\n\n\ndtype: int64\n\n\n\n\nTrain the model\n\n\nMake predictions\n\ntitanic_df2.head()\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nName_group\nSex_male\nEmbarked_Q\nEmbarked_S\n...\nCabin_E8\nCabin_F E69\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n1\nTrue\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n2\n1\n1\n38.0\n1\n0\n1\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n3\n1\n3\n26.0\n0\n0\n1\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n4\n1\n1\n35.0\n1\n0\n1\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n5\n0\n3\n35.0\n0\n0\n1\nTrue\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 1973 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Predicting the Test set results\nDT = DecisionTreeClassifier()\nDT.fit(X_train, y_train)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier() \n\n\n\ny_pred = DT.predict(X_test)\n\n\n\nEvaluate the Model\nAccuracy – The percentage of total predictions that are correct.\nExample: If a spam filter correctly classifies 90 out of 100 emails (whether spam or not), the accuracy is 90%.\nF1 Score – Out of all the positive predictions, how many were actually correct.\nExample: If a spam filter predicts 20 emails as spam, but only 15 are actually spam, precision is 15/20 = 75%.\nRecall Score – Out of all the actual positive cases, how many did the model correctly identify.\nExample: If there were 25 spam emails in total, and the model correctly identified 15 of them, recall is 15/25 = 60%.\nPrecision Score – A balance between precision and recall (harmonic mean).\nExample: If precision is 75% and recall is 60%, F1 score is (2 × 75 × 60) / (75 + 60) = 66.7%.\nAccuracy: 82.68 %. F1 Score: 0.82 %. Recall Score: 0.83 %. Precision Score: 0.83 %.\n\n# Accuracy, F1 Score, Recall score, Precision score\naccuracy = accuracy_score(y_test, y_pred)*100\nf1_score = f1_score(y_test, y_pred, average='weighted')\nrecall_score = recall_score(y_test, y_pred, average='weighted')\nprecision_score = precision_score(y_test, y_pred, average='weighted')\n\nprint('Accuracy: ' + str(round(accuracy, 2)) + ' %.')\nprint('F1 Score: ' + str(round(f1_score, 2)) + ' %.')\nprint('Recall Score: ' + str(round(recall_score, 2)) + ' %.')\nprint('Precision Score: ' + str(round(precision_score, 2)) + ' %.')\n\nAccuracy: 82.12 %.\nF1 Score: 0.82 %.\nRecall Score: 0.82 %.\nPrecision Score: 0.82 %."
  },
  {
    "objectID": "Machine Learning projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#overview-of-random-forests",
    "href": "Machine Learning projects/2_DecisionTree_RandomForests_Model/ML_Bootcamp_DecisionTree_RandomForest.html#overview-of-random-forests",
    "title": "Decision Tree Machine Learning Model 💻 🧠",
    "section": "Overview of Random Forests",
    "text": "Overview of Random Forests\nRandom Forest (RF) is a supervised machine learning algorithm used for both classification and regression tasks. It builds multiple decision trees and combines their outputs for better accuracy and stability.\n\nDefinition\nA Random Forest is an ensemble learning method that creates a “forest” of decision trees during training. Instead of relying on a single tree, Random Forest combines the predictions of multiple trees to improve performance. The structure consists of:\n\nDecision Trees: Individual trees that make predictions based on subsets of data and features.\nBagging (Bootstrap Aggregating): Each tree is trained on a randomly sampled subset of the data.\nVoting/Averaging: Combines the predictions from all trees:\nFor classification, the majority vote decides the class.\nFor regression, the average of all tree predictions is used.\nRandom Forest reduces the risk of overfitting compared to individual decision trees and works well for both categorical and numerical data.\n\n\nKey Concepts\n\nBagging:\n\n\nRandom Forest uses bootstrapping to train each tree on a different sample of the data.\nThis creates diversity among trees, making the model more robust.\n\n\nFeature Randomness:\n\n\nAt each split, Random Forest considers a random subset of features rather than all features.\nThis reduces correlation between trees and improves generalization.\n\n\nOut-of-Bag (OOB) Error:\n\n\nTrees not trained on certain data points (left out during bootstrapping) can be used to validate the model.\nOOB error gives an unbiased estimate of model performance.\n\n\nFeature Importance:\n\n\nRandom Forest provides a ranking of feature importance based on how often features are used for splitting across trees.\nUseful for identifying key predictors in your data. \n\nPros\nImproved Accuracy – Combines multiple trees, reducing overfitting. Robust to Noise – Handles outliers and noisy data better than individual trees. Handles Large Datasets – Can scale well with more data. Feature Selection – Provides insights into the importance of features. No Need for Feature Scaling – Works with unscaled data, both numerical and categorical. \nCons\nLess Interpretable – Harder to visualize compared to a single decision tree. Computationally Intensive – Training many trees can take time and memory for large datasets. Overfitting Risk with Too Many Trees – Although rare, excessive trees might still overfit without tuning. Slower Inference – Predictions may take longer because they aggregate results from multiple trees. \nTips\n\nTune n_estimators – Adjust the number of trees to balance accuracy and computational cost.\nLimit Tree Depth – Use max_depth to avoid overfitting while maintaining performance.\nOptimize Feature Subset Size – Use max_features to control how many features each tree considers at a split.\nUse Feature Importance – Rank and prioritize the most important features in your dataset.\nCombine with Other Methods – Random Forest pairs well with techniques like PCA for dimensionality reduction. \n\nUseful Articles and Videos\nhttps://www.datacamp.com/tutorial/random-forests-classifier-python\nhttps://www.ibm.com/topics/random-forest\nhttps://www.youtube.com/watch?v=J4Wdy0Wc_xQ\nhttps://www.youtube.com/watch?v=QHOazyP-YlM\n\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n\nCreate Model\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nTrain Model\n\ny.head()\n\n\n\n\n\n\n\n\nSurvived\n\n\n\n\n0\n0\n\n\n1\n1\n\n\n2\n1\n\n\n3\n1\n\n\n4\n0\n\n\n\n\ndtype: int64\n\n\n\n\nMake Predictions\n\n# Predicting the Test set results\nRF = RandomForestClassifier()\nRF.fit(X_train, y_train)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier() \n\n\n\n\nHyperparameter Search\n\ny_pred = RF.predict(X_test)\n\n\n\nEvaluate the Model\n\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n\n# Assuming y_test and y_pred are defined somewhere in your code\naccuracy = accuracy_score(y_test, y_pred) * 100\nf1_value = f1_score(y_test, y_pred, average='weighted') * 100\nrecall_value = recall_score(y_test, y_pred, average='weighted') * 100\nprecision_value = precision_score(y_test, y_pred, average='weighted') * 100\n\nprint('Accuracy: ' + str(round(accuracy, 2)) + ' %.')\nprint('F1 Score: ' + str(round(f1_value, 2)) + ' %.')\nprint('Recall Score: ' + str(round(recall_value, 2)) + ' %.')\nprint('Precision Score: ' + str(round(precision_value, 2)) + ' %.')\n\nAccuracy: 81.56 %.\nF1 Score: 80.97 %.\nRecall Score: 81.56 %.\nPrecision Score: 82.44 %."
  },
  {
    "objectID": "Machine Learning projects/3_Regression_Model/readme.html",
    "href": "Machine Learning projects/3_Regression_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "There are two datasets in the Data folder that will be used to learn regression. 1. New York City bicycle routes through bridges 2. Idaho Falls Chukars (Pioneer League Baseball) data to find the optimal amount of pitches to throw out a batter based on the teamID (player is a trench goal).\nOverview: XGBoost Regression is a supervised machine learning algorithm that builds an ensemble of decision trees to predict continuous values. It is optimized for speed and performance using gradient boosting techniques.\n\nDefinition XGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:\n\nBoosting Trees: A collection of decision trees built sequentially to reduce errors.\n\nGradient Descent Optimization: Adjusts model weights using the gradient of a loss function.\n\nRegularization: Controls model complexity to prevent overfitting.\n\nFor regression, XGBoost predicts continuous values by minimizing a chosen loss function, commonly Mean Squared Error (MSE) or Mean Absolute Error (MAE).\n\nKey Concepts 1. Boosting Mechanism:\n- Unlike a single decision tree, XGBoost builds multiple trees in sequence.\n- Each new tree corrects the errors of the previous ones by focusing on residuals.\n\nLoss Functions:\n\nDetermines how errors are measured and minimized.\n\nCommon choices:\n\nMean Squared Error (MSE) – Penalizes larger errors more heavily.\n\nMean Absolute Error (MAE) – Treats all errors equally.\n\nHuber Loss – A mix of MSE and MAE to handle outliers.\n\n\nRegularization Techniques:\n\nPrevents overfitting by adding penalties to complex models.\n\nL1 Regularization (Lasso) – Shrinks coefficients, promoting sparsity.\n\nL2 Regularization (Ridge) – Penalizes large coefficients to reduce variance.\n\nFeature Importance & Selection:\n\nXGBoost ranks features by importance, aiding feature selection.\n\nCan be used to eliminate redundant or irrelevant features.\n\n\n\nPros 1. High Performance – Optimized for speed, scalability, and efficiency.\n2. Handles Missing Data – Automatically learns how to deal with missing values.\n3. Regularization Built-in – Reduces overfitting with L1 and L2 penalties.\n4. Works Well with Large Datasets – Efficient memory usage and parallel processing.\n\nCons 1. Complexity – More difficult to tune compared to simpler models.\n2. Computationally Intensive – Training can be slow on very large datasets.\n3. Sensitive to Hyperparameters – Performance depends on careful tuning of learning rate, tree depth, and regularization.\n\nTips * Optimize Hyperparameters – Use grid search or Bayesian optimization for tuning.\n* Use Early Stopping – Stops training if performance stops improving on validation data.\n* Scale Features if Needed – Although XGBoost can handle unscaled data, standardization may help in some cases.\n* Leverage Feature Importance – Identify and remove less relevant features to improve efficiency.\n\nUseful Articles and Videos\n* XGBoost Official Documentation\n* XGBoost for Regression – Machine Learning Mastery\n* Understanding XGBoost – Analytics Vidhya\n* XGBoost Explained – YouTube\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning projects/3_Regression_Model"
    ]
  },
  {
    "objectID": "machine_learning.html",
    "href": "machine_learning.html",
    "title": "Machine Learning - Bootcamp",
    "section": "",
    "text": "/machine_learning_projects /1_KNN_Model ML_Bootcamp_KNN.ipynb /2_DecisionTree_RandomForests_Model ML_Bootcamp_DecisionTree_RandomForest.ipynb /3_Regression_Model ML_Bootcamp_Regression.ipynb /4_Classification_Model ML_Bootcamp_Classification_XGBoost.ipynb",
    "crumbs": [
      "Machine Learning projects"
    ]
  },
  {
    "objectID": "machine_learning.html#my-projects",
    "href": "machine_learning.html#my-projects",
    "title": "Machine Learning - Bootcamp",
    "section": "",
    "text": "/machine_learning_projects /1_KNN_Model ML_Bootcamp_KNN.ipynb /2_DecisionTree_RandomForests_Model ML_Bootcamp_DecisionTree_RandomForest.ipynb /3_Regression_Model ML_Bootcamp_Regression.ipynb /4_Classification_Model ML_Bootcamp_Classification_XGBoost.ipynb",
    "crumbs": [
      "Machine Learning projects"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#machine-learning-projects",
    "href": "index.html#machine-learning-projects",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Machine Learning projects/3_Regression_Model/ML_Bootcamp_Regression.html#overview",
    "href": "Machine Learning projects/3_Regression_Model/ML_Bootcamp_Regression.html#overview",
    "title": "Regression Machine Learning Model 🧠 📈",
    "section": "Overview",
    "text": "Overview\nXGBoost Regression is a supervised machine learning algorithm that builds an ensemble of decision trees to predict continuous values. It is optimized for speed and performance using gradient boosting techniques.\n\n\n\nDefinition\nXGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:\n\nBoosting Trees: A collection of decision trees built sequentially to reduce errors.\n\nGradient Descent Optimization: Adjusts model weights using the gradient of a loss function.\n\nRegularization: Controls model complexity to prevent overfitting.\n\nFor regression, XGBoost predicts continuous values by minimizing a chosen loss function, commonly Mean Squared Error (MSE) or Mean Absolute Error (MAE).\n\n\n\nKey Concepts\n1. Boosting Mechanism:\n- Unlike a single decision tree, XGBoost builds multiple trees in sequence.\n- Each new tree corrects the errors of the previous ones by focusing on residuals.\n\nLoss Functions:\n\nDetermines how errors are measured and minimized.\n\nCommon choices:\n\nMean Squared Error (MSE) – Penalizes larger errors more heavily.\n\nMean Absolute Error (MAE) – Treats all errors equally.\n\nHuber Loss – A mix of MSE and MAE to handle outliers.\n\n\nRegularization Techniques:\n\nPrevents overfitting by adding penalties to complex models.\n\nL1 Regularization (Lasso) – Shrinks coefficients, promoting sparsity.\n\nL2 Regularization (Ridge) – Penalizes large coefficients to reduce variance.\n\nFeature Importance & Selection:\n\nXGBoost ranks features by importance, aiding feature selection.\n\nCan be used to eliminate redundant or irrelevant features.\n\n\n\n\n\nPros\n1. High Performance – Optimized for speed, scalability, and efficiency.\n2. Handles Missing Data – Automatically learns how to deal with missing values.\n3. Regularization Built-in – Reduces overfitting with L1 and L2 penalties.\n4. Works Well with Large Datasets – Efficient memory usage and parallel processing.\n\n\n\nCons\n1. Complexity – More difficult to tune compared to simpler models.\n2. Computationally Intensive – Training can be slow on very large datasets.\n3. Sensitive to Hyperparameters – Performance depends on careful tuning of learning rate, tree depth, and regularization.\n\n\n\nTips\n* Optimize Hyperparameters – Use grid search or Bayesian optimization for tuning.\n* Use Early Stopping – Stops training if performance stops improving on validation data.\n* Scale Features if Needed – Although XGBoost can handle unscaled data, standardization may help in some cases.\n* Leverage Feature Importance – Identify and remove less relevant features to improve efficiency.\n\n\n\nUseful Articles and Videos\n* XGBoost Official Documentation\n* XGBoost for Regression – Machine Learning Mastery\n* Understanding XGBoost – Analytics Vidhya\n* XGBoost Explained – YouTube"
  },
  {
    "objectID": "Machine Learning projects/3_Regression_Model/ML_Bootcamp_Regression.html#import-datalibraries",
    "href": "Machine Learning projects/3_Regression_Model/ML_Bootcamp_Regression.html#import-datalibraries",
    "title": "Regression Machine Learning Model 🧠 📈",
    "section": "Import Data/Libraries",
    "text": "Import Data/Libraries\n\n!pip install lets_plot\n\nRequirement already satisfied: lets_plot in /usr/local/lib/python3.11/dist-packages (4.6.2)\nRequirement already satisfied: pypng in /usr/local/lib/python3.11/dist-packages (from lets_plot) (0.20220715.0)\nRequirement already satisfied: palettable in /usr/local/lib/python3.11/dist-packages (from lets_plot) (3.3.3)\n\n\n\n# needed libraries for Regression models\nimport pandas as pd\nfrom sklearn import tree\nimport xgboost as xgb\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, mean_squared_error\nfrom sklearn.model_selection import cross_val_score,  train_test_split , KFold\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nimport lets_plot as lp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport kagglehub\n\n# foundation dataset\ndata_raw_url = \"https://raw.githubusercontent.com/BYUIDSS/DSS-ML-Bootcamp/refs/heads/main/3a_Regression_Model/Data/nyc-east-river-bicycle-counts.csv?token=GHSAT0AAAAAAC7UQH3WXY3OI4SLVS6O3K6SZ7KFJNQ\"\nbicycle_df = pd.read_csv(data_raw_url)"
  },
  {
    "objectID": "Machine Learning projects/3_Regression_Model/ML_Bootcamp_Regression.html#explore-visualize-and-understand-the-data",
    "href": "Machine Learning projects/3_Regression_Model/ML_Bootcamp_Regression.html#explore-visualize-and-understand-the-data",
    "title": "Regression Machine Learning Model 🧠 📈",
    "section": "Explore, Visualize and Understand the Data",
    "text": "Explore, Visualize and Understand the Data\n\nbicycle_df.head(10)\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nDate\nDay\nHigh Temp (°F)\nLow Temp (°F)\nPrecipitation\nBrooklyn Bridge\nManhattan Bridge\nWilliamsburg Bridge\nQueensboro Bridge\nTotal\n\n\n\n\n0\n0\n2016-04-01 00:00:00\n2016-04-01 00:00:00\n78.1\n66.0\n0.01\n1704.0\n3126\n4115.0\n2552.0\n11497\n\n\n1\n1\n2016-04-02 00:00:00\n2016-04-02 00:00:00\n55.0\n48.9\n0.15\n827.0\n1646\n2565.0\n1884.0\n6922\n\n\n2\n2\n2016-04-03 00:00:00\n2016-04-03 00:00:00\n39.9\n34.0\n0.09\n526.0\n1232\n1695.0\n1306.0\n4759\n\n\n3\n3\n2016-04-04 00:00:00\n2016-04-04 00:00:00\n44.1\n33.1\n0.47 (S)\n521.0\n1067\n1440.0\n1307.0\n4335\n\n\n4\n4\n2016-04-05 00:00:00\n2016-04-05 00:00:00\n42.1\n26.1\n0\n1416.0\n2617\n3081.0\n2357.0\n9471\n\n\n5\n5\n2016-04-06 00:00:00\n2016-04-06 00:00:00\n45.0\n30.0\n0\n1885.0\n3329\n3856.0\n2849.0\n11919\n\n\n6\n6\n2016-04-07 00:00:00\n2016-04-07 00:00:00\n57.0\n53.1\n0.09\n1276.0\n2581\n3282.0\n2457.0\n9596\n\n\n7\n7\n2016-04-08 00:00:00\n2016-04-08 00:00:00\n46.9\n44.1\n0.01\n1982.0\n3455\n4113.0\n3194.0\n12744\n\n\n8\n8\n2016-04-09 00:00:00\n2016-04-09 00:00:00\n43.0\n37.9\n0.09\n504.0\n997\n1507.0\n1502.0\n4510\n\n\n9\n9\n2016-04-10 00:00:00\n2016-04-10 00:00:00\n48.9\n30.9\n0\n1447.0\n2387\n3132.0\n2160.0\n9126\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nbicycle_df.describe()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nHigh Temp (°F)\nLow Temp (°F)\nBrooklyn Bridge\nManhattan Bridge\nWilliamsburg Bridge\nQueensboro Bridge\nTotal\n\n\n\n\ncount\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n\n\nmean\n104.500000\n60.580000\n46.413333\n2269.633333\n4049.533333\n4862.466667\n3352.866667\n14534.500000\n\n\nstd\n60.765944\n11.183223\n9.522796\n981.237786\n1704.731356\n1814.039499\n1099.254419\n5569.173496\n\n\nmin\n0.000000\n39.900000\n26.100000\n504.000000\n997.000000\n1440.000000\n1306.000000\n4335.000000\n\n\n25%\n52.250000\n55.000000\n44.100000\n1447.000000\n2617.000000\n3282.000000\n2457.000000\n9596.000000\n\n\n50%\n104.500000\n62.100000\n46.900000\n2379.500000\n4165.000000\n5194.000000\n3477.000000\n15292.500000\n\n\n75%\n156.750000\n68.000000\n50.000000\n3147.000000\n5309.000000\n6030.000000\n4192.000000\n18315.000000\n\n\nmax\n209.000000\n81.000000\n66.000000\n3871.000000\n6951.000000\n7834.000000\n5032.000000\n23318.000000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nbicycle_df.dtypes\n\n\n\n\n\n\n\n\n0\n\n\n\n\nUnnamed: 0\nint64\n\n\nDate\nobject\n\n\nDay\nobject\n\n\nHigh Temp (°F)\nfloat64\n\n\nLow Temp (°F)\nfloat64\n\n\nPrecipitation\nobject\n\n\nBrooklyn Bridge\nfloat64\n\n\nManhattan Bridge\nint64\n\n\nWilliamsburg Bridge\nfloat64\n\n\nQueensboro Bridge\nfloat64\n\n\nTotal\nint64\n\n\n\n\ndtype: object\n\n\n\nsns.barplot(data=bicycle_df, x=\"Brooklyn Bridge\", y =\"Total\", color=\"blue\")\nplt.title(\"Plot\")\nplt.legend(loc='upper right')\nplt.show()\n\nUserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  plt.legend(loc='upper right')\n\n\n\n\n\n\n\n\n\n\nsns.barplot(data=bicycle_df, x=\"Manhattan Bridge\", y =\"Total\", hue=\"Total\", palette=\"Set2\")\nplt.title(\"Plot\")\nplt.show()"
  },
  {
    "objectID": "Machine Learning projects/3_Regression_Model/ML_Bootcamp_Regression.html#feature-enginnering-and-data-augmentation",
    "href": "Machine Learning projects/3_Regression_Model/ML_Bootcamp_Regression.html#feature-enginnering-and-data-augmentation",
    "title": "Regression Machine Learning Model 🧠 📈",
    "section": "Feature Enginnering and Data Augmentation",
    "text": "Feature Enginnering and Data Augmentation\n\nData Augmentation\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\nFeature Engineering\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns.\n\nbicycle_df1 = pd.get_dummies(bicycle_df, columns=[\"Date\", \"Day\",\"Precipitation\"])\n\n\nX = bicycle_df1.drop(columns=[\"Brooklyn Bridge\",    \"Manhattan Bridge\", \"Williamsburg Bridge\",  \"Queensboro Bridge\", \"Total\"])\ny = bicycle_df1[\"Total\"]\n\n\nbicycle_df1.describe()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nHigh Temp (°F)\nLow Temp (°F)\nBrooklyn Bridge\nManhattan Bridge\nWilliamsburg Bridge\nQueensboro Bridge\nTotal\n\n\n\n\ncount\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n\n\nmean\n104.500000\n60.580000\n46.413333\n2269.633333\n4049.533333\n4862.466667\n3352.866667\n14534.500000\n\n\nstd\n60.765944\n11.183223\n9.522796\n981.237786\n1704.731356\n1814.039499\n1099.254419\n5569.173496\n\n\nmin\n0.000000\n39.900000\n26.100000\n504.000000\n997.000000\n1440.000000\n1306.000000\n4335.000000\n\n\n25%\n52.250000\n55.000000\n44.100000\n1447.000000\n2617.000000\n3282.000000\n2457.000000\n9596.000000\n\n\n50%\n104.500000\n62.100000\n46.900000\n2379.500000\n4165.000000\n5194.000000\n3477.000000\n15292.500000\n\n\n75%\n156.750000\n68.000000\n50.000000\n3147.000000\n5309.000000\n6030.000000\n4192.000000\n18315.000000\n\n\nmax\n209.000000\n81.000000\n66.000000\n3871.000000\n6951.000000\n7834.000000\n5032.000000\n23318.000000"
  },
  {
    "objectID": "Machine Learning projects/3_Regression_Model/ML_Bootcamp_Regression.html#machine-learning-model",
    "href": "Machine Learning projects/3_Regression_Model/ML_Bootcamp_Regression.html#machine-learning-model",
    "title": "Regression Machine Learning Model 🧠 📈",
    "section": "Machine Learning Model",
    "text": "Machine Learning Model\n\nSplit the data to train and test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n\n\nCreate the model\n\nparams = {\n    'objective': 'reg:squarederror',  # Regression task\n    'max_depth': 6,                   # Maximum depth of trees\n    'learning_rate': 0.1,             # Learning rate\n    'n_estimators': 100,              # Number of boosting rounds\n}\n\n\n\nTrain the model\n\nmodel = xgb.train(params, dtrain, num_boost_round=100)\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:53:09] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n\n\nMake predictions\n\n# Predicting the Test set results\ny_pred = model.predict(dtest)\n\n\nHyperparameter Search\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid to search\nparam_grid = {\n    'max_depth': [3, 4, 5, 6, 7],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [50, 100, 200],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n# Create the XGBoost model\nxgb_model = xgb.XGBRegressor(objective='reg:squarederror', seed=42)\n\n# Create and Configure GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=xgb_model,\n    param_grid=param_grid,\n    scoring='neg_mean_squared_error',  # Since MSE should be minimized, we use its negative (because GridSearchCV maximizes the score)\n    cv=5,  # Uses 5-fold cross-validation to evaluate each parameter set\n    verbose=1, # Prints progress updates\n    n_jobs=-1 # Uses all available CPU cores for parallel computation, making it faster\n)\n# Fit the Grid Search to Training Data\ngrid_search.fit(X_train, y_train)\n\n# Get the Best Model & Hyperparameters\nprint(\"Best hyperparameters:\", grid_search.best_params_) # The optimal hyperparameter combination\nprint(\"Best score:\", grid_search.best_score_) # The highest cross-validation score (negative MSE)\nbest_model = grid_search.best_estimator_ # The trained XGBoost model with the best hyperparameters\nprint(f\"Grid Search: {best_model}\")\n\nFitting 5 folds for each of 405 candidates, totalling 2025 fits\n\n\n\n\n\nEvaluate the Model\nAccuracy – The percentage of total predictions that are correct.\nExample: If a spam filter correctly classifies 90 out of 100 emails (whether spam or not), the accuracy is 90%.\nF1 Score – Out of all the positive predictions, how many were actually correct.\nExample: If a spam filter predicts 20 emails as spam, but only 15 are actually spam, precision is 15/20 = 75%.\nRecall Score – Out of all the actual positive cases, how many did the model correctly identify.\nExample: If there were 25 spam emails in total, and the model correctly identified 15 of them, recall is 15/25 = 60%.\nPrecision Score – A balance between precision and recall (harmonic mean).\nExample: If precision is 75% and recall is 60%, F1 score is (2 × 75 × 60) / (75 + 60) = 66.7%.\n\n# Evaluate the model using regression metrics\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mse)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\n\n# Calculate R-squared (R2)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint('Mean Squared Error (MSE):', mse)\nprint('Root Mean Squared Error (RMSE):', rmse)\nprint('Mean Absolute Error (MAE):', mae)\nprint('R-squared (R2):', r2)"
  },
  {
    "objectID": "Machine Learning projects/2_DecisionTree_RandomForests_Model/readme.html",
    "href": "Machine Learning projects/2_DecisionTree_RandomForests_Model/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "Decision Tree (DT) is a supervised machine learning algorithm used for both classification and regression tasks. It splits data into branches based on feature values to make predictions.\nDefinition A Decision Tree is a flowchart-like model where data is split into smaller subsets based on feature conditions. The structure consists of:\nRoot Node: The starting point that represents the entire dataset. Decision Nodes: Points where the data is split based on a feature. Leaves: Terminal nodes that provide the final prediction. For classification, a Decision Tree assigns class labels based on feature splits. For regression, it predicts continuous values using the average or mean of data points in each leaf.\nKey Concepts\nSplitting Criteria:\nDetermines how the dataset is divided at each step. Common methods: Gini Impurity (Classification) – Measures the likelihood of incorrect classification. Entropy (Classification) – Uses information gain to decide splits. Mean Squared Error (MSE) (Regression) – Measures variance within nodes. Tree Depth & Overfitting:\nDeeper trees fit training data better but may overfit. Pruning (removing unnecessary branches) improves generalization. Feature Importance:\nDecision Trees rank features by their impact on predictions. Helps in feature selection for other models. Handling Missing Data:\nSome implementations allow surrogate splits to handle missing values.\nPros\nEasy to Understand & Interpret – Can be visualized as a simple flowchart. No Need for Feature Scaling – Works with both categorical and numerical features. Handles Non-Linearity – Can model complex relationships without requiring transformation. Fast for Small Datasets – Training and inference are relatively quick.\nCons\nProne to Overfitting – Deep trees can memorize training data, reducing generalization. Unstable to Small Changes – Small variations in data can change the tree structure significantly. Less Efficient on Large Datasets – Computationally expensive for large datasets.\nTips\nLimit Tree Depth – Use max_depth to prevent overfitting. Pruning Techniques – Use pre-pruning (max_leaf_nodes) or post-pruning to remove unnecessary branches. Use Feature Importance – Identify the most influential features and remove irrelevant ones. Consider Ensemble Methods – Random Forest or Gradient Boosting improve Decision Trees by reducing variance.\nUseful Articles and Videos\nhttps://www.datacamp.com/tutorial/decision-tree-classification-python https://www.ibm.com/think/topics/decision-trees https://www.youtube.com/watch?v=6DlWndLbk90 https://www.youtube.com/watch?v=ZOiBe-nrmc4\nRandom Forest (RF) is a supervised machine learning algorithm used for both classification and regression tasks. It builds multiple decision trees and combines their outputs for better accuracy and stability.\nDefinition A Random Forest is an ensemble learning method that creates a “forest” of decision trees during training. Instead of relying on a single tree, Random Forest combines the predictions of multiple trees to improve performance. The structure consists of:\nDecision Trees: Individual trees that make predictions based on subsets of data and features. Bagging (Bootstrap Aggregating): Each tree is trained on a randomly sampled subset of the data. Voting/Averaging: Combines the predictions from all trees: For classification, the majority vote decides the class. For regression, the average of all tree predictions is used. Random Forest reduces the risk of overfitting compared to individual decision trees and works well for both categorical and numerical data.\nKey Concepts\nBagging: Random Forest uses bootstrapping to train each tree on a different sample of the data. This creates diversity among trees, making the model more robust. Feature Randomness: At each split, Random Forest considers a random subset of features rather than all features. This reduces correlation between trees and improves generalization. Out-of-Bag (OOB) Error: Trees not trained on certain data points (left out during bootstrapping) can be used to validate the model. OOB error gives an unbiased estimate of model performance. Feature Importance: Random Forest provides a ranking of feature importance based on how often features are used for splitting across trees. Useful for identifying key predictors in your data. Pros\nImproved Accuracy – Combines multiple trees, reducing overfitting. Robust to Noise – Handles outliers and noisy data better than individual trees. Handles Large Datasets – Can scale well with more data. Feature Selection – Provides insights into the importance of features. No Need for Feature Scaling – Works with unscaled data, both numerical and categorical.\nCons\nLess Interpretable – Harder to visualize compared to a single decision tree. Computationally Intensive – Training many trees can take time and memory for large datasets. Overfitting Risk with Too Many Trees – Although rare, excessive trees might still overfit without tuning. Slower Inference – Predictions may take longer because they aggregate results from multiple trees.\nTips\nTune n_estimators – Adjust the number of trees to balance accuracy and computational cost. Limit Tree Depth – Use max_depth to avoid overfitting while maintaining performance. Optimize Feature Subset Size – Use max_features to control how many features each tree considers at a split. Use Feature Importance – Rank and prioritize the most important features in your dataset. Combine with Other Methods – Random Forest pairs well with techniques like PCA for dimensionality reduction. Useful Articles and Videos\nhttps://www.datacamp.com/tutorial/random-forests-classifier-python https://www.ibm.com/topics/random-forest https://www.youtube.com/watch?v=J4Wdy0Wc_xQ https://www.youtube.com/watch?v=QHOazyP-YlM\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning projects/2_DecisionTree_RandomForests_Model"
    ]
  },
  {
    "objectID": "Machine Learning projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#overview",
    "href": "Machine Learning projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#overview",
    "title": "Classification Machine Learning Model 🧠 📈",
    "section": "Overview",
    "text": "Overview\nXGBoost Classification is a supervised machine learning algorithm that builds an ensemble of decision trees to predict categorical outcomes. It is optimized for speed and performance using gradient boosting techniques.\n\n\n\nDefinition\nXGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:\n\nBoosting Trees: A collection of decision trees built sequentially to reduce errors.\n\nGradient Descent Optimization: Adjusts model weights using the gradient of a loss function.\n\nRegularization: Controls model complexity to prevent overfitting.\n\nFor classification, XGBoost predicts categorical outcomes by minimizing a chosen loss function, such as logistic loss for binary classification or softmax (cross-entropy) loss for multi-class classification.\n\n\n\nKey Concepts\n1. Boosting Mechanism:\n- Unlike a single decision tree, XGBoost builds multiple trees in sequence.\n- Each new tree corrects the errors of the previous ones by focusing on misclassified examples.\n\nLoss Functions:\n\nDetermines how errors are measured and minimized.\n\nCommon choices for classification include:\n\nLogistic Loss – Used for binary classification tasks.\n\nSoftmax (Cross-Entropy Loss) – Used for multi-class classification tasks.\n\n\nRegularization Techniques:\n\nPrevents overfitting by adding penalties to complex models.\n\nL1 Regularization (Lasso) – Encourages sparsity by shrinking coefficients.\n\nL2 Regularization (Ridge) – Penalizes large coefficients to reduce variance.\n\nFeature Importance & Selection:\n\nXGBoost ranks features by importance, aiding in feature selection.\n\nHelps in eliminating redundant or irrelevant features for better performance.\n\n\n\n\n\nPros\n1. High Performance – Optimized for speed, scalability, and efficiency.\n2. Handles Missing Data – Automatically learns how to deal with missing values.\n3. Regularization Built-in – Reduces overfitting with L1 and L2 penalties.\n4. Probabilistic Predictions – Provides probability scores for classification, enabling threshold tuning.\n5. Works Well with Large Datasets – Efficient memory usage and parallel processing.\n\n\n\nCons\n1. Complexity – More difficult to tune compared to simpler models.\n2. Computationally Intensive – Training can be slow on very large datasets.\n3. Sensitive to Hyperparameters – Performance depends on careful tuning of learning rate, tree depth, and regularization.\n4. Less Interpretable – Decision boundaries may be challenging to interpret compared to simpler models.\n\n\n\nTips\n* Optimize Hyperparameters – Use grid search or Bayesian optimization for tuning.\n* Use Early Stopping – Stop training if performance ceases to improve on validation data.\n* Scale Features if Needed – Although XGBoost can handle unscaled data, standardization might improve performance.\n* Leverage Feature Importance – Identify and remove less relevant features to improve efficiency.\n* Adjust Decision Thresholds – Fine-tune the probability threshold to balance precision and recall for your specific task.\n\n\n\nUseful Articles and Videos\n* XGBoost Official Documentation\n* XGBoost for Classification – Machine Learning Mastery\n* Understanding XGBoost – Analytics Vidhya\n* XGBoost Explained for Classification – YouTube"
  },
  {
    "objectID": "Machine Learning projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#import-datalibraries",
    "href": "Machine Learning projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#import-datalibraries",
    "title": "Classification Machine Learning Model 🧠 📈",
    "section": "Import Data/Libraries",
    "text": "Import Data/Libraries\n\n!pip install lets_plot\n\nRequirement already satisfied: lets_plot in /usr/local/lib/python3.11/dist-packages (4.6.2)\nRequirement already satisfied: pypng in /usr/local/lib/python3.11/dist-packages (from lets_plot) (0.20220715.0)\nRequirement already satisfied: palettable in /usr/local/lib/python3.11/dist-packages (from lets_plot) (3.3.3)\n\n\n\n# needed libraries for Classification models\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import f1_score\nfrom xgboost import XGBClassifier\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Load the training dataset\n#### If this gives an error go into the Data folder in GitHub and click on the data csv and then \"Raw\"\n#### (underneath history in the upper righthand corner) then copy that url to replace the \"data_raw_url\"\ndata_raw_url = 'https://raw.githubusercontent.com/BYUIDSS/DSS-ML-Bootcamp/refs/heads/main/3b_Classification_Model/data/Churn_Modelling.csv?token=GHSAT0AAAAAAC7UQH3WQQ4HLOI3GC7L7EVIZ7KGDGQ'\nbanking_df = pd.read_csv(data_raw_url)"
  },
  {
    "objectID": "Machine Learning projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#explore-visualize-and-understand-the-data",
    "href": "Machine Learning projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#explore-visualize-and-understand-the-data",
    "title": "Classification Machine Learning Model 🧠 📈",
    "section": "Explore, Visualize and Understand the Data",
    "text": "Explore, Visualize and Understand the Data\n\nbanking_df.head(10)\n\n\n  \n    \n\n\n\n\n\n\nRowNumber\nCustomerId\nSurname\nCreditScore\nGeography\nGender\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\n\n\n\n\n0\n1\n15634602\nHargrave\n619\nFrance\nFemale\n42.0\n2\n0.00\n1\n1.0\n1.0\n101348.88\n1\n\n\n1\n2\n15647311\nHill\n608\nSpain\nFemale\n41.0\n1\n83807.86\n1\n0.0\n1.0\n112542.58\n0\n\n\n2\n3\n15619304\nOnio\n502\nFrance\nFemale\n42.0\n8\n159660.80\n3\n1.0\n0.0\n113931.57\n1\n\n\n3\n4\n15701354\nBoni\n699\nFrance\nFemale\n39.0\n1\n0.00\n2\n0.0\n0.0\n93826.63\n0\n\n\n4\n5\n15737888\nMitchell\n850\nSpain\nFemale\n43.0\n2\n125510.82\n1\nNaN\n1.0\n79084.10\n0\n\n\n5\n6\n15574012\nChu\n645\nSpain\nMale\n44.0\n8\n113755.78\n2\n1.0\n0.0\n149756.71\n1\n\n\n6\n7\n15592531\nBartlett\n822\nNaN\nMale\n50.0\n7\n0.00\n2\n1.0\n1.0\n10062.80\n0\n\n\n7\n8\n15656148\nObinna\n376\nGermany\nFemale\n29.0\n4\n115046.74\n4\n1.0\n0.0\n119346.88\n1\n\n\n8\n9\n15792365\nHe\n501\nFrance\nMale\n44.0\n4\n142051.07\n2\n0.0\nNaN\n74940.50\n0\n\n\n9\n10\n15592389\nH?\n684\nFrance\nMale\nNaN\n2\n134603.88\n1\n1.0\n1.0\n71725.73\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nbanking_df.describe()\n\n\n  \n    \n\n\n\n\n\n\nRowNumber\nCustomerId\nCreditScore\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\n\n\n\n\ncount\n10002.000000\n1.000200e+04\n10002.000000\n10001.000000\n10002.000000\n10002.000000\n10002.000000\n10001.000000\n10001.000000\n10002.000000\n10002.000000\n\n\nmean\n5001.499600\n1.569093e+07\n650.555089\n38.922311\n5.012498\n76491.112875\n1.530194\n0.705529\n0.514949\n100083.331145\n0.203759\n\n\nstd\n2887.472338\n7.193177e+04\n96.661615\n10.487200\n2.891973\n62393.474144\n0.581639\n0.455827\n0.499801\n57508.117802\n0.402812\n\n\nmin\n1.000000\n1.556570e+07\n350.000000\n18.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n11.580000\n0.000000\n\n\n25%\n2501.250000\n1.562852e+07\n584.000000\n32.000000\n3.000000\n0.000000\n1.000000\n0.000000\n0.000000\n50983.750000\n0.000000\n\n\n50%\n5001.500000\n1.569073e+07\n652.000000\n37.000000\n5.000000\n97198.540000\n1.000000\n1.000000\n1.000000\n100185.240000\n0.000000\n\n\n75%\n7501.750000\n1.575323e+07\n718.000000\n44.000000\n7.000000\n127647.840000\n2.000000\n1.000000\n1.000000\n149383.652500\n0.000000\n\n\nmax\n10000.000000\n1.581569e+07\n850.000000\n92.000000\n10.000000\n250898.090000\n4.000000\n1.000000\n1.000000\n199992.480000\n1.000000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nbanking_df.dtypes\n\n\n\n\n\n\n\n\n0\n\n\n\n\nRowNumber\nint64\n\n\nCustomerId\nint64\n\n\nSurname\nobject\n\n\nCreditScore\nint64\n\n\nGeography\nobject\n\n\nGender\nobject\n\n\nAge\nfloat64\n\n\nTenure\nint64\n\n\nBalance\nfloat64\n\n\nNumOfProducts\nint64\n\n\nHasCrCard\nfloat64\n\n\nIsActiveMember\nfloat64\n\n\nEstimatedSalary\nfloat64\n\n\nExited\nint64\n\n\n\n\ndtype: object\n\n\n\n# 0 means staying and 1 means leaving.\n# Most of Germany left the bank compared to Spain and France. Does this mean geography has a huge impact on the decision to stay or leave?\nplt.figure(figsize=(10, 6))\nsns.countplot(x='Geography', hue='Exited', data=banking_df)\nplt.title('Geography vs Exited')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.countplot(data = banking_df, x = \"IsActiveMember\", hue = \"Exited\", palette= \"Set2\")\nplt.title(\"Exited by Status\")\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.boxplot(data = banking_df, x = \"Geography\", y = \"Exited\", hue = \"Geography\", palette= \"Set2\")\nplt.title(\"Exited by Geography\")\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.boxplot(data = banking_df, x = \"Geography\", y = \"Exited\", hue = \"Exited\", palette= \"Set2\")\nplt.title(\"Exited by Estimated Salary\")\nplt.show()"
  },
  {
    "objectID": "Machine Learning projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#feature-enginnering-and-data-augmentation",
    "href": "Machine Learning projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#feature-enginnering-and-data-augmentation",
    "title": "Classification Machine Learning Model 🧠 📈",
    "section": "Feature Enginnering and Data Augmentation",
    "text": "Feature Enginnering and Data Augmentation\n\nX = banking_df.drop(['Exited', 'Surname'], axis=1)\ny = banking_df['Exited']\n\n\nData Augmentation\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\nFeature Engineering\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns.\n\ncategorical_features = ['Geography', 'Gender']"
  },
  {
    "objectID": "Machine Learning projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#machine-learning-model",
    "href": "Machine Learning projects/4_Classification_Model/ML_Bootcamp_Classification_XGBoost.html#machine-learning-model",
    "title": "Classification Machine Learning Model 🧠 📈",
    "section": "Machine Learning Model",
    "text": "Machine Learning Model\n\nSplit the data to train and test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nCreate the model\n\n# Create an ordinal encoder\nordinal_encoder = OrdinalEncoder()\n\n# Create a column transformer to apply ordinal encoding to categorical features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', ordinal_encoder, categorical_features)\n    ],\n    remainder='passthrough'  # Keep other columns unchanged\n)\n\n\n\nTrain the model\n\n# Fit and transform the training data\nX_train_encoded = preprocessor.fit_transform(X_train)\n\n# Transform the test data using the fitted preprocessor\nX_test_encoded = preprocessor.transform(X_test)\n\n\n\nMake predictions\n\n# Initialize the XGBoost classifier\nxgb_classifier = XGBClassifier(random_state=42)\n\n\nHyperparameter Search\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid = {\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7],\n}\n\n# Perform GridSearchCV\ngrid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=3, scoring='f1')\ngrid_search.fit(X_train_encoded, y_train)\n\n# Get the best model from GridSearchCV\nbest_xgb_classifier = grid_search.best_estimator_\n\n# Make predictions using the best model\ny_pred = best_xgb_classifier.predict(X_test_encoded)\n\n\n\n\nEvaluate the Model\nAccuracy – The percentage of total predictions that are correct.\nExample: If a spam filter correctly classifies 90 out of 100 emails (whether spam or not), the accuracy is 90%.\nF1 Score – Out of all the positive predictions, how many were actually correct.\nExample: If a spam filter predicts 20 emails as spam, but only 15 are actually spam, precision is 15/20 = 75%.\nRecall Score – Out of all the actual positive cases, how many did the model correctly identify.\nExample: If there were 25 spam emails in total, and the model correctly identified 15 of them, recall is 15/25 = 60%.\nPrecision Score – A balance between precision and recall (harmonic mean).\nExample: If precision is 75% and recall is 60%, F1 score is (2 × 75 × 60) / (75 + 60) = 66.7%.\n\n# Evaluate the model using classification metrics\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mse)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\n\n# Calculate R-squared (R2)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint('Mean Squared Error (MSE):', mse)\nprint('Root Mean Squared Error (RMSE):', rmse)\nprint('Mean Absolute Error (MAE):', mae)\nprint('R-squared (R2):', r2)\n\nMean Squared Error (MSE): 0.13693153423288357\nRoot Mean Squared Error (RMSE): 0.37004261137453287\nMean Absolute Error (MAE): 0.13693153423288357\nR-squared (R2): 0.14705086201263828"
  },
  {
    "objectID": "Machine Learning projects/4_Classification_Model/data/readme.html",
    "href": "Machine Learning projects/4_Classification_Model/data/readme.html",
    "title": "Tam Tran - Data Science Portfolio",
    "section": "",
    "text": "– Data Overview –\nThe bank customer churn dataset is a commonly used dataset for predicting customer churn in the banking industry. It contains information on bank customers who either left the bank or continue to be a customer.\nThe data dictionary: - Customer ID: A unique identifier for each customer - Surname: The customer’s surname or last name - Credit Score: A numerical value representing the customer’s credit score - Geography: The country where the customer resides (France, Spain or Germany) - Gender: The customer’s gender (Male or Female) - Age: The customer’s age. - Tenure: The number of years the customer has been with the bank - Balance: The customer’s account balance - NumOfProducts: The number of bank products the customer uses (e.g., savings account, credit card) - HasCrCard: Whether the customer has a credit card (1 = yes, 0 = no) - IsActiveMember: Whether the customer is an active member (1 = yes, 0 = no) - EstimatedSalary: The estimated salary of the customer - Exited: Whether the customer has churned (1 = yes, 0 = no)\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine Learning projects/1_KNN_Model/ML_Bootcamp_KNN.html#import-datalibraries",
    "href": "Machine Learning projects/1_KNN_Model/ML_Bootcamp_KNN.html#import-datalibraries",
    "title": "K-Nearest Neighbors Machine Leaning Model 💻 🧠",
    "section": "Import Data/Libraries",
    "text": "Import Data/Libraries\n\n!pip3 install lets-plot\n\nCollecting lets-plot\n  Downloading lets_plot-4.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting pypng (from lets-plot)\n  Downloading pypng-0.20220715.0-py3-none-any.whl.metadata (13 kB)\nCollecting palettable (from lets-plot)\n  Downloading palettable-3.3.3-py2.py3-none-any.whl.metadata (3.3 kB)\nDownloading lets_plot-4.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 28.8 MB/s eta 0:00:00\nDownloading palettable-3.3.3-py2.py3-none-any.whl (332 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 332.3/332.3 kB 16.1 MB/s eta 0:00:00\nDownloading pypng-0.20220715.0-py3-none-any.whl (58 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 kB 3.9 MB/s eta 0:00:00\nInstalling collected packages: pypng, palettable, lets-plot\nSuccessfully installed lets-plot-4.6.2 palettable-3.3.3 pypng-0.20220715.0\n\n\n\n\n# needed libraries for KNN models\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.model_selection import cross_val_score,  train_test_split , KFold\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nimport lets_plot as lp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n\n# foundation dataset\nfrom sklearn.datasets import load_iris\n\n# stretch dataset\ncleveland_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data', header=None)\nhungarian_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data', header=None)\nswitzerland_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data', header=None)\n\n\n# Code to set up access to the foundational dataset\niris = load_iris()\niris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\niris_df['target'] = iris.target_names[iris.target]\n# iris_df is now the dataframe to be used for the foundational model\niris_df.head()\n\n\n  \n    \n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa"
  },
  {
    "objectID": "Machine Learning projects/1_KNN_Model/ML_Bootcamp_KNN.html#explore-visualize-and-understand-the-data",
    "href": "Machine Learning projects/1_KNN_Model/ML_Bootcamp_KNN.html#explore-visualize-and-understand-the-data",
    "title": "K-Nearest Neighbors Machine Leaning Model 💻 🧠",
    "section": "Explore, Visualize and Understand the Data",
    "text": "Explore, Visualize and Understand the Data\n\n\nsns.pairplot(iris_df, hue = \"target\", size=3, markers=[\"o\", \"s\", \"D\"])\nplt.figure()\nplt.show()\n\n/usr/local/lib/python3.11/dist-packages/seaborn/axisgrid.py:2100: UserWarning: The `size` parameter has been renamed to `height`; please update your code.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;"
  },
  {
    "objectID": "Machine Learning projects/1_KNN_Model/ML_Bootcamp_KNN.html#feature-enginnering-and-data-augmentation",
    "href": "Machine Learning projects/1_KNN_Model/ML_Bootcamp_KNN.html#feature-enginnering-and-data-augmentation",
    "title": "K-Nearest Neighbors Machine Leaning Model 💻 🧠",
    "section": "Feature Enginnering and Data Augmentation",
    "text": "Feature Enginnering and Data Augmentation\n\nData Augmentation\nDefinition: Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.\n\n\nFeature Engineering\nDefinition: Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns."
  },
  {
    "objectID": "Machine Learning projects/1_KNN_Model/ML_Bootcamp_KNN.html#machine-learning-model",
    "href": "Machine Learning projects/1_KNN_Model/ML_Bootcamp_KNN.html#machine-learning-model",
    "title": "K-Nearest Neighbors Machine Leaning Model 💻 🧠",
    "section": "Machine Learning Model",
    "text": "Machine Learning Model\n\nSplit the data\n\nx= iris_df.drop(columns=['target'])\ny = iris_df['target']\n\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42) #Training 80% and testing 20%\n\n\ny.head()\n\n\n\n\n\n\n\n\ntarget\n\n\n\n\n0\nsetosa\n\n\n1\nsetosa\n\n\n2\nsetosa\n\n\n3\nsetosa\n\n\n4\nsetosa\n\n\n\n\ndtype: object\n\n\n\n\nCreate the model\n\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(x, y)\n\nKNeighborsClassifier(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=3) \n\n\n\n\nTrain the model\n\nneigh.fit(x, y)\n\nKNeighborsClassifier(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=3) \n\n\n\n\nMake predictions\n\ny_pred = neigh.predict(X_test)"
  },
  {
    "objectID": "Machine Learning projects/1_KNN_Model/ML_Bootcamp_KNN.html#evaluate-the-model",
    "href": "Machine Learning projects/1_KNN_Model/ML_Bootcamp_KNN.html#evaluate-the-model",
    "title": "K-Nearest Neighbors Machine Leaning Model 💻 🧠",
    "section": "Evaluate the Model",
    "text": "Evaluate the Model\n\naccuracy = accuracy_score(y_test, y_pred)*100\n\nprint('Accuracy: ' + str(round(accuracy, 2)) + ' %.')\n\nAccuracy: 100.0 %."
  }
]