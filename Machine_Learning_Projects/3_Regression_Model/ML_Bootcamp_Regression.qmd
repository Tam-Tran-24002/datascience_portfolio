---
title: "Regression Machine Learning Model \U0001F9E0  \U0001F4C8"
jupyter: python3
---


---




## Overview


**XGBoost Regression is a supervised machine learning algorithm that builds an ensemble of decision trees to predict continuous values. It is optimized for speed and performance using gradient boosting techniques.**  

<br>

---

<br>

**Definition**  
XGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for efficiency and accuracy. It improves predictions by sequentially training trees while correcting previous errors. The key components include:  

* **Boosting Trees**: A collection of decision trees built sequentially to reduce errors.  
* **Gradient Descent Optimization**: Adjusts model weights using the gradient of a loss function.  
* **Regularization**: Controls model complexity to prevent overfitting.  

For **regression**, XGBoost predicts continuous values by minimizing a chosen loss function, commonly Mean Squared Error (MSE) or Mean Absolute Error (MAE).  

<br>

---

<br>

**Key Concepts**  
1. **Boosting Mechanism**:  
   - Unlike a single decision tree, XGBoost builds multiple trees in sequence.  
   - Each new tree corrects the errors of the previous ones by focusing on residuals.  

2. **Loss Functions**:  
   - Determines how errors are measured and minimized.  
   - Common choices:  
     * **Mean Squared Error (MSE)** – Penalizes larger errors more heavily.  
     * **Mean Absolute Error (MAE)** – Treats all errors equally.  
     * **Huber Loss** – A mix of MSE and MAE to handle outliers.  

3. **Regularization Techniques**:  
   - Prevents overfitting by adding penalties to complex models.  
   - **L1 Regularization (Lasso)** – Shrinks coefficients, promoting sparsity.  
   - **L2 Regularization (Ridge)** – Penalizes large coefficients to reduce variance.  

4. **Feature Importance & Selection**:  
   - XGBoost ranks features by importance, aiding feature selection.  
   - Can be used to eliminate redundant or irrelevant features.  

<br>

---

<br>

**Pros**  
1. **High Performance** – Optimized for speed, scalability, and efficiency.  
2. **Handles Missing Data** – Automatically learns how to deal with missing values.  
3. **Regularization Built-in** – Reduces overfitting with L1 and L2 penalties.  
4. **Works Well with Large Datasets** – Efficient memory usage and parallel processing.  

<br>

---

<br>

**Cons**  
1. **Complexity** – More difficult to tune compared to simpler models.  
2. **Computationally Intensive** – Training can be slow on very large datasets.  
3. **Sensitive to Hyperparameters** – Performance depends on careful tuning of learning rate, tree depth, and regularization.  

<br>

---

<br>

**Tips**  
* **Optimize Hyperparameters** – Use grid search or Bayesian optimization for tuning.  
* **Use Early Stopping** – Stops training if performance stops improving on validation data.  
* **Scale Features if Needed** – Although XGBoost can handle unscaled data, standardization may help in some cases.  
* **Leverage Feature Importance** – Identify and remove less relevant features to improve efficiency.  

<br>

---

<br>

**Useful Articles and Videos**  
* [XGBoost Official Documentation](https://xgboost.readthedocs.io/en/stable/tutorials/model.html)  
* [XGBoost for Regression – Machine Learning Mastery](https://machinelearningmastery.com/xgboost-for-regression/)  
* [Understanding XGBoost – Analytics Vidhya](https://www.analyticsvidhya.com/blog/2016/01/xgboost-algorithm-easy-steps/)  
* [XGBoost Explained – YouTube](https://www.youtube.com/watch?v=OtD8wVaFm6E)  

<br>

## Import Data/Libraries

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
!pip install lets_plot
```

```{python}
# needed libraries for Regression models
import pandas as pd
from sklearn import tree
import xgboost as xgb
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, mean_squared_error
from sklearn.model_selection import cross_val_score,  train_test_split , KFold
from sklearn.preprocessing import StandardScaler, Normalizer
import lets_plot as lp
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
# import kagglehub

# foundation dataset
data_raw_url = "nyc-east-river-bicycle-counts.csv"
bicycle_df = pd.read_csv(data_raw_url)
```

## Explore, Visualize and Understand the Data

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 554}
bicycle_df.head(10)
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 300}
bicycle_df.describe()
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 429}
bicycle_df.dtypes
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 528}
sns.barplot(data=bicycle_df, x="Brooklyn Bridge", y ="Total", color="blue")
plt.title("Plot")
plt.legend(loc='upper right')
plt.show()
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 740}
sns.barplot(data=bicycle_df, x="Manhattan Bridge", y ="Total", hue="Total", palette="Set2")
plt.title("Plot")
plt.show()
```

## Feature Enginnering and Data Augmentation

### **Data Augmentation**  
**Definition:**
Data augmentation is the process of artificially expanding the size and diversity of a training dataset by applying transformations or modifications to the existing data while preserving the underlying labels or structure. It is commonly used in machine learning, especially in computer vision and natural language processing, to improve model performance and robustness.

### **Feature Engineering**  
**Definition:**
Feature engineering is the process of creating, modifying, or selecting relevant features (input variables) from raw data to improve the performance of a machine learning model. It involves transforming raw data into a format that makes it more suitable for algorithms to learn patterns.

```{python}
bicycle_df1 = pd.get_dummies(bicycle_df, columns=["Date", "Day","Precipitation"])
```

```{python}
X = bicycle_df1.drop(columns=["Brooklyn Bridge",	"Manhattan Bridge",	"Williamsburg Bridge",	"Queensboro Bridge", "Total"])
y = bicycle_df1["Total"]
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 300}
bicycle_df1.describe()
```

## Machine Learning Model

### Split the data to train and test

```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

```{python}
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)
```


### Create the model

```{python}
params = {
    'objective': 'reg:squarederror',  # Regression task
    'max_depth': 6,                   # Maximum depth of trees
    'learning_rate': 0.1,             # Learning rate
    'n_estimators': 100,              # Number of boosting rounds
}
```

### Train the model

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
model = xgb.train(params, dtrain, num_boost_round=100)
```

### Make predictions

```{python}
# Predicting the Test set results
y_pred = model.predict(dtest)
```

#### Hyperparameter Search

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
from sklearn.model_selection import GridSearchCV

# Define the parameter grid to search
param_grid = {
    'max_depth': [3, 4, 5, 6, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [50, 100, 200],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# Create the XGBoost model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', seed=42)

# Create and Configure GridSearchCV
grid_search = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',  # Since MSE should be minimized, we use its negative (because GridSearchCV maximizes the score)
    cv=5,  # Uses 5-fold cross-validation to evaluate each parameter set
    verbose=1, # Prints progress updates
    n_jobs=-1 # Uses all available CPU cores for parallel computation, making it faster
)
# Fit the Grid Search to Training Data
grid_search.fit(X_train, y_train)

# Get the Best Model & Hyperparameters
print("Best hyperparameters:", grid_search.best_params_) # The optimal hyperparameter combination
print("Best score:", grid_search.best_score_) # The highest cross-validation score (negative MSE)
best_model = grid_search.best_estimator_ # The trained XGBoost model with the best hyperparameters
print(f"Grid Search: {best_model}")
```

### Evaluate the Model

**Accuracy – The percentage of total predictions that are correct.**  
Example: If a spam filter correctly classifies 90 out of 100 emails (whether spam or not), the accuracy is 90%.  

**F1 Score  – Out of all the positive predictions, how many were actually correct.**  
Example: If a spam filter predicts 20 emails as spam, but only 15 are actually spam, precision is 15/20 = 75%.  


**Recall Score – Out of all the actual positive cases, how many did the model correctly identify.**  
Example: If there were 25 spam emails in total, and the model correctly identified 15 of them, recall is 15/25 = 60%.  

**Precision Score – A balance between precision and recall (harmonic mean).**  
Example: If precision is 75% and recall is 60%, F1 score is (2 × 75 × 60) / (75 + 60) = 66.7%.  


```{python}
# Evaluate the model using regression metrics
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)

# Calculate Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)

# Calculate R-squared (R2)
r2 = r2_score(y_test, y_pred)

# Print the evaluation metrics
print('Mean Squared Error (MSE):', mse)
print('Root Mean Squared Error (RMSE):', rmse)
print('Mean Absolute Error (MAE):', mae)
print('R-squared (R2):', r2)
```

